"""
ÙˆØ­Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ø¨Ø³Ø§Ù…
ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆØ§Ù„Ø¹Ø§Ø·ÙÙŠ ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
"""

import re
import random
from typing import Dict, List, Optional, Any, Tuple
from core.utils import is_arabic, normalize_text

class AdvancedIntelligence:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
    
    def __init__(self):
        self.emotion_patterns = self._load_emotion_patterns()
        self.question_patterns = self._load_question_patterns()
        self.context_templates = self._load_context_templates()
        
    def _load_emotion_patterns(self) -> Dict[str, List[str]]:
        """ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        return {
            'positive': [
                'Ø´ÙƒØ±Ø§', 'Ù…Ù…ØªØ§Ø²', 'Ø±Ø§Ø¦Ø¹', 'Ø¬Ù…ÙŠÙ„', 'Ø£Ø­Ø¨Ùƒ', 'Ø³Ø¹ÙŠØ¯', 'ÙØ±Ø­', 'Ø­Ù„Ùˆ', 'Ø¹Ø¸ÙŠÙ…', 
                'Ø±Ø§Ø¦Ø¹', 'Ù…Ù…ØªØ¹', 'Ù…ÙÙŠØ¯', 'Ù†Ø¬Ø­', 'Ù†Ø¬Ø§Ø­', 'ÙØ§Ø²', 'ÙÙˆØ²', 'Ø£ÙØ¶Ù„'
            ],
            'negative': [
                'Ø²Ø¹Ù„Ø§Ù†', 'Ø­Ø²ÙŠÙ†', 'ØµØ¹Ø¨', 'Ù…Ø´ÙƒÙ„Ø©', 'Ø®Ø·Ø£', 'ØºÙ„Ø·', 'ÙØ´Ù„', 'Ø³ÙŠØ¡', 
                'ØµØ¹ÙˆØ¨Ø©', 'ØªØ¹Ø¨', 'Ù…ØªØ¹Ø¨', 'Ù…Ø´Ø§ÙƒÙ„', 'Ø®Ø§ÙŠÙ', 'Ù‚Ù„Ù‚Ø§Ù†', 'Ù…Ù‚Ù„Ù‚'
            ],
            'help_request': [
                'Ø³Ø§Ø¹Ø¯Ù†ÙŠ', 'Ù…Ø³Ø§Ø¹Ø¯Ø©', 'Ø£Ø±ÙŠØ¯', 'Ø£Ø­ØªØ§Ø¬', 'Ù…Ù† ÙØ¶Ù„Ùƒ', 'Ø±Ø¬Ø§Ø¡Ù‹', 
                'Ù„Ùˆ Ø³Ù…Ø­Øª', 'Ù…Ù…ÙƒÙ†', 'Ø¹Ø§ÙŠØ²', 'Ø£Ù‚Ø¯Ø±', 'ÙƒÙŠÙ'
            ],
            'confusion': [
                'Ù…Ø§ ÙÙ‡Ù…Øª', 'Ù…Ø´ ÙØ§Ù‡Ù…', 'ØºÙŠØ± ÙˆØ§Ø¶Ø­', 'Ù…Ø¹Ù‚Ø¯', 'ØµØ¹Ø¨ Ø§Ù„ÙÙ‡Ù…', 
                'Ù…Ø§ Ø£Ø¹Ø±Ù', 'Ù…Ø­ØªØ§Ø±', 'Ù…Ø´ Ø¹Ø§Ø±Ù'
            ],
            'gratitude': [
                'Ø´ÙƒØ±Ø§', 'Ø´ÙƒØ±Ù‹Ø§', 'Ù…Ø´ÙƒÙˆØ±', 'Ø´Ø§ÙƒØ±', 'Ù…Ù…Ù†ÙˆÙ†', 'Ø£Ø´ÙƒØ±Ùƒ', 
                'Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±', 'Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ'
            ]
        }
    
    def _load_question_patterns(self) -> Dict[str, List[str]]:
        """ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
        return {
            'definition': ['Ù…Ø§ Ù‡Ùˆ', 'Ù…Ø§ Ù‡ÙŠ', 'Ù…Ø§Ù‡Ùˆ', 'Ù…Ø§Ù‡ÙŠ', 'Ø¹Ø±Ù', 'ØªØ¹Ø±ÙŠÙ', 'Ù…Ø¹Ù†Ù‰'],
            'explanation': ['ÙƒÙŠÙ', 'Ø§Ø´Ø±Ø­', 'ÙˆØ¶Ø­', 'ÙØ³Ø±', 'Ø¨ÙŠÙ†', 'Ø£Ø¹Ø·Ù†ÙŠ', 'Ø¹Ù„Ù…Ù†ÙŠ'],
            'reason': ['Ù„Ù…Ø§Ø°Ø§', 'Ù„ÙŠØ´', 'Ø§Ù„Ø³Ø¨Ø¨', 'Ù„Ø£Ù†', 'Ø¹Ù„Ù„', 'Ù…Ø§ Ø§Ù„Ø³Ø¨Ø¨'],
            'location': ['Ø£ÙŠÙ†', 'ÙˆÙŠÙ†', 'Ù…ÙƒØ§Ù†', 'Ù…ÙˆÙ‚Ø¹', 'Ù…ÙƒØ§Ù†Ù‡Ø§', 'Ù…ÙˆÙ‚Ø¹Ù‡Ø§'],
            'time': ['Ù…ØªÙ‰', 'ÙˆÙ‚Øª', 'ØªØ§Ø±ÙŠØ®', 'Ø²Ù…Ù†', 'Ø³Ù†Ø©', 'ÙŠÙˆÙ…', 'Ø³Ø§Ø¹Ø©'],
            'person': ['Ù…Ù†', 'Ù…ÙŠÙ†', 'Ø´Ø®Øµ', 'Ø¥Ù†Ø³Ø§Ù†', 'Ø±Ø¬Ù„', 'Ø§Ù…Ø±Ø£Ø©'],
            'quantity': ['ÙƒÙ…', 'Ø¹Ø¯Ø¯', 'Ù…Ù‚Ø¯Ø§Ø±', 'Ø­Ø¬Ù…', 'ÙƒÙ…ÙŠØ©', 'Ù…Ø³Ø§Ø­Ø©'],
            'yes_no': ['Ù‡Ù„', 'Ø£', 'ÙŠØ§ ØªØ±Ù‰', 'Ù…Ù…ÙƒÙ†', 'ØµØ­ÙŠØ­', 'Ø®Ø·Ø£'],
            'comparison': ['Ø£ÙŠÙ‡Ù…Ø§', 'Ø£ÙØ¶Ù„', 'Ø§Ù„ÙØ±Ù‚', 'Ù…Ù‚Ø§Ø±Ù†Ø©', 'Ø§Ø®ØªÙ„Ø§Ù', 'Ø£Ø­Ø³Ù†'],
            'mathematical': ['Ø§Ø­Ø³Ø¨', 'Ø­Ù„', 'Ù…Ø´ØªÙ‚', 'ØªÙƒØ§Ù…Ù„', 'Ù…Ø¹Ø§Ø¯Ù„Ø©', 'Ø±ÙŠØ§Ø¶ÙŠØ©', '+', '-', '*', '/', '=', '^']
        }
    
    def _load_context_templates(self) -> Dict[str, str]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        return {
            'definition': """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ¹Ù„ÙŠÙ…ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª. 
            Ù‚Ø¯Ù… ØªØ¹Ø±ÙŠÙØ§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹ Ù…Ø¹ Ø£Ù…Ø«Ù„Ø© ÙˆØªÙˆØ¶ÙŠØ­Ø§Øª Ø¹Ù…Ù„ÙŠØ©.
            Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ù…Ø¹ Ø§Ù„ØªØ¯Ø±Ø¬ Ù…Ù† Ø§Ù„Ø¨Ø³ÙŠØ· Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù‚Ø¯.""",
            
            'explanation': """Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ù…Ø§Ù‡Ø± Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø´Ø±Ø­ ÙˆØ§Ù„ØªÙˆØ¶ÙŠØ­.
            Ø§Ø´Ø±Ø­ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„Ø®Ø·ÙˆØ§Øª Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø«Ù„Ø© ÙˆØªØ´Ø¨ÙŠÙ‡Ø§Øª Ù…ÙÙ‡ÙˆÙ…Ø©.
            Ù‚Ø³Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø±ØªØ¨Ø© Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹.""",
            
            'reason': """Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø¹Ù…ÙŠÙ‚ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ ÙˆØ§Ù„Ø¹ÙˆØ§Ù…Ù„.
            ÙˆØ¶Ø­ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ ÙˆØ§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…ÙØµÙ„.
            Ø§Ø±Ø¨Ø· Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆÙ‚Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„.""",
            
            'location': """Ø£Ù†Øª Ø¬ØºØ±Ø§ÙÙŠ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©.
            Ø­Ø¯Ø¯ Ø§Ù„Ù…ÙƒØ§Ù† Ø¨Ø¯Ù‚Ø© Ù…Ø¹ ÙˆØµÙ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©.""",
            
            'time': """Ø£Ù†Øª Ù…Ø¤Ø±Ø® ÙˆØ®Ø¨ÙŠØ± Ø²Ù…Ù†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø£Ø­Ø¯Ø§Ø«.
            Ø­Ø¯Ø¯ Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¨Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨.""",
            
            'person': """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø³ÙŠØ± ÙˆØ§Ù„ØªØ±Ø§Ø¬Ù… Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª.
            Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø´Ø§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ù…Ø¹ Ø¥Ù†Ø¬Ø§Ø²Ø§ØªÙ‡Ù… ÙˆØ³ÙŠØ§Ù‚Ù‡Ù… Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ.""",
            
            'quantity': """Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø¥Ø­ØµØ§Ø¦ÙŠ Ø¯Ù‚ÙŠÙ‚ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„ÙƒÙ…ÙŠØ§Øª.
            Ù‚Ø¯Ù… Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù…Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.""",
            
            'yes_no': """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ­Ù„ÙŠÙ„ÙŠ ÙŠÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¤ÙƒØ¯Ø©.
            Ø£Ø¬Ø¨ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ø¹ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø£Ø¯Ù„Ø© ÙˆØ§Ù„Ù…Ø¨Ø±Ø±Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.""",
            
            'comparison': """Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ù‚Ø§Ø±Ù† Ù…ØªØ®ØµØµ ÙÙŠ Ø¯Ø±Ø§Ø³Ø© Ø§Ù„ÙØ±ÙˆÙ‚ ÙˆØ§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª.
            Ù‚Ø§Ø±Ù† Ø¨Ø¹Ù…Ù‚ Ù…Ø¹ Ø¥Ø¨Ø±Ø§Ø² Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠØ©.""",
            
            'mathematical': """Ø£Ù†Øª Ø£Ø³ØªØ§Ø° Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø®Ø¨ÙŠØ± ÙˆÙ…ØªØ®ØµØµ ÙÙŠ ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª.
            Ø§Ø´Ø±Ø­ Ø§Ù„Ø­Ù„ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙØµÙ„Ø©.
            Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ¨Ø³ÙŠØ· ÙˆØ§Ù„ØªÙˆØ¶ÙŠØ­ ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ©.""",
            
            'general': """Ø£Ù†Øª Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø¹Ø±Ø¨ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª Ø¹Ø§Ø·ÙÙŠØ© ÙˆÙ„ØºÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©.
            Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªÙƒØ§Ù…Ù„Ø© ÙˆÙ…ÙØµÙ„Ø© ÙˆØ´Ø§Ù…Ù„Ø© ØªØºØ·ÙŠ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©."""
        }
    
    def detect_question_type(self, question: str) -> str:
        """ÙƒØ´Ù Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©"""
        question_lower = question.lower().strip()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙØ¶Ù„
        clean_question = re.sub(r'[^\w\s]', ' ', question_lower)
        
        # ÙØ­Øµ ÙƒÙ„ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        for q_type, patterns in self.question_patterns.items():
            for pattern in patterns:
                if pattern in clean_question:
                    return q_type
        
        return 'general'
    
    def analyze_question(self, question: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø³Ø¤Ø§Ù„ ÙŠØ´Ù…Ù„ Ø§Ù„Ù†ÙˆØ¹ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø³ÙŠØ§Ù‚"""
        if not question.strip():
            return {
                'question_type': 'general',
                'emotional_context': {'primary_emotion': 'neutral', 'confidence': 0.5},
                'complexity_level': 'simple',
                'requires_research': False
            }
        
        # ÙƒØ´Ù Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        question_type = self.detect_question_type(question)
        
        # ÙƒØ´Ù Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        emotion, confidence = self.detect_emotion(question)
        emotional_context = {
            'primary_emotion': emotion,
            'confidence': confidence,
            'emotional_indicators': self._extract_emotional_indicators(question)
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_level = self._assess_complexity(question, question_type)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªØ§Ø¬ Ø¨Ø­Ø«
        requires_research = self._needs_research(question, question_type)
        
        # ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ø¥Ø¶Ø§ÙÙŠ
        linguistic_features = self._analyze_linguistic_features(question)
        
        return {
            'question_type': question_type,
            'emotional_context': emotional_context,
            'complexity_level': complexity_level,
            'requires_research': requires_research,
            'linguistic_features': linguistic_features,
            'recommended_approach': self._recommend_approach(question_type, emotion),
            'expected_response_length': self._estimate_response_length(complexity_level, question_type)
        }
    
    def _extract_emotional_indicators(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        indicators = []
        text_lower = text.lower()
        
        for emotion, patterns in self.emotion_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    indicators.append(f"{emotion}:{pattern}")
        
        return indicators
    
    def _assess_complexity(self, question: str, question_type: str) -> str:
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„"""
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_score = 0
        
        # Ø·ÙˆÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„
        if len(question) > 100:
            complexity_score += 2
        elif len(question) > 50:
            complexity_score += 1
        
        # Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        complex_types = ['mathematical', 'comparison', 'reason', 'explanation']
        if question_type in complex_types:
            complexity_score += 2
        
        # ÙˆØ¬ÙˆØ¯ Ù…ØµØ·Ù„Ø­Ø§Øª ØªÙ‚Ù†ÙŠØ© Ø£Ùˆ Ø¹Ù„Ù…ÙŠØ©
        technical_indicators = ['Ù…Ø¹Ø§Ø¯Ù„Ø©', 'Ù†Ø¸Ø±ÙŠØ©', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø¨Ø¯Ø£', 'ØªØ­Ù„ÙŠÙ„', 'Ø¯Ø±Ø§Ø³Ø©']
        for indicator in technical_indicators:
            if indicator in question:
                complexity_score += 1
        
        # Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
        if 'ØŸ' in question and question.count('ØŸ') > 1:
            complexity_score += 1
        
        if complexity_score >= 4:
            return 'complex'
        elif complexity_score >= 2:
            return 'moderate'
        else:
            return 'simple'
    
    def _needs_research(self, question: str, question_type: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ­ØªØ§Ø¬ Ø¨Ø­Ø« Ø®Ø§Ø±Ø¬ÙŠ"""
        research_indicators = [
            'Ø¢Ø®Ø±', 'Ø£Ø­Ø¯Ø«', 'Ø¬Ø¯ÙŠØ¯', 'Ø­Ø§Ù„ÙŠØ§Ù‹', 'Ø§Ù„Ø¢Ù†', 'Ø§Ù„ÙŠÙˆÙ…', 
            'Ø£Ø³Ø¹Ø§Ø±', 'Ø³Ø¹Ø±', 'Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª', 'Ø£Ø±Ù‚Ø§Ù… Ø­Ø¯ÙŠØ«Ø©'
        ]
        
        for indicator in research_indicators:
            if indicator in question:
                return True
        
        # Ø£Ù†ÙˆØ§Ø¹ Ø£Ø³Ø¦Ù„Ø© ØªØ­ØªØ§Ø¬ Ø¹Ø§Ø¯Ø© Ù„Ø¨Ø­Ø«
        research_types = ['location', 'time', 'person', 'quantity']
        return question_type in research_types
    
    def _analyze_linguistic_features(self, question: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù„ØºÙˆÙŠØ© Ù„Ù„Ø³Ø¤Ø§Ù„"""
        return {
            'is_arabic': is_arabic(question),
            'word_count': len(question.split()),
            'has_numbers': bool(re.search(r'\d', question)),
            'has_symbols': bool(re.search(r'[+\-*/=<>%]', question)),
            'question_marks': question.count('ØŸ') + question.count('?'),
            'formality_level': self._assess_formality(question)
        }
    
    def _assess_formality(self, text: str) -> str:
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø±Ø³Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ"""
        formal_indicators = ['ÙŠØ±Ø¬Ù‰', 'Ù…Ù† ÙØ¶Ù„Ùƒ', 'Ù„Ùˆ Ø³Ù…Ø­Øª', 'Ù†Ø±Ø¬Ùˆ', 'Ù†ØªÙ…Ù†Ù‰']
        informal_indicators = ['Ø§Ø²Ø§ÙŠ', 'Ø§ÙŠÙ‡', 'Ø¹Ø§ÙŠØ²', 'Ù…Ù…ÙƒÙ†']
        
        formal_count = sum(1 for indicator in formal_indicators if indicator in text)
        informal_count = sum(1 for indicator in informal_indicators if indicator in text)
        
        if formal_count > informal_count:
            return 'formal'
        elif informal_count > formal_count:
            return 'informal'
        else:
            return 'neutral'
    
    def _recommend_approach(self, question_type: str, emotion: str) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø£ÙØ¶Ù„ Ù…Ù†Ù‡Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        if emotion == 'confusion':
            return 'step_by_step_simple'
        elif emotion == 'help_request':
            return 'supportive_detailed'
        elif question_type == 'mathematical':
            return 'structured_solution'
        elif question_type in ['definition', 'explanation']:
            return 'comprehensive_educational'
        else:
            return 'balanced_informative'
    
    def _estimate_response_length(self, complexity: str, question_type: str) -> str:
        """ØªÙ‚Ø¯ÙŠØ± Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"""
        if complexity == 'complex' or question_type == 'mathematical':
            return 'detailed'  # 300+ ÙƒÙ„Ù…Ø©
        elif complexity == 'moderate':
            return 'medium'    # 150-300 ÙƒÙ„Ù…Ø©
        else:
            return 'concise'   # 50-150 ÙƒÙ„Ù…Ø©
    
    def enhance_response(self, response: str, analysis: Dict[str, Any], original_question: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙˆØªÙ‡Ø°ÙŠØ¨ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        if not response or not response.strip():
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©."
        
        enhanced = response.strip()
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø¯Ù…Ø© Ø¹Ø§Ø·ÙÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        emotion = analysis.get('emotional_context', {}).get('primary_emotion', 'neutral')
        if emotion == 'confusion':
            enhanced = f"Ø£ÙÙ‡Ù… Ø£Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ Ù…Ø¹Ù‚Ø¯Ø§Ù‹ØŒ Ø¯Ø¹Ù†ÙŠ Ø£ÙˆØ¶Ø­ Ø§Ù„Ø£Ù…Ø±:\n\n{enhanced}"
        elif emotion == 'help_request':
            enhanced = f"Ø¨ÙƒÙ„ Ø³Ø±ÙˆØ± Ø³Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±:\n\n{enhanced}"
        elif emotion == 'gratitude':
            enhanced = f"Ø´ÙƒØ±Ø§Ù‹ Ù„Ø«Ù‚ØªÙƒ Ø¨ÙŠØŒ Ø¥Ù„ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:\n\n{enhanced}"
        
        # Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ù†Ø§Ø³Ø¨
        question_type = analysis.get('question_type', 'general')
        follow_up = self._generate_follow_up_question(question_type, original_question)
        
        if follow_up:
            enhanced += f"\n\nğŸ’¡ **Ø³Ø¤Ø§Ù„ Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©:** {follow_up}"
        
        return enhanced
    
    def _generate_follow_up_question(self, question_type: str, original_question: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ù†Ø§Ø³Ø¨"""
        follow_ups = {
            'definition': [
                "Ù‡Ù„ ØªØ±ÙŠØ¯ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ø£Ùˆ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…ÙÙ‡ÙˆÙ…ØŸ",
                "Ø£Ù… ØªÙØ¶Ù„ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§ØªÙ‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ"
            ],
            'explanation': [
                "Ù‡Ù„ ØªØ±ÙŠØ¯ Ø´Ø±Ø­Ø§Ù‹ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù„Ø£ÙŠ Ù†Ù‚Ø·Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ",
                "Ø£Ù… ØªØ­ØªØ§Ø¬ Ø£Ù…Ø«Ù„Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªÙˆØ¶ÙŠØ­ØŸ"
            ],
            'mathematical': [
                "Ù‡Ù„ ØªØ±ÙŠØ¯ Ø±Ø¤ÙŠØ© Ø·Ø±Ù‚ Ø­Ù„ Ø£Ø®Ø±Ù‰ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©ØŸ",
                "Ø£Ù… ØªØ­ØªØ§Ø¬ Ø´Ø±Ø­Ø§Ù‹ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù„Ø£ÙŠ Ø®Ø·ÙˆØ©ØŸ"
            ],
            'comparison': [
                "Ù‡Ù„ ØªØ±ÙŠØ¯ Ù…Ù‚Ø§Ø±Ù†Ø© ØªÙØµÙŠÙ„ÙŠØ© Ø£ÙƒØ«Ø± Ù„Ø¬ÙˆØ§Ù†Ø¨ Ù…Ø¹ÙŠÙ†Ø©ØŸ",
                "Ø£Ù… ØªÙØ¶Ù„ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ù„Ù„ÙØ±ÙˆÙ‚ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©ØŸ"
            ],
            'reason': [
                "Ù‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ø¹ÙˆØ§Ù…Ù„ Ø£Ø®Ø±Ù‰ Ù‚Ø¯ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±ØŸ",
                "Ø£Ù… ØªØ­ØªØ§Ø¬ Ø£Ù…Ø«Ù„Ø© ØªØ§Ø±ÙŠØ®ÙŠØ© Ø£Ùˆ Ø­Ø¯ÙŠØ«Ø©ØŸ"
            ]
        }
        
        if question_type in follow_ups:
            import random
            return random.choice(follow_ups[question_type])
        
        # Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø© Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©
        general_follow_ups = [
            "Ù‡Ù„ ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­Ø§Ù‹ Ø¥Ø¶Ø§ÙÙŠØ§Ù‹ Ù„Ø£ÙŠ Ù†Ù‚Ø·Ø©ØŸ",
            "Ø£Ù… ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ø°ÙŠ ØµÙ„Ø©ØŸ",
            "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø¬Ø§Ù†Ø¨ Ø¢Ø®Ø± ØªÙˆØ¯ Ø§Ø³ØªÙƒØ´Ø§ÙÙ‡ØŸ"
        ]
        
        import random
        return random.choice(general_follow_ups)
    
    def detect_emotion(self, text: str) -> Tuple[str, float]:
        """ÙƒØ´Ù Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ø¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©"""
        text_lower = text.lower().strip()
        clean_text = re.sub(r'[^\w\s]', ' ', text_lower)
        
        emotion_scores = {}
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        for emotion, patterns in self.emotion_patterns.items():
            score = 0
            for pattern in patterns:
                # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„Ø¸Ù‡ÙˆØ± Ù…Ø¹ ÙˆØ²Ù†
                occurrences = len(re.findall(r'\b' + pattern + r'\b', clean_text))
                score += occurrences
            
            if score > 0:
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                emotion_scores[emotion] = score / len(patterns)
        
        if emotion_scores:
            # Ø£Ù‚ÙˆÙ‰ Ù…Ø´Ø§Ø¹Ø±
            dominant_emotion = max(emotion_scores, key=emotion_scores.get)
            confidence = emotion_scores[dominant_emotion]
            return dominant_emotion, confidence
        
        return 'neutral', 0.0
    
    def generate_emotional_response(self, emotion: str, confidence: float) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø¹Ø§Ø·ÙÙŠ Ù…Ù†Ø§Ø³Ø¨"""
        if confidence < 0.3:  # Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©
            return ""
        
        responses = {
            'positive': [
                'ğŸ˜Š Ø£Ø³Ø¹Ø¯Ù†ÙŠ Ø£Ù†Ùƒ Ø±Ø§Ø¶Ù! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø£ÙƒØ«Ø±ØŸ',
                'ğŸŒŸ Ø±Ø§Ø¦Ø¹! Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ù„Ø£Ù†Ù†ÙŠ Ø£Ø³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.',
                'â¤ï¸ Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ! Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„ÙØ®Ø± Ø¹Ù†Ø¯Ù…Ø§ Ø£Ø³Ø§Ø¹Ø¯Ùƒ.',
                'ğŸ‰ Ù…Ù…ØªØ§Ø²! Ø¯Ø¹Ù†ÙŠ Ø£Ù‚Ø¯Ù… Ù„Ùƒ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©.'
            ],
            'negative': [
                'ğŸ˜Ÿ Ø£Ø¹ØªØ°Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø¥Ø²Ø¹Ø§Ø¬. Ø¯Ø¹Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.',
                'ğŸ’™ Ø£ØªÙÙ‡Ù… Ø´Ø¹ÙˆØ±Ùƒ. Ø³Ø£Ø¨Ø°Ù„ Ù‚ØµØ§Ø±Ù‰ Ø¬Ù‡Ø¯ÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.',
                'ğŸ¤— Ù„Ø§ ØªÙ‚Ù„Ù‚ØŒ Ø³Ù†Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹Ø§Ù‹ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©.',
                'ğŸ’ª Ø£ÙÙ‡Ù… Ø§Ù„ØµØ¹ÙˆØ¨Ø©. Ø¯Ø¹Ù†ÙŠ Ø£Ù‚Ø¯Ù… Ù„Ùƒ Ø­Ù„ÙˆÙ„Ø§Ù‹ Ù…Ø¨Ø³Ø·Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.'
            ],
            'help_request': [
                'ğŸ™‹â€â™‚ï¸ Ø¨Ø§Ù„Ø·Ø¨Ø¹! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„ÙŠÙ‡ØŸ',
                'âœ‹ Ø£ÙƒÙŠØ¯! Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.',
                'ğŸ’ª Ù…Ø¹Ø§Ù‹ Ø³Ù†Ø¬Ø¯ Ø§Ù„Ø­Ù„! Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­.',
                'ğŸ¯ ØªÙ…Ø§Ù…! Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø² Ù„ØªÙ‚Ø¯ÙŠÙ… Ø£ÙØ¶Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ù…ÙƒÙ†Ø©.'
            ],
            'confusion': [
                'ğŸ¤” Ø¯Ø¹Ù†ÙŠ Ø£ÙˆØ¶Ø­ Ø§Ù„Ø£Ù…Ø± Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£Ø¨Ø³Ø· ÙˆØ£ÙˆØ¶Ø­.',
                'ğŸ’¡ Ø³Ø£Ø´Ø±Ø­ Ù„Ùƒ Ø¨ØªÙØµÙŠÙ„ Ø£ÙƒØ¨Ø± ÙˆÙˆØ¶ÙˆØ­ Ø£ÙƒØ«Ø±.',
                'ğŸ“š Ù„Ø§ Ù…Ø´ÙƒÙ„Ø©! Ø³Ø£Ø¹ÙŠØ¯ Ø§Ù„Ø´Ø±Ø­ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ© ÙˆÙ…Ø¨Ø³Ø·Ø©.',
                'ğŸ” Ø¯Ø¹Ù†ÙŠ Ø£Ø¹Ø·ÙŠÙƒ Ø´Ø±Ø­Ø§Ù‹ Ù…ÙØµÙ„Ø§Ù‹ ÙˆØ£Ù…Ø«Ù„Ø© ÙˆØ§Ø¶Ø­Ø©.'
            ],
            'gratitude': [
                'ğŸ¥° Ø§Ù„Ø¹ÙÙˆ! Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ù„Ø£Ù†Ù†ÙŠ Ø³Ø§Ø¹Ø¯ØªÙƒ.',
                'ğŸ˜Š Ù„Ø§ Ø´ÙƒØ± Ø¹Ù„Ù‰ ÙˆØ§Ø¬Ø¨! Ù‡Ø°Ø§ Ø¹Ù…Ù„ÙŠ ÙˆØ£Ø­Ø¨Ù‡.',
                'ğŸŒŸ ØªØ³Ù„Ù…! Ø£ÙŠ ÙˆÙ‚Øª ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£Ù†Ø§ Ù‡Ù†Ø§.',
                'â¤ï¸ Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©! Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙÙŠ Ø®Ø¯Ù…ØªÙƒ.'
            ]
        }
        
        if emotion in responses:
            return random.choice(responses[emotion])
        
        return ""
    
    def analyze_text_complexity(self, text: str) -> str:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        word_count = len(text.split())
        technical_terms = len(re.findall(r'\b(?:ØªÙ‚Ù†ÙŠ|Ø¹Ù„Ù…ÙŠ|ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§|Ø¨Ø±Ù…Ø¬Ø©|Ù‡Ù†Ø¯Ø³Ø©|Ø·Ø¨|ÙÙŠØ²ÙŠØ§Ø¡|ÙƒÙŠÙ…ÙŠØ§Ø¡|Ø±ÙŠØ§Ø¶ÙŠØ§Øª)\b', text.lower()))
        
        if word_count < 5:
            return 'simple'
        elif word_count < 15 and technical_terms == 0:
            return 'medium'
        else:
            return 'advanced'
    
    def create_enhanced_context(self, question: str, question_type: str, emotion: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙŠØ§Ù‚ Ù…Ø­Ø³Ù† Ù„Ù„Ø³Ø¤Ø§Ù„"""
        base_context = self.context_templates.get(question_type, self.context_templates['general'])
        complexity = self.analyze_text_complexity(question)
        
        # ØªØ®ØµÙŠØµ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_additions = {
            'simple': "Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ø§Ù‹ ÙˆÙ…ÙØ±Ø¯Ø§Øª Ø³Ù‡Ù„Ø©. Ù‚Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.",
            'medium': "Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ø¹ Ø´Ø±Ø­ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©.",
            'advanced': "ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù…Ø¹ ØªÙˆØ¶ÙŠØ­Ù‡Ø§ Ø¨Ø§Ù„ØªÙØµÙŠÙ„."
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©
        emotional_guidance = {
            'help_request': "ÙƒÙ† Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Ù‹ ÙˆÙ…Ø´Ø¬Ø¹Ø§Ù‹ ÙÙŠ Ø±Ø¯Ùƒ.",
            'confusion': "ÙƒÙ† ØµØ¨ÙˆØ±Ø§Ù‹ ÙˆÙˆØ¶Ø­ Ø§Ù„Ø£Ù…ÙˆØ± Ø¨Ø·Ø±Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø©.",
            'negative': "ÙƒÙ† Ù…Ø±ÙŠØ­Ø§Ù‹ ÙˆÙ…Ø·Ù…Ø¦Ù†Ø§Ù‹ ÙÙŠ Ø£Ø³Ù„ÙˆØ¨Ùƒ.",
            'positive': "Ø´Ø§Ø±Ùƒ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ø³ØªÙ…Ø± ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø£ÙØ¶Ù„."
        }
        
        enhanced_context = f"""
{base_context}

Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {complexity_additions[complexity]}

{emotional_guidance.get(emotion, "")}

Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:
- Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø© Ø«Ù… ÙØµÙ„
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ÙˆØ§Ù„ØªØ±Ù‚ÙŠÙ… Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª  
- Ø£Ø¶Ù Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
- Ø§Ø®ØªØªÙ… Ø¨Ø³Ø¤Ø§Ù„ Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø£Ùˆ Ø§Ù‚ØªØ±Ø§Ø­ Ù„Ù„ØªÙˆØ³Ø¹
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø¨Ø­ÙƒÙ…Ø© Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­ÙŠÙˆÙŠØ©
- Ù‚Ø¯Ù… Ù…Ø±Ø§Ø¬Ø¹ Ø£Ùˆ Ù…ØµØ§Ø¯Ø± Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù…ÙƒØ§Ù†
"""
        
        return enhanced_context
    
    def create_detailed_summary(self, content: str, max_sentences: int = 5) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ„Ø®ÙŠØµ Ù…ÙØµÙ„ ÙˆÙ…Ù†Ø¸Ù…"""
        if not content or len(content.strip()) < 100:
            return content
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ Ø¬Ù…Ù„
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        if len(sentences) <= max_sentences:
            return content
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© (Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø©ØŒ Ø¬Ù…Ù„ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©ØŒ Ø¢Ø®Ø± Ø¬Ù…Ù„Ø©)
        important_sentences = []
        
        # Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø¯Ø§Ø¦Ù…Ø§Ù‹
        if sentences:
            important_sentences.append(sentences[0])
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…Ù„ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = ['Ù…Ù‡Ù…', 'Ø£Ø³Ø§Ø³ÙŠ', 'Ø±Ø¦ÙŠØ³ÙŠ', 'ÙŠØ¬Ø¨', 'Ø¶Ø±ÙˆØ±ÙŠ', 'Ø£ÙˆÙ„Ø§Ù‹', 'Ø«Ø§Ù†ÙŠØ§Ù‹', 'Ø£Ø®ÙŠØ±Ø§Ù‹', 'Ø®Ù„Ø§ØµØ©', 'Ù†ØªÙŠØ¬Ø©']
        for sentence in sentences[1:-1]:
            if any(keyword in sentence for keyword in keywords) and len(important_sentences) < max_sentences - 1:
                important_sentences.append(sentence)
        
        # Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø³Ø§Ø­Ø©
        if len(sentences) > 1 and len(important_sentences) < max_sentences:
            important_sentences.append(sentences[-1])
        
        return '. '.join(important_sentences) + '.'
    
    def generate_follow_up_questions(self, topic: str, question_type: str) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ø°ÙƒÙŠØ©"""
        follow_ups = {
            'definition': [
                f'Ù‡Ù„ ØªØ±ÙŠØ¯ Ø£Ù…Ø«Ù„Ø© Ø£ÙƒØ«Ø± Ø¹Ù† {topic}ØŸ',
                f'Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ Ù…Ø¹Ø±ÙØ© ØªØ·Ø¨ÙŠÙ‚Ø§Øª {topic} Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ',
                f'Ù‡Ù„ ØªØ­Ø¨ Ø£Ù† Ù†ØªØ­Ø¯Ø« Ø¹Ù† ØªØ§Ø±ÙŠØ® {topic}ØŸ'
            ],
            'explanation': [
                f'Ù‡Ù„ Ø§Ù„Ø´Ø±Ø­ ÙˆØ§Ø¶Ø­ Ø£Ù… ØªØ­ØªØ§Ø¬ ØªÙØµÙŠÙ„ Ø£ÙƒØ«Ø± ÙÙŠ Ù†Ù‚Ø·Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ',
                f'ØªØ­Ø¨ Ù†Ø´ÙˆÙ Ø£Ù…Ø«Ù„Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù„Ù‰ {topic}ØŸ',
                f'ÙÙŠÙ‡ Ø¬Ø²Ø¡ Ù…Ø¹ÙŠÙ† Ù…Ù† Ø§Ù„Ø´Ø±Ø­ ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­ Ø£ÙƒØªØ± ÙÙŠÙ‡ØŸ'
            ],
            'mathematical': [
                'Ù‡Ù„ ØªØ±ÙŠØ¯ Ø­Ù„ Ù…Ø³Ø§Ø¦Ù„ Ù…Ø´Ø§Ø¨Ù‡Ø©ØŸ',
                'ØªØ­Ø¨ Ø£Ø´Ø±Ø­ Ù„Ùƒ Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ Ù„Ø­Ù„ Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ØŸ',
                'Ø¹Ù†Ø¯Ùƒ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù„Ù‰ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø­Ù„ØŸ'
            ],
            'general': [
                f'Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø¬Ø§Ù†Ø¨ Ù…Ø¹ÙŠÙ† Ù…Ù† {topic} ØªØ±ÙŠØ¯ Ø§Ù„ØªÙˆØ³Ø¹ ÙÙŠÙ‡ØŸ',
                'Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¥Ø¶Ø§ÙÙŠØ© ØªØ­ØªØ§Ø¬Ù‡Ø§ØŸ',
                'Ù‡Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø£Ø¬Ø§Ø¨Øª Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ØŸ'
            ]
        }
        
        return follow_ups.get(question_type, follow_ups['general'])
    
    def enhance_arabic_text(self, text: str) -> str:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ÙÙ‡Ù…"""
        if not text:
            return text
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        important_words = {
            'Ø§Ù„Ù„Ù‡': 'Ø§Ù„Ù„Ù‡Ù',
            'Ù…Ø­Ù…Ø¯': 'Ù…Ø­Ù…Ø¯ÙŒ',
            'Ø§Ù„Ù‚Ø±Ø§Ù†': 'Ø§Ù„Ù‚Ø±Ø¢Ù†',
            'Ø§Ù„Ø§Ø³Ù„Ø§Ù…': 'Ø§Ù„Ø¥Ø³Ù„Ø§Ù…'
        }
        
        enhanced_text = text
        for word, enhanced in important_words.items():
            enhanced_text = re.sub(rf'\b{word}\b', enhanced, enhanced_text, flags=re.IGNORECASE)
        
        # ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        enhanced_text = re.sub(r'([.!?])\s*', r'\1 ', enhanced_text)  # Ù…Ø³Ø§ÙØ© Ø¨Ø¹Ø¯ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        enhanced_text = re.sub(r'\s+', ' ', enhanced_text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        enhanced_text = enhanced_text.strip()
        
        return enhanced_text