"""
Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø« - Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ
ÙŠØ¯Ù…Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©
"""

import os
import time
import asyncio
from typing import Optional, Dict, Any, List

try:
    from core.multi_llm_engine import multi_llm_engine
    from core.free_architecture import free_architecture
    from core.advanced_intelligence import AdvancedIntelligence
    from core.utils import is_arabic, normalize_text
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø«Ø©")
except ImportError as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {e}")

class EnhancedAIEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©"""
    
    def __init__(self):
        self.intelligence = AdvancedIntelligence()
        self.multi_llm = multi_llm_engine
        self.architecture = free_architecture
        print("ðŸ¤– ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø«")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        self.session_stats = {
            'questions_answered': 0,
            'models_used': set(),
            'total_tokens': 0,
            'cache_hits': 0
        }
    
    def is_available(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬"""
        available_models = self.multi_llm.get_available_models()
        return len(available_models) > 0
    
    async def answer_question(self, question: str, context: str = "") -> Optional[Dict[str, Any]]:
        """Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­"""
        if not question.strip():
            return None
        
        try:
            # ÙØ­Øµ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø£ÙˆÙ„Ø§Ù‹
            cached_response = self.architecture.get_cached_response(question)
            if cached_response:
                self.session_stats['cache_hits'] += 1
                cached_response['from_cache'] = True
                return cached_response
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            analysis = self.intelligence.analyze_question(question)
            
            # Ø¨Ù†Ø§Ø¡ Ø³ÙŠØ§Ù‚ Ù…Ø­Ø³Ù†
            enhanced_context = self._build_enhanced_context(question, analysis, context)
            
            start_time = time.time()
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
            response = await self.multi_llm.generate_response(
                enhanced_context,
                context=f"ØªØ­Ù„ÙŠÙ„: {analysis.get('question_type', 'Ø¹Ø§Ù…')}",
                max_tokens=1000
            )
            
            response_time = time.time() - start_time
            
            if response['success']:
                # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
                enhanced_response = self.intelligence.enhance_response(
                    response['response'], analysis, question
                )
                
                result = {
                    'success': True,
                    'answer': enhanced_response,
                    'question_type': analysis.get('question_type', 'general'),
                    'emotional_tone': analysis.get('emotional_context', {}),
                    'enhanced': True,
                    'from_cache': False,
                    'model_used': response['model'],
                    'provider': response['provider'],
                    'tokens_used': response.get('tokens_used', 0),
                    'response_time': response_time
                }
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                self.session_stats['questions_answered'] += 1
                self.session_stats['models_used'].add(response['model'])
                self.session_stats['total_tokens'] += response.get('tokens_used', 0)
                
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                self.architecture.record_usage(
                    service=response['provider'],
                    operation='question_answer',
                    tokens_used=response.get('tokens_used', 0),
                    response_time=response_time,
                    success=True
                )
                
                # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
                self.architecture.cache_response(question, result, response['model'])
                
                return result
            else:
                # ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯
                return {
                    'success': False,
                    'answer': "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.",
                    'error': 'no_models_available'
                }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„: {e}")
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£
            self.architecture.record_usage(
                service='enhanced_ai',
                operation='question_answer_error',
                response_time=time.time() - start_time if 'start_time' in locals() else 0,
                success=False
            )
            
            return {
                'success': False,
                'answer': "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
                'error': str(e)
            }
    
    def _build_enhanced_context(self, question: str, analysis: Dict[str, Any], context: str = "") -> str:
        """Ø¨Ù†Ø§Ø¡ Ø³ÙŠØ§Ù‚ Ù…Ø­Ø³Ù† Ù„Ù„Ø³Ø¤Ø§Ù„"""
        
        # Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        enhanced_context = f"""Ø£Ù†Øª Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„:
- Ø§Ù„Ù†ÙˆØ¹: {analysis.get('question_type', 'Ø¹Ø§Ù…')}
- Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {analysis.get('emotional_context', {}).get('primary_emotion', 'Ù…Ø­Ø§ÙŠØ¯')}
- Ø§Ù„Ù„ØºØ©: {'Ø¹Ø±Ø¨ÙŠ' if is_arabic(question) else 'Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ'}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø§Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø¬Ù…ÙŠÙ„Ø©
2. ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹
3. Ø§Ø³ØªØ®Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
4. Ø§Ø®ØªØªÙ… Ø¨Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ù†Ø§Ø³Ø¨

"""
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ø¥Ù† ÙˆØ¬Ø¯
        if context:
            enhanced_context += f"Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ: {context}\n\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ø¤Ø§Ù„
        enhanced_context += f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
        
        return enhanced_context
    
    async def smart_search_enhancement(self, query: str, search_results: List[Dict]) -> Optional[str]:
        """ØªØ­Ø³ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        if not search_results:
            return None
        
        try:
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            content_summary = []
            for result in search_results[:3]:  # Ø£Ø®Ø° Ø£ÙˆÙ„ 3 Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·
                title = result.get('title', '')
                snippet = result.get('snippet', '')
                content_summary.append(f"â€¢ {title}: {snippet}")
            
            combined_content = "\n".join(content_summary)
            
            # Ø¨Ù†Ø§Ø¡ prompt Ù„Ù„ØªÙ„Ø®ÙŠØµ
            enhancement_prompt = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ù‚Ø¯Ù… Ù…Ù„Ø®ØµØ§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙÙŠØ¯Ø§Ù‹ Ø¹Ù† "{query}":

{combined_content}

Ø§ÙƒØªØ¨ Ù…Ù„Ø®ØµØ§Ù‹ Ù…ÙØµÙ„Ø§Ù‹ ÙˆÙ…Ù†Ø¸Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©."""
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†
            response = await self.multi_llm.generate_response(
                enhancement_prompt,
                max_tokens=800
            )
            
            if response['success']:
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                self.architecture.record_usage(
                    service=response['provider'],
                    operation='search_enhancement',
                    tokens_used=response.get('tokens_used', 0),
                    success=True
                )
                
                return response['response']
            
            return None
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«: {e}")
            return None
    
    def get_system_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        model_stats = self.multi_llm.get_model_stats()
        
        # ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        system_health = self.architecture.get_system_health()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø©
        session_models = list(self.session_stats['models_used'])
        
        return {
            'models': {
                'available': model_stats['total_models'],
                'free_models': model_stats['free_models'],
                'local_models': model_stats['local_models'],
                'best_model': model_stats['best_model'],
                'session_models_used': session_models
            },
            'session': self.session_stats,
            'architecture': {
                'cache_size_mb': system_health['cache']['size_mb'],
                'cache_items': system_health['cache']['items'],
                'db_size_mb': system_health['database']['size_mb']
            },
            'usage_limits': system_health['usage'],
            'system_healthy': model_stats['total_models'] > 0
        }
    
    def optimize_system(self) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©"""
        return self.architecture.optimize_for_free_hosting()

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø«ÙŠÙ„ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø«
enhanced_ai_engine = EnhancedAIEngine()