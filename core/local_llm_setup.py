"""
Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ© - Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ
Ù†Ø¸Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØªÙ†Ø²ÙŠÙ„ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
"""

import os
import subprocess
import json
import requests
from typing import Dict, List, Optional

class LocalLLMSetup:
    """Ù…Ø¯ÙŠØ± Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©"""
    
    def __init__(self):
        self.ollama_installed = self._check_ollama()
        self.available_models = [
            {
                'name': 'llama3.1:8b',
                'size': '4.7GB',
                'description': 'Meta Llama 3.1 8B - Ù†Ù…ÙˆØ°Ø¬ Ù…ØªÙ‚Ø¯Ù… ÙˆÙ…ØªÙˆØ§Ø²Ù†',
                'priority': 1,
                'supports_arabic': True
            },
            {
                'name': 'mistral:7b', 
                'size': '4.1GB',
                'description': 'Mistral 7B - Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„',
                'priority': 2,
                'supports_arabic': True
            },
            {
                'name': 'gemma2:9b',
                'size': '5.4GB', 
                'description': 'Google Gemma 2 9B - Ù…ØªØ·ÙˆØ± ÙˆÙ…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª',
                'priority': 3,
                'supports_arabic': True
            },
            {
                'name': 'qwen2:7b',
                'size': '4.4GB',
                'description': 'Qwen 2 7B - Ù…Ù…ØªØ§Ø² Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©',
                'priority': 4,
                'supports_arabic': True
            },
            {
                'name': 'phi3:mini',
                'size': '2.3GB',
                'description': 'Microsoft Phi-3 Mini - Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹',
                'priority': 5,
                'supports_arabic': False
            }
        ]
        self.installed_models = self._get_installed_models()
    
    def _check_ollama(self) -> bool:
        """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ollama Ù…Ø«Ø¨Øª"""
        try:
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except:
            return False
    
    def _get_installed_models(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø«Ø¨ØªØ©"""
        if not self.ollama_installed:
            return []
        
        try:
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                models = []
                for line in lines:
                    if line.strip():
                        model_name = line.split()[0]
                        models.append(model_name)
                return models
        except:
            pass
        
        return []
    
    def install_ollama(self) -> Dict[str, str]:
        """ØªØ«Ø¨ÙŠØª Ollama ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        if self.ollama_installed:
            return {'status': 'already_installed', 'message': 'Ollama Ù…Ø«Ø¨Øª Ø¨Ø§Ù„ÙØ¹Ù„'}
        
        try:
            # ØªÙ†Ø²ÙŠÙ„ ÙˆØªØ«Ø¨ÙŠØª Ollama
            print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø²ÙŠÙ„ Ollama...")
            
            # Ù„Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            import platform
            system = platform.system().lower()
            
            if system == 'linux':
                # ØªØ«Ø¨ÙŠØª Ollama Ø¹Ù„Ù‰ Linux
                install_cmd = 'curl -fsSL https://ollama.ai/install.sh | sh'
                result = subprocess.run(install_cmd, shell=True, capture_output=True, text=True)
                
                if result.returncode == 0:
                    self.ollama_installed = True
                    return {'status': 'installed', 'message': 'ØªÙ… ØªØ«Ø¨ÙŠØª Ollama Ø¨Ù†Ø¬Ø§Ø­'}
                else:
                    return {'status': 'error', 'message': f'ÙØ´Ù„ Ø§Ù„ØªØ«Ø¨ÙŠØª: {result.stderr}'}
            
            else:
                return {
                    'status': 'manual_required',
                    'message': f'ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª Ollama ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù…Ù† https://ollama.ai Ù„Ù„Ù†Ø¸Ø§Ù… {system}'
                }
        
        except Exception as e:
            return {'status': 'error', 'message': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ«Ø¨ÙŠØª: {e}'}
    
    def install_model(self, model_name: str) -> Dict[str, str]:
        """ØªØ«Ø¨ÙŠØª Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø¯Ø¯"""
        if not self.ollama_installed:
            install_result = self.install_ollama()
            if install_result['status'] != 'installed' and install_result['status'] != 'already_installed':
                return install_result
        
        if model_name in self.installed_models:
            return {'status': 'already_installed', 'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} Ù…Ø«Ø¨Øª Ø¨Ø§Ù„ÙØ¹Ù„'}
        
        try:
            print(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}...")
            
            # ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            result = subprocess.run(['ollama', 'pull', model_name], 
                                  capture_output=True, text=True, timeout=1800)  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
            
            if result.returncode == 0:
                self.installed_models.append(model_name)
                return {'status': 'installed', 'message': f'ØªÙ… ØªØ«Ø¨ÙŠØª {model_name} Ø¨Ù†Ø¬Ø§Ø­'}
            else:
                return {'status': 'error', 'message': f'ÙØ´Ù„ ØªØ«Ø¨ÙŠØª {model_name}: {result.stderr}'}
        
        except subprocess.TimeoutExpired:
            return {'status': 'timeout', 'message': 'Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„ØªÙ†Ø²ÙŠÙ„ - ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹'}
        except Exception as e:
            return {'status': 'error', 'message': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ«Ø¨ÙŠØª: {e}'}
    
    def auto_install_best_model(self) -> Dict[str, str]:
        """ØªØ«Ø¨ÙŠØª Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        
        # ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© (ØªÙ‚Ø±ÙŠØ¨ÙŠ)
        try:
            import shutil
            free_space_gb = shutil.disk_usage('.').free / (1024**3)
        except:
            free_space_gb = 10  # Ø§ÙØªØ±Ø§Ø¶ 10GB
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ù†Ø³Ø¨
        for model in sorted(self.available_models, key=lambda x: x['priority']):
            model_size_gb = float(model['size'].replace('GB', ''))
            
            if free_space_gb > model_size_gb + 2:  # 2GB Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø£Ù…Ø§Ù†
                if model['name'] not in self.installed_models:
                    print(f"ğŸ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model['name']} ({model['size']})")
                    return self.install_model(model['name'])
                else:
                    return {
                        'status': 'already_installed', 
                        'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„ {model["name"]} Ù…Ø«Ø¨Øª Ø¨Ø§Ù„ÙØ¹Ù„'
                    }
        
        return {'status': 'no_space', 'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³Ø§Ø­Ø© ÙƒØ§ÙÙŠØ© Ù„ØªØ«Ø¨ÙŠØª Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬'}
    
    def start_ollama_service(self) -> Dict[str, str]:
        """ØªØ´ØºÙŠÙ„ Ø®Ø¯Ù…Ø© Ollama"""
        if not self.ollama_installed:
            return {'status': 'not_installed', 'message': 'Ollama ØºÙŠØ± Ù…Ø«Ø¨Øª'}
        
        try:
            # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø®Ø¯Ù…Ø© ØªØ¹Ù…Ù„
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=5)
            
            if result.returncode == 0:
                return {'status': 'running', 'message': 'Ø®Ø¯Ù…Ø© Ollama ØªØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„'}
            else:
                # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
                subprocess.Popen(['ollama', 'serve'])
                return {'status': 'started', 'message': 'ØªÙ… ØªØ´ØºÙŠÙ„ Ø®Ø¯Ù…Ø© Ollama'}
        
        except Exception as e:
            return {'status': 'error', 'message': f'Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø©: {e}'}
    
    def test_model(self, model_name: str) -> Dict[str, str]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø¯Ø¯"""
        if model_name not in self.installed_models:
            return {'status': 'not_installed', 'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…Ø«Ø¨Øª'}
        
        try:
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
            test_prompt = "Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ"
            result = subprocess.run(['ollama', 'run', model_name, test_prompt], 
                                  capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0 and result.stdout.strip():
                return {
                    'status': 'working', 
                    'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ',
                    'response': result.stdout.strip()[:100] + "..."
                }
            else:
                return {'status': 'error', 'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} Ù„Ø§ ÙŠØ³ØªØ¬ÙŠØ¨'}
        
        except Exception as e:
            return {'status': 'error', 'message': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}'}
    
    def get_system_info(self) -> Dict[str, any]:
        """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø´Ø§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…"""
        
        # ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§Ø­Ø©
        try:
            import shutil
            total, used, free = shutil.disk_usage('.')
            disk_info = {
                'total_gb': round(total / (1024**3), 2),
                'used_gb': round(used / (1024**3), 2),
                'free_gb': round(free / (1024**3), 2)
            }
        except:
            disk_info = {'error': 'Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø±Øµ'}
        
        return {
            'ollama_installed': self.ollama_installed,
            'installed_models': self.installed_models,
            'available_models': self.available_models,
            'disk_space': disk_info,
            'recommended_model': self._get_recommended_model()
        }
    
    def _get_recommended_model(self) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙÙˆØµÙ‰ Ø¨Ù‡"""
        try:
            import shutil
            free_space_gb = shutil.disk_usage('.').free / (1024**3)
            
            for model in sorted(self.available_models, key=lambda x: x['priority']):
                model_size_gb = float(model['size'].replace('GB', ''))
                if free_space_gb > model_size_gb + 2:
                    return model['name']
            
            return None
        except:
            return self.available_models[0]['name']  # Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
local_llm_setup = LocalLLMSetup()