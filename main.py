# ============ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù‘Ù… + ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØµÙ‚) ============
# Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„ØµÙ‚ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ app = FastAPI(...)

# --- ÙˆØ§Ø±Ø¯ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³ØªÙˆØ±Ø¯Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§Ø› Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¢Ù…Ù†Ø© ---
import os, time, re
from urllib.parse import urlparse
import httpx
from fastapi import Query, Body, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse

# DuckDuckGo + Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙØ­Ø§Øª
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
try:
    from readability import Document
except Exception:
    Document = None

# Ø¨Ø¯Ø§Ø¦Ù„ Ø¢Ù…Ù†Ø© Ø¥Ù† Ù„Ù… ØªÙƒÙ† Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ù„ÙÙƒ:
try:
    summarize_text
except NameError:
    # ØªÙ„Ø®ÙŠØµ Ù…Ø¨Ø³Ù‘Ø· Ø§Ø­ØªÙŠØ§Ø·ÙŠ (ÙŠÙØ¶Ù‘ÙÙ„ Ø¯Ø§Ù„ØªÙƒ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¥Ù† ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©)
    def summarize_text(txt: str, max_sentences: int = 3) -> str:
        txt = (txt or "").strip()
        if not txt:
            return ""
        # Ø¬ÙÙ…Ù‘ÙØ¹ Ø£ÙˆÙ„ Ø¬ÙÙ…ÙÙ„Ù Ù‚ØµÙŠØ±Ø© ÙƒÙ†Ù‘Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        parts = re.split(r"[\.!\ØŸ\!]\s+", txt)
        return " ".join(parts[:max_sentences])[:600]

try:
    ensure_safe_filename
except NameError:
    def ensure_safe_filename(name: str) -> str:
        name = re.sub(r"[^\w\-.]+", "_", name or "")
        return name[:120] or f"file_{int(time.time())}"

# ØªØ£ÙƒÙ‘Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¥Ù† Ù„Ù… ØªÙƒÙ† Ù…Ø¹Ø±ÙØ©
DATA_DIR     = globals().get("DATA_DIR", "data")
FILES_DIR    = globals().get("FILES_DIR", "files")
UPLOADS_DIR  = globals().get("UPLOADS_DIR", os.path.join(FILES_DIR, "uploads"))
os.makedirs(UPLOADS_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ PDF Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¥Ù† Ù„Ù… ØªÙˆØ¬Ø¯ Ø¯Ø§Ù„ØªÙƒ
try:
    extract_pdf_text
except NameError:
    try:
        from pypdf import PdfReader
    except Exception:
        PdfReader = None
    def extract_pdf_text(path: str) -> str:
        if not PdfReader:
            return ""
        try:
            reader = PdfReader(path)
            return "\n".join((p.extract_text() or "") for p in reader.pages)
        except Exception:
            return ""

# Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ù„Ùˆ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© (Ù„Ø§ Ø¨Ø£Ø³ Ø¥Ù† Ù„Ù… ØªÙˆØ¬Ø¯)
def _rebuild_index_if_exists():
    try:
        build_index()
    except Exception:
        pass

# ------------------ Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…Ù†ØµØ§Øª ÙˆÙ†ØµØ§Ø¦Ø­ ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ------------------
PLATFORM_FILTERS = {
    "social": [
        "site:x.com", "site:twitter.com", "site:facebook.com",
        "site:instagram.com", "site:linkedin.com", "site:tiktok.com",
        "site:reddit.com", "site:snapchat.com"
    ],
    "video": [
        "site:youtube.com", "site:vimeo.com", "site:tiktok.com", "site:dailymotion.com"
    ],
    "markets": [
        "site:alibaba.com", "site:amazon.com", "site:aliexpress.com",
        "site:etsy.com", "site:ebay.com", "site:noon.com"
    ],
    "gov": [
        "site:gov", "site:gov.sa", "site:gov.ae", "site:gov.eg",
        "site:edu", "site:edu.sa", "site:edu.eg"
    ],
    "all": []  # Ø¨Ø¯ÙˆÙ† ÙÙ„ØªØ±
}

QUERY_EXPANSIONS = {
    "Ø§Ø¨Ø­Ø«": ["Ø§Ø¨Ø­Ø« Ø¹Ù†", "ØªÙ‚ØµÙ‘Ù‰", "Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†"],
    "Ù…Ù†ØµØ§Øª": ["Ø³ÙˆØ´Ø§Ù„ Ù…ÙŠØ¯ÙŠØ§", "ØªÙˆØ§ØµÙ„ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ"],
    "Ø³Ø¹Ø±": ["Ø«Ù…Ù†", "ØªÙƒÙ„ÙØ©"],
}

def _expand_query_text(q: str):
    vars_ = {q}
    for k, alts in QUERY_EXPANSIONS.items():
        if k in q:
            for a in alts:
                vars_.add(q.replace(k, a))
    return list(vars_)

# ------------------ Ø¨Ø­Ø« Ø¹Ø§Ù… Ø¹Ø¨Ø± DuckDuckGo ------------------
def _web_search_basic(q: str, limit: int = 8):
    try:
        with DDGS() as ddgs:
            out = []
            for r in ddgs.text(q, region="xa-ar", safesearch="off", max_results=limit):
                out.append({
                    "title":   r.get("title", ""),
                    "link":    r.get("href", ""),
                    "snippet": r.get("body", ""),
                })
            return out
    except Exception:
        return []

# ------------------ Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù‘Ù… (ÙÙ„ØªØ±Ø© Ù…Ù†ØµØ§Øª) ------------------
def _deep_search(q: str, mode: str = "all", per_site: int = 4, max_total: int = 30):
    domains = PLATFORM_FILTERS.get(mode, [])
    # Ù„Ø§ ÙÙ„Ø§ØªØ± â†’ Ø¨Ø­Ø« Ø¹Ø§Ù… Ù‚ÙˆÙŠ + Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª + ØªÙ„Ø®ÙŠØµ
    if not domains:
        hits = []
        for v in _expand_query_text(q):
            hits += _web_search_basic(v, limit=10)
        seen, out = set(), []
        for h in hits:
            link = h.get("link")
            if not link or link in seen:
                continue
            seen.add(link)
            out.append(h)
        for h in out:
            h["summary"] = summarize_text(h.get("snippet", ""), 2)
        return out[:max_total]

    results, seen = [], set()
    try:
        with DDGS() as ddgs:
            for dom in domains:
                query = f"{q} {dom}"
                for r in ddgs.text(query, region="xa-ar", safesearch="off", max_results=per_site):
                    link = r.get("href", "")
                    if not link or link in seen:
                        continue
                    seen.add(link)
                    results.append({
                        "title":   r.get("title", ""),
                        "link":    link,
                        "snippet": r.get("body", ""),
                        "domain":  dom.replace("site:", ""),
                    })
                    if len(results) >= max_total:
                        break
                if len(results) >= max_total:
                    break
    except Exception:
        pass
    for r in results:
        r["summary"] = summarize_text(r.get("snippet", ""), 2)
    return results

# ------------------ ØªÙ†Ø²ÙŠÙ„/ØªÙ„Ø®ÙŠØµ Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯ ------------------
async def _fetch_and_digest(url: str) -> dict:
    """
    - PDF: ÙŠÙØ­ÙÙØ¸ ÙÙŠ /files/uploads ÙˆÙŠÙÙÙ‡Ø±Ø³ Ù†ØµÙ‘Ù‡ (RAG) Ø¥Ù† Ø£Ù…ÙƒÙ†.
    - HTML: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…Ù‚Ø±ÙˆØ¡ ÙˆØªÙ„Ø®ÙŠØµÙ‡.
    - ØºÙŠØ± Ø°Ù„Ùƒ: ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù ÙƒÙ…Ø§ Ù‡Ùˆ ÙÙŠ /files/uploads.
    """
    headers = {"User-Agent": "Mozilla/5.0 (BassamBot; +https://render.com)"}
    timeout = httpx.Timeout(20.0, connect=10.0)
    async with httpx.AsyncClient(headers=headers, timeout=timeout, follow_redirects=True) as client:
        resp = await client.get(url)
        resp.raise_for_status()
        content_type = (resp.headers.get("content-type") or "").lower()

        # PDF
        if "application/pdf" in content_type or url.lower().endswith(".pdf"):
            safe = ensure_safe_filename(os.path.basename(urlparse(url).path) or f"doc_{int(time.time())}.pdf")
            dest = os.path.join(UPLOADS_DIR, safe)
            with open(dest, "wb") as f:
                f.write(resp.content)
            txt = extract_pdf_text(dest)
            indexed = False
            if txt.strip():
                txt_name = safe.rsplit(".", 1)[0] + ".txt"
                with open(os.path.join(DATA_DIR, txt_name), "w", encoding="utf-8") as f:
                    f.write(txt)
                _rebuild_index_if_exists()
                indexed = True
            return {"kind": "pdf", "file_url": f"/files/uploads/{safe}", "indexed": indexed}

        # HTML
        text = resp.text or ""
        if "text/html" in content_type or "<html" in text.lower():
            try:
                if Document:
                    doc = Document(text)
                    txt = BeautifulSoup(doc.summary(), "html.parser").get_text(" ", strip=True)
                else:
                    txt = BeautifulSoup(text, "html.parser").get_text(" ", strip=True)
            except Exception:
                txt = BeautifulSoup(text, "html.parser").get_text(" ", strip=True)
            return {"kind": "html", "summary": summarize_text(txt, 4)}

        # Ù…Ù„Ù Ø¢Ø®Ø±
        safe = ensure_safe_filename(os.path.basename(urlparse(url).path) or f"file_{int(time.time())}")
        dest = os.path.join(UPLOADS_DIR, safe)
        with open(dest, "wb") as f:
            f.write(resp.content)
        return {"kind": "file", "file_url": f"/files/uploads/{safe}"}

# ------------------ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ------------------

@app.get("/search")
def search_endpoint(
    q: str = Query(..., description="Ù†Øµ Ø§Ù„Ø¨Ø­Ø«"),
    mode: str = "all",
    per_site: int = 4,
    max_total: int = 30
):
    """
    Ø£Ù…Ø«Ù„Ø©:
      /search?q=Ø§Ø³Ù…+Ø§Ù„Ø´Ø®Øµ&mode=social
      /search?q=ÙƒØ§Ù…ÙŠØ±Ø§+Ø³ÙˆÙ†ÙŠ&mode=markets
      /search?q=Ù‚Ø§Ù†ÙˆÙ†+Ù…Ø±ÙˆØ±&mode=gov
      /search?q=Ø´Ø±Ø­+Ù…Ø´ÙƒÙ„Ø©+Ø¬ÙˆØ§Ù„&mode=video
    """
    q = (q or "").strip()
    if not q:
        return {"type": "search", "results": []}
    results = _deep_search(q, mode=mode, per_site=per_site, max_total=max_total)
    if results:
        return {"type": "chat", "answer": "Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ğŸ‘‡", "sources": results}
    return {"type": "chat", "answer": "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ø¶Ø­Ø©ØŒ Ø¬Ø±Ù‘Ø¨ ÙˆØµÙÙ‹Ø§ Ø£Ø¯Ù‚."}

@app.post("/fetch_url")
async def fetch_url(payload: dict = Body(...)):
    """
    ØªÙ†Ø²ÙŠÙ„ Ø±Ø§Ø¨Ø· (HTML/PDF/Ù…Ù„Ù) Ø«Ù… ØªÙ„Ø®ÙŠØµÙ‡/ÙÙ‡Ø±Ø³ØªÙ‡.
    JSON: {"url": "https://..."}
    """
    url = (payload.get("url") or "").strip()
    if not url:
        raise HTTPException(400, "Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ…Ø±ÙŠØ± url")
    try:
        info = await _fetch_and_digest(url)
        return {"ok": True, **info}
    except Exception as e:
        return {"ok": False, "error": str(e)}
# ======================= Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ =======================
