# === Ø¥Ø¶Ù€Ù€Ù€Ù€Ø§ÙØ§Øª Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù ===
import httpx
from urllib.parse import urlparse

# Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…Ù†ØµÙ‘Ø§Øª (ØªØ³ØªØ¹Ù…Ù„ site: Ù„ØªÙˆØ¬ÙŠÙ‡ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø«)
PLATFORM_FILTERS = {
    "social": [
        "site:x.com", "site:twitter.com", "site:facebook.com", "site:instagram.com",
        "site:linkedin.com", "site:tiktok.com", "site:reddit.com", "site:snapchat.com"
    ],
    "video": [
        "site:youtube.com", "site:vimeo.com", "site:tiktok.com", "site:dailymotion.com"
    ],
    "markets": [
        "site:alibaba.com", "site:amazon.com", "site:aliexpress.com",
        "site:etsy.com", "site:ebay.com", "site:noon.com"
    ],
    "gov": [
        "site:gov", "site:gov.sa", "site:gov.ae", "site:gov.eg",
        "site:edu", "site:edu.sa", "site:edu.eg"
    ],
    "all": []  # Ø¨Ø¯ÙˆÙ† ÙÙ„Ø§ØªØ± Ø®Ø§ØµØ©
}

# ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù…Ø±Ø§Ø¯ÙØ§Øª Ø¨Ø³ÙŠØ·Ø© (ØªÙ‚Ø¯Ø± ØªÙˆØ³Ù‘Ø¹Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§)
QUERY_EXPANSIONS = {
    "Ø§Ø¨Ø­Ø«": ["Ø§Ø¨Ø­Ø« Ø¹Ù†", "ØªÙ‚ØµÙ‘Ù‰", "Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†"],
    "Ù…Ù†ØµØ§Øª": ["Ø³ÙˆØ´Ø§Ù„ Ù…ÙŠØ¯ÙŠØ§", "ØªÙˆØ§ØµÙ„ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ"],
    "Ø³Ø¹Ø±": ["Ø«Ù…Ù†", "ØªÙƒÙ„ÙØ©"]
}

def expand_query_text(q: str) -> list[str]:
    vars = {q}
    for k, alts in QUERY_EXPANSIONS.items():
        if k in q:
            for a in alts:
                vars.add(q.replace(k, a))
    return list(vars)

def web_search_basic(q: str, limit: int = 8):
    """Ø¨Ø­Ø« Ø¹Ø§Ù… Ø¹Ø¨Ø± DuckDuckGo (Ù†Ø¸ÙŠÙ ÙˆÙ…Ø¨Ø§Ø´Ø±)."""
    try:
        with DDGS() as ddgs:
            out = []
            for r in ddgs.text(q, region="xa-ar", safesearch="off", max_results=limit):
                out.append({
                    "title": r.get("title", ""),
                    "link":  r.get("href",  ""),
                    "snippet": r.get("body", "")
                })
            return out
    except Exception:
        return []

def deep_search(q: str, mode: str = "all", per_site: int = 4, max_total: int = 30):
    """
    Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù‘Ù…: ÙŠÙ„Ù Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø­Ø¯Ø¯Ø© (Ø¨ÙˆØ§Ø³Ø·Ø© site:) ÙˆÙŠØ¬Ù…Ø¹ Ù†ØªØ§Ø¦Ø¬ ÙƒØ«ÙŠØ±Ø©.
    mode âˆˆ {'all','social','video','markets','gov'}
    """
    domains = PLATFORM_FILTERS.get(mode, [])
    # Ù„Ùˆ Ù…Ø§ ÙÙŠ ÙÙ„Ø§ØªØ±ØŒ Ù†Ø¹Ù…Ù„ Ø¨Ø­Ø« Ø¹Ø§Ù… Ù‚ÙˆÙŠ
    if not domains:
        hits = []
        for v in expand_query_text(q):
            hits += web_search_basic(v, limit=10)
        # Ø£Ø²Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        seen, out = set(), []
        for h in hits:
            link = h.get("link")
            if not link or link in seen: 
                continue
            seen.add(link)
            out.append(h)
        # Ø§Ù„Ù‚ØµÙ‘ ÙˆÙ„Ø®Ù‘Øµ
        for h in out:
            h["summary"] = summarize_text(h.get("snippet", ""), 2)
        return out[:max_total]

    # Ù…Ø¹ ÙÙ„Ø§ØªØ± Ù…Ù†ØµÙ‘Ø§Øª
    results, seen = [], set()
    try:
        with DDGS() as ddgs:
            for dom in domains:
                query = f'{q} {dom}'
                for r in ddgs.text(query, region="xa-ar", safesearch="off", max_results=per_site):
                    link = r.get("href", "")
                    if not link or link in seen:
                        continue
                    seen.add(link)
                    results.append({
                        "title": r.get("title", ""),
                        "link":  link,
                        "snippet": r.get("body", ""),
                        "domain": dom.replace("site:", "")
                    })
                    if len(results) >= max_total:
                        break
                if len(results) >= max_total:
                    break
    except Exception:
        pass

    for r in results:
        r["summary"] = summarize_text(r.get("snippet", ""), 2)
    return results

async def fetch_and_digest(url: str) -> dict:
    """
    ØªÙ†Ø²ÙŠÙ„ URL Ø¨Ø£Ù…Ø§Ù†:
      - Ù„Ùˆ PDF: Ù†Ø­ÙØ¸Ù‡ ÙÙŠ /files/uploads ÙˆÙ†Ø¯Ø®Ù‘Ù„Ù‡ ÙÙ‡Ø±Ø³ RAG
      - Ù„Ùˆ HTML: Ù†Ù„Ø®Ù‘Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡
    """
    headers = {"User-Agent": "Mozilla/5.0 (BassamBot; +https://render.com)"}
    timeout = httpx.Timeout(20.0, connect=10.0)
    async with httpx.AsyncClient(headers=headers, timeout=timeout, follow_redirects=True) as client:
        resp = await client.get(url)
        ct = (resp.headers.get("content-type") or "").lower()

        # PDFØŸ
        if "application/pdf" in ct or url.lower().endswith(".pdf"):
            safe = ensure_safe_filename(os.path.basename(urlparse(url).path) or f"doc_{int(time.time())}.pdf")
            dest = os.path.join(UPLOADS_DIR, safe)
            with open(dest, "wb") as f:
                f.write(resp.content)
            txt = extract_pdf_text(dest)
            if txt.strip():
                txt_name = safe.rsplit(".", 1)[0] + ".txt"
                with open(os.path.join(DATA_DIR, txt_name), "w", encoding="utf-8") as f:
                    f.write(txt)
                build_index()
            return {"kind": "pdf", "file_url": f"/files/uploads/{safe}", "indexed": bool(txt.strip())}

        # HTMLØŸ
        if "text/html" in ct or "<html" in resp.text.lower():
            html = resp.text
            # readability â†’ Ù†Øµ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ„Ø®ÙŠØµ
            try:
                doc = Document(html)
                txt = BeautifulSoup(doc.summary(), "html.parser").get_text(" ", strip=True)
            except Exception:
                txt = BeautifulSoup(html, "html.parser").get_text(" ", strip=True)
            return {"kind": "html", "summary": summarize_text(txt, 4)}

        # Ù†ÙˆØ¹ Ø¢Ø®Ø± (Ù†Ù†Ø²Ù‘Ù„Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ ÙÙ‚Ø·)
        safe = ensure_safe_filename(os.path.basename(urlparse(url).path) or f"file_{int(time.time())}")
        dest = os.path.join(UPLOADS_DIR, safe)
        with open(dest, "wb") as f:
            f.write(resp.content)
        return {"kind": "file", "file_url": f"/files/uploads/{safe}"}

# ====== Ù…Ø³Ø§Ø±Ø§Øª (Endpoints) ======

@app.get("/search")
def search_endpoint(q: str = Query(..., description="Ù†Øµ Ø§Ù„Ø¨Ø­Ø«"), mode: str = "all",
                    per_site: int = 4, max_total: int = 30):
    """
    Ø£Ù…Ø«Ù„Ø©:
      /search?q=Ø§Ø³Ù…+Ø§Ù„Ø´Ø®Øµ&mode=social
      /search?q=ÙƒØ§Ù…ÙŠØ±Ø§+Ø³ÙˆÙ†ÙŠ&mode=markets
      /search?q=Ù‚Ø§Ù†ÙˆÙ†+Ù…Ø±ÙˆØ±&mode=gov
      /search?q=Ø´Ø±Ø­+Ù…Ø´ÙƒÙ„Ø©+Ø¬ÙˆØ§Ù„&mode=video
    """
    q = (q or "").strip()
    if not q:
        return {"type": "search", "results": []}
    results = deep_search(q, mode=mode, per_site=per_site, max_total=max_total)
    # Ø±Ø¯ Ù…ÙˆØ­Ù‘Ø¯ (Ù†ÙØ³ Ø´ÙƒÙ„ ÙÙ‚Ø§Ø¹Ø© Ø¨Ø³Ù‘Ø§Ù…)
    if results:
        return {"type": "chat", "answer": "Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ğŸ‘‡", "sources": results}
    return {"type": "chat", "answer": "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ø¶Ø­Ø©ØŒ Ø¬Ø±Ù‘Ø¨ ÙˆØµÙÙ‹Ø§ Ø£Ø¯Ù‚."}

@app.post("/fetch_url")
async def fetch_url(payload: Dict[str, Any] = Body(...)):
    """
    ØªÙ†Ø²ÙŠÙ„ Ø±Ø§Ø¨Ø· (HTML/PDF/Ù…Ù„Ù) ÙˆØªÙ„Ø®ÙŠØµÙ‡ Ø£Ùˆ ÙÙ‡Ø±Ø³ØªÙ‡.
    JSON: {"url": "https://..."}
    """
    url = (payload.get("url") or "").strip()
    if not url:
        raise HTTPException(400, "Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ…Ø±ÙŠØ± url")
    try:
        info = await fetch_and_digest(url)
        return {"ok": True, **info}
    except Exception as e:
        return {"ok": False, "error": str(e)}
