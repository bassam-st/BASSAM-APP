# main.py â€” Ø¨Ø³Ù‘Ø§Ù… v4.1 (Ø¹Ø±Ø¨ÙŠ/ÙÙØ²Ù‘ÙŠ + Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù‘Ù… + Ø³ÙˆØ´Ø§Ù„ + Ø±ÙØ¹ PDF + RAG + ÙˆÙŠÙƒÙŠ + Ø±ÙŠØ§Ø¶ÙŠØ§Øª)
from fastapi import FastAPI, Request, Query, Body, UploadFile, File
from fastapi.responses import HTMLResponse, RedirectResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware

import os, json, time, re, unicodedata, uuid
from typing import List, Dict, Any

# Ù†Øµ/ÙˆÙŠØ¨
from duckduckgo_search import DDGS
try:
    from sumy.parsers.text import PlaintextParser
except Exception:
    from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
import wikipedia

# Ø±ÙŠØ§Ø¶ÙŠØ§Øª
from sympy import symbols, sympify, diff, integrate, simplify

# RAG
from rank_bm25 import BM25Okapi

# ÙÙØ²Ù‘ÙŠ
from rapidfuzz.fuzz import partial_ratio

# PDF
from pdfminer.high_level import extract_text as pdf_extract_text

APP_VER = "4.1"

DATA_DIR     = "data"
NOTES_DIR    = os.path.join(DATA_DIR, "notes")
UPLOADS_DIR  = os.path.join(DATA_DIR, "uploads")
DERIVED_DIR  = os.path.join(DATA_DIR, "derived")
LEARN_PATH   = os.path.join(NOTES_DIR, "learned.jsonl")
USAGE_PATH   = os.path.join(DATA_DIR, "usage_stats.json")

# ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± "Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…ÙˆØ³Ù‘Ø¹" (Ø¨Ø­Ø« Ø£Ø¹Ù…Ù‚ Ø¶Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·)
PROTECTED_PASS = "093589"

app = FastAPI(title="Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ ğŸ¤–", version=APP_VER)

# Static/Templates
try:
    app.mount("/static", StaticFiles(directory="static"), name="static")
    templates = Jinja2Templates(directory="templates")
except Exception:
    templates = None

# Ù…Ø¬Ù„Ø¯ data Ù…ØªØ§Ø­ Ù„Ù„ØªÙ†Ø²ÙŠÙ„/Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
app.mount("/files", StaticFiles(directory=DATA_DIR), name="files")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

# ===== ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª =====
def _ensure_dirs():
    for p in [DATA_DIR, NOTES_DIR, UPLOADS_DIR, DERIVED_DIR]:
        os.makedirs(p, exist_ok=True)
    if not os.path.exists(LEARN_PATH):
        open(LEARN_PATH, "a", encoding="utf-8").close()
    if not os.path.exists(USAGE_PATH):
        with open(USAGE_PATH, "w", encoding="utf-8") as f:
            json.dump({"requests": 0, "last_time": int(time.time())}, f)
_ensure_dirs()

# ===== ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ + ÙÙØ²Ù‘ÙŠ =====
AR_DIAC = re.compile(r"[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]")
def normalize_ar(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKC", s)
    s = AR_DIAC.sub("", s)
    s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§")
    s = s.replace("Ù‰","ÙŠ").replace("Ø©","Ù‡").replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ")
    s = s.replace("Ù ","0").replace("Ù¡","1").replace("Ù¢","2").replace("Ù£","3").replace("Ù¤","4")
    s = s.replace("Ù¥","5").replace("Ù¦","6").replace("Ù§","7").replace("Ù¨","8").replace("Ù©","9")
    s = re.sub(r"\s+"," ", s)
    return s.strip().lower()

def is_like(text: str, keywords: List[str], threshold: int = 80) -> bool:
    t = normalize_ar(text)
    return any(partial_ratio(t, normalize_ar(k)) >= threshold for k in keywords)

# ===== ØªÙ„Ø®ÙŠØµ =====
def summarize_text(text: str, max_sentences: int = 3) -> str:
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("arabic"))
        summ = TextRankSummarizer()
        sents = summ(parser.document, max_sentences)
        return " ".join(str(s) for s in sents) if sents else text[:400]
    except Exception:
        return text[:400]

# ===== Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù‘Ù… (DuckDuckGo) + Ø³ÙˆØ´Ø§Ù„ =====
SOCIAL_SITES = {
    "ØªÙˆÙŠØªØ±": ["x.com", "twitter.com"], "x": ["x.com","twitter.com"],
    "Ø§Ù†Ø³ØªÙ‚Ø±Ø§Ù…": ["instagram.com"], "Ø§Ù†Ø³ØªØºØ±Ø§Ù…": ["instagram.com"], "instagram": ["instagram.com"],
    "ÙÙŠØ³Ø¨ÙˆÙƒ": ["facebook.com"], "facebook": ["facebook.com"],
    "ÙŠÙˆØªÙŠÙˆØ¨": ["youtube.com"], "youtube": ["youtube.com"],
    "Ù„ÙŠÙ†ÙƒØ¯Ø§Ù†": ["linkedin.com"], "Ù„ÙŠÙ†ÙƒØ¯Ø¥Ù†": ["linkedin.com"], "linkedin": ["linkedin.com"],
    "ØªÙŠÙƒ ØªÙˆÙƒ": ["tiktok.com"], "tiktok": ["tiktok.com"],
    "Ø±Ø¯ÙŠØª": ["reddit.com"], "reddit": ["reddit.com"],
    "ØªÙ„ØºØ±Ø§Ù…": ["t.me","telegram.me","telegram.org"],
    "Ø³Ù†Ø§Ø¨": ["snapchat.com"], "snapchat": ["snapchat.com"],
    "ØªØ§Ù†Ø¬Ùˆ": ["tango.me","tango.me/en"],  # ØªØºØ·ÙŠØ© ØªÙ‚Ø±ÙŠØ¨ÙŠØ©
}

INTENT_SEARCH = ["Ø§Ø¨Ø­Ø«", "Ø¨Ø­Ø«", "Ø¯ÙˆØ±", "Ø§Ø³ØªØ¹Ù„Ù…", "ÙØªØ´", "search", "look up", "find", "google"]
GOV_HINTS = ["Ø­ÙƒÙˆÙ…ÙŠ", "ÙˆØ²Ø§Ø±Ø©", "gov", "government"]
EDU_HINTS = ["ØªØ¹Ù„ÙŠÙ…ÙŠ", "Ø¬Ø§Ù…Ø¹Ø©", "Ù…Ø¯Ø±Ø³Ù‡", "edu", "education"]

def ddg_search(query: str, site_domains: List[str] = None, max_results: int = 8, exact_phrases: List[str] = None):
    q = query.strip()
    if exact_phrases:
        for ph in exact_phrases:
            ph = ph.strip()
            if ph:
                q += f' "{ph}"'
    if site_domains:
        site_filter = " OR ".join([f"site:{d}" for d in site_domains])
        q = f"({q}) ({site_filter})"
    try:
        with DDGS() as ddgs:
            res = ddgs.text(q, region="xa-ar", max_results=max_results)
            out = [{"title": r.get("title",""), "link": r.get("href",""), "snippet": r.get("body","")} for r in res]
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
            seen, uniq = set(), []
            for it in out:
                if it["link"] in seen: continue
                seen.add(it["link"]); uniq.append(it)
            return uniq[:max_results]
    except Exception:
        return []

def smart_search_router(q: str, deep: bool = False):
    """
    deep=True Ø¹Ù†Ø¯ ØªÙØ¹ÙŠÙ„ ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø±: ÙŠØ²ÙŠØ¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ + ÙŠØ¶ÙŠÙ Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ù‚ØªØ¨Ø³Ø© Ù„Ø±ÙØ¹ Ø§Ù„Ø¯Ù‚Ø©.
    (Ù…Ø§ ÙŠØ²Ø§Ù„ Ø¶Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·.)
    """
    qn = normalize_ar(q)
    if qn.startswith("Ø§Ø¨Ø­Ø«") or is_like(qn, INTENT_SEARCH, 82):
        chosen = []
        for key, domains in SOCIAL_SITES.items():
            if key in qn or key in q.lower():
                chosen += domains
        if any(h in qn for h in GOV_HINTS):
            chosen.append(".gov")
        if any(h in qn for h in EDU_HINTS):
            chosen.append(".edu")
        # ÙˆØ¶Ø¹ Ù…ÙˆØ³Ù‘Ø¹: Ù†ØªØ§Ø¦Ø¬ Ø£ÙƒØ«Ø± + Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ø§Ø³Ù… Ø¨ÙŠÙ† ""
        maxr = 12 if deep else 8
        phrases = []
        # Ø§Ù„ØªÙ‚Ø· Ø§Ù„Ø§Ø³Ù… Ø¨Ø¹Ø¯ ÙƒÙ„Ù…Ø© "Ø¹Ù†"
        m = re.search(r"Ø¹Ù†\s+(.+)", q)
        if m:
            phrases = [m.group(1).strip()]
        results = ddg_search(q, chosen or None, max_results=maxr, exact_phrases=phrases if deep else None)
        for item in results:
            item["summary"] = summarize_text(item.get("snippet",""), 2)
        return {"type": "search", "query": q, "domains": chosen, "results": results}
    return None

# Ø¨Ø­Ø« Ø§Ø³Ù… Ø¹Ø¨Ø± Ù…Ù†ØµÙ‘Ø§Øª Ø§Ù„Ø³ÙˆØ´Ø§Ù„ (ØªØ¬Ù…ÙŠØ¹ Ø³Ø±ÙŠØ¹ Ø¨Ø§Ù„Ù€ site:)
def social_name_search(name: str, deep: bool = False):
    domains = []
    for dlist in SOCIAL_SITES.values():
        domains += dlist
    maxr = 15 if deep else 8
    res = ddg_search(name, domains, max_results=maxr, exact_phrases=[name] if deep else None)
    for item in res:
        item["summary"] = summarize_text(item.get("snippet",""), 2)
    return {"type":"search","query":name,"domains":domains,"results":res}

# ===== ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§: ØªØ¹Ø±ÙŠÙØ§Øª + Ø¹ÙˆØ§ØµÙ… =====
def wiki_define(term: str):
    term = (term or "").strip()
    for lang in ["ar", "en"]:
        try:
            wikipedia.set_lang(lang)
            page = wikipedia.page(term, auto_suggest=True, redirect=True)
            return {"lang": lang, "term": term, "summary": page.summary[:800], "source": page.url}
        except Exception:
            continue
    return None

CAPITAL_PATTERNS = [
    r"^Ù…Ø§ ?Ù‡ÙŠ?\s+Ø¹Ø§ØµÙ…Ø©\s+(.+)$",
    r"^Ø¹Ø§ØµÙ…Ø©\s+(.+)$",
    r"^what\s+is\s+the\s+capital\s+of\s+(.+)\??$",
]
def detect_capital_query(q: str):
    qq = q.strip().lower()
    for pat in CAPITAL_PATTERNS:
        m = re.match(pat, qq, flags=re.IGNORECASE)
        if m: return m.group(1).strip(" ØŸØŸ!.:ØŒ")
    return None

def wiki_capital(country: str):
    for lang in ["ar", "en"]:
        try:
            wikipedia.set_lang(lang)
            page = wikipedia.page(country, auto_suggest=True, redirect=True)
            summary = page.summary[:1200]
            src = page.url
            if lang == "ar":
                m = re.search(r"(?:Ø¹Ø§ØµÙ…ØªÙ‡Ø§|Ø§Ù„Ø¹Ø§ØµÙ…Ø©(?:\s*Ù‡ÙŠ)?)\s+([^\.\ØŒ\n]+)", summary)
                if m: return {"country": country, "capital": m.group(1).strip(), "source": src, "summary": summary}
            else:
                m = re.search(r"capital(?:\s+is|:)?\s+([^\.\,\n]+)", summary, flags=re.I)
                if m: return {"country": country, "capital": m.group(1).strip(), "source": src, "summary": summary}
        except Exception:
            continue
    return None

# ===== RAG (BM25) + Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§ØªÙƒ (PDF Ø§Ø³ØªØ®Ø±Ø§Ø¬) =====
def _tokenize_ar(s: str) -> List[str]:
    return re.findall(r"[\w\u0600-\u06FF]+", s.lower())

def _read_documents() -> List[Dict[str, str]]:
    docs = []
    for root, _, files in os.walk(DATA_DIR):
        for fn in files:
            path = os.path.join(root, fn)
            try:
                if fn.endswith(".txt") or fn.endswith(".md"):
                    with open(path, "r", encoding="utf-8", errors="ignore") as f:
                        docs.append({"file": path, "text": f.read()})
                elif fn.endswith(".pdf"):
                    derived_txt = os.path.join(DERIVED_DIR, fn + ".txt")
                    if not os.path.exists(derived_txt) or os.path.getmtime(derived_txt) < os.path.getmtime(path):
                        try:
                            txt = pdf_extract_text(path) or ""
                            with open(derived_txt, "w", encoding="utf-8") as out:
                                out.write(txt)
                        except Exception:
                            pass
                    if os.path.exists(derived_txt):
                        with open(derived_txt, "r", encoding="utf-8", errors="ignore") as f:
                            docs.append({"file": derived_txt, "text": f.read()})
            except:
                pass
    # Ø¨Ù†Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ
    try:
        with open(LEARN_PATH, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip(): continue
                obj = json.loads(line)
                text = f"Ø³: {obj.get('question','')}\nØ¬: {obj.get('answer','')}\nÙˆØ³ÙˆÙ…:{','.join(obj.get('tags',[]))}"
                docs.append({"file": "learned", "text": text})
    except:
        pass
    return docs

BM25_INDEX = None
BM25_CORPUS = []
BM25_DOCS: List[Dict[str, str]] = []
def build_index():
    global BM25_INDEX, BM25_CORPUS, BM25_DOCS
    BM25_DOCS = _read_documents()
    BM25_CORPUS = [_tokenize_ar(d["text"]) for d in BM25_DOCS]
    BM25_INDEX = BM25Okapi(BM25_CORPUS) if BM25_CORPUS else None
    return len(BM25_DOCS)
build_index()

def rag_bm25(query: str, k: int = 3):
    if not BM25_INDEX: return []
    toks = _tokenize_ar(query)
    scores = BM25_INDEX.get_scores(toks)
    pairs = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:k]
    results = []
    for idx, sc in pairs:
        if sc < 1.0: continue
        doc = BM25_DOCS[idx]
        results.append({"file": doc["file"], "score": float(sc), "snippet": doc["text"][:800]})
    return results

# ===== Ø±ÙŠØ§Ø¶ÙŠØ§Øª =====
def solve_math(expr: str):
    try:
        x = symbols('x')
        parsed = sympify(expr)
        return {
            "input": str(parsed),
            "simplified": str(simplify(parsed)),
            "derivative": str(diff(parsed, x)),
            "integral": str(integrate(parsed, x)),
        }
    except Exception as e:
        return {"error": f"ØªØ¹Ø°Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {e}"}

# ===== Ø³Ø¬Ù„Ø§Øª/ØªØ¹Ù„Ù… + Ø£Ø³Ù„ÙˆØ¨ Ù…Ø±Ø­ =====
FUN_LINES = [
    "Ø¹Ù„Ù‰ Ø±Ø§Ø³ÙŠ ÙŠØ§ ÙÙ‡ÙŠÙ…! ğŸ¤“",
    "Ø£Ù‡Ø§! Ø§Ù„Ø¯Ù…Ø§Øº Ø¨Ø¯Ø£ ÙŠØ³Ø®Ù‘Ù† ğŸ”¥",
    "Ø®Ù„Ù‘Ù†ÙŠ Ø£Ù„Ø¨Ø³ Ù†Ø¸Ø§Ø±Ø© Ø§Ù„Ø¹Ø¨Ù‚Ø±ÙŠØ© ğŸ¤“âœ¨",
    "ÙŠØ§ Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„! ğŸš€",
]
def fun_wrap(text: str, mood: str) -> str:
    if mood == "plain":  # Ø¨Ø¯ÙˆÙ† Ù…Ø²Ø§Ø­
        return text
    add = FUN_LINES[int(time.time()) % len(FUN_LINES)]
    return f"{add}\n\n{text}"

def log_usage():
    try:
        with open(USAGE_PATH, "r+", encoding="utf-8") as f:
            data = json.load(f)
            data["requests"] = int(data.get("requests", 0)) + 1
            data["last_time"] = int(time.time())
            f.seek(0); json.dump(data, f); f.truncate()
    except Exception:
        pass

def save_feedback(question: str, answer: str, tags: List[str]):
    record = {"time": int(time.time()), "question": question.strip(), "answer": answer.strip(), "tags": tags or []}
    with open(LEARN_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

# ===== ØµÙØ­Ø§Øª =====
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    if templates and os.path.exists("templates/index.html"):
        return templates.TemplateResponse("index.html", {"request": request, "version": APP_VER})
    html = """<!doctype html><meta charset='utf-8'><title>Bassam</title>
    <h3>Ø¨Ø³Ù‘Ø§Ù… v4.1</h3><p>Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø§Ø±ÙØ¹ templates/index.html</p>"""
    return HTMLResponse(html)

@app.get("/chatui")
def chat_alias():
    return RedirectResponse("/")

@app.get("/healthz")
def healthz():
    return {"status":"ok","version":APP_VER,"docs_indexed":len(BM25_DOCS)}

# ===== Ø±ÙØ¹ Ù…Ù„ÙØ§Øª + ÙÙ‡Ø±Ø³Ø© =====
@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    fn = file.filename or f"file-{uuid.uuid4().hex}"
    safe = re.sub(r"[^A-Za-z0-9_.\\-]+","_", fn)
    path = os.path.join(UPLOADS_DIR, safe)
    with open(path, "wb") as f:
        f.write(await file.read())

    if safe.lower().endswith(".pdf"):
        try:
            txt = pdf_extract_text(path) or ""
            out = os.path.join(DERIVED_DIR, safe + ".txt")
            with open(out, "w", encoding="utf-8") as o:
                o.write(txt)
        except Exception:
            pass

    n = build_index()
    return {"ok": True, "saved": f"/files/uploads/{safe}", "indexed_docs": n}

@app.get("/list-files")
def list_files():
    out = []
    for root, _, files in os.walk(DATA_DIR):
        for fn in files:
            rel = os.path.relpath(os.path.join(root, fn), DATA_DIR)
            out.append(f"/files/{rel.replace('\\', '/')}")
    return {"files": sorted(out)}

# ===== Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© =====
GREET = ["Ù…Ø±Ø­Ø¨Ø§","Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…","Ø§Ù‡Ù„Ø§","ÙƒÙŠÙ Ø§Ù„Ø­Ø§Ù„","Ù‡Ø§ÙŠ","hello","hi"]

@app.get("/ask")
def ask(
    q: str = Query(..., description="Ø³Ø¤Ø§Ù„Ùƒ"),
    mood: str = Query("fun", description="fun|plain"),
    pwd: str = Query("", description="ÙƒÙ„Ù…Ø© Ø³Ø± Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…ÙˆØ³Ù‘Ø¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
):
    """
    pwd == PROTECTED_PASS => ÙŠÙØ¹Ù‘Ù„ ÙˆØ¶Ø¹ Ù…ÙˆØ³Ù‘Ø¹ (Ù†ØªØ§Ø¦Ø¬/ÙÙ„Ø§ØªØ± Ø£ÙƒØ«Ø±) Ù„ÙƒÙ† Ø¶Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·.
    Ù„Ù† Ù†ØµÙ„ Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø­Ø¸ÙˆØ± Ø£Ùˆ ØºÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ.
    """
    log_usage()
    q = (q or "").strip()
    if not q:
        return {"type":"none","answer":"Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù„Ùˆ Ø³Ù…Ø­Øª ğŸ˜Š"}

    deep = (pwd == PROTECTED_PASS)
    qn = normalize_ar(q)

    # 0) ØªØ­ÙŠØ©
    if is_like(qn, GREET, 80):
        msg = fun_wrap("Ø£Ù‡Ù„Ù‹Ø§ Ø¨ØµØ§Ø­Ø¨ÙŠ! Ø£Ù†Ø§ Ø¨Ø³Ù‘Ø§Ù… ğŸ˜Š Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŒ ØªØ¹Ø±ÙŠÙØ§ØªØŒ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆÙŠØ¨ØŒ Ø£Ùˆ Ù…Ù† Ù…Ù„ÙØ§ØªÙƒâ€¦", mood)
        return {"type":"greet","answer":msg}

    # 1) Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù‘Ù… (Ù…Ù†ØµÙ‘Ø§Øª/Ø­ÙƒÙˆÙ…ÙŠ/ØªØ¹Ù„ÙŠÙ…ÙŠ)
    smart = smart_search_router(q, deep=deep)
    if smart:
        tops = "\n\n".join([f"â€¢ {w['title']}\n{w['link']}\n{w.get('summary','')}" for w in smart.get("results",[])])
        ans = f"Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø«{(' (Ù…ÙÙ„ØªØ±: ' + ', '.join(smart['domains']) + ')' ) if smart.get('domains') else ''}:\n{tops or 'â€”'}"
        smart["answer"] = fun_wrap(ans, mood)
        return smart

    # 2) Ø¨Ø­Ø« Ø£Ø³Ù…Ø§Ø¡ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø³ÙˆØ´Ø§Ù„ (Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙƒÙ„Ù…Ø© Ø§Ø³Ù… ÙÙ‚Ø·)
    if len(q.split()) <= 3 and any(s in qn for s in ["Ø­Ø³Ø§Ø¨","Ø§Ø³Ù…","ÙŠÙˆØ²Ø±","username","profile"]):
        soc = social_name_search(q.replace("Ø­Ø³Ø§Ø¨","").replace("Ø§Ø³Ù…","").replace("ÙŠÙˆØ²Ø±","").strip(), deep=deep)
        tops = "\n\n".join([f"â€¢ {w['title']}\n{w['link']}\n{w.get('summary','')}" for w in soc.get("results",[])])
        soc["answer"] = fun_wrap("ØªØ±Ø´ÙŠØ­Ø§Øª Ù…Ù„ÙØ§Øª/Ù†ØªØ§Ø¦Ø¬ Ø¹Ø§Ù…Ø© (Ø±ÙˆØ§Ø¨Ø· Ø¹Ù„Ù†ÙŠØ© ÙÙ‚Ø·):\n"+(tops or "â€”"), mood)
        return soc

    # 3) Ø±ÙŠØ§Ø¶ÙŠØ§Øª
    if any(tok in q for tok in ["sin","cos","tan","log","exp","^","+","-","*","/"]) or is_like(qn, ["Ù…Ø´ØªÙ‚Ù‡","ØªÙƒØ§Ù…Ù„","Ø­Ù„ Ù…Ø¹Ø§Ø¯Ù„Ù‡"], 70):
        res = solve_math(q)
        ans = "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©." if "error" in res else (
            f"Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {res['input']}\nØªØ¨Ø³ÙŠØ·: {res['simplified']}\nØ§Ù„Ù…Ø´ØªÙ‚Ø©: {res['derivative']}\nØ§Ù„ØªÙƒØ§Ù…Ù„: {res['integral']}"
        )
        return {"type":"math","result":res,"answer": fun_wrap(ans, mood)}

    # 4) Ø¹ÙˆØ§ØµÙ…
    country = detect_capital_query(q)
    if country:
        cap = wiki_capital(country)
        if cap:
            ans = f"Ø¹Ø§ØµÙ…Ø© {country}: {cap['capital']}"
            return {"type":"fact","kind":"capital","query_country":country,"answer": fun_wrap(ans, mood),"source":cap["source"],"summary":cap["summary"][:400]}

    # 5) ØªØ¹Ø±ÙŠÙØ§Øª (ÙˆÙŠÙƒÙŠ)
    if len(q.split()) == 1 or is_like(qn, ["Ø¹Ø±Ù","ØªØ¹Ø±ÙŠÙ","Ù…Ø§Ù‡Ùˆ","Ù…Ø§ Ù‡ÙŠ","explain","definition","what is"], 75):
        d = wiki_define(q)
        if d:
            ans = d["summary"]
            return {"type":"definition","term":q,"answer": fun_wrap(ans, mood),"source":d["source"]}

    # 6) RAG Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ
    rag = rag_bm25(q, k=3)
    if rag:
        s = summarize_text(rag[0]["snippet"], 3)
        srcs = "\n".join([f"- {os.path.basename(h['file'])} (score={h['score']:.2f})" for h in rag])
        ans = f"Ù…Ù„Ø®Øµ Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ:\n{s}\n\nÙ…ØµØ§Ø¯Ø±:\n{srcs}"
        return {"type":"rag","hits":rag,"summary":s,"answer": fun_wrap(ans, mood)}

    # 7) Ø¨Ø­Ø« ÙˆÙŠØ¨ Ø¹Ø§Ù…
    web = ddg_search(q, None, max_results=(12 if deep else 8))
    if web:
        for item in web:
            item["summary"] = summarize_text(item.get("snippet",""), 2)
        tops = "\n\n".join([f"â€¢ {w['title']}\n{w['link']}\n{w.get('summary','')}" for w in web])
        return {"type":"web","results":web,"answer": fun_wrap("Ù†ØªÙŠØ¬Ø© Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹Ø©:\n"+tops, mood)}

    # 8) ØºÙ…ÙˆØ¶
    return {"type":"none","answer": fun_wrap("ØªÙ…Ø§Ù… ÙŠØ§ Ø¨Ø·Ù„! Ù…Ø§ Ù„Ù‚ÙŠØª Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø¢Ù†. Ø£Ø¹Ø·Ù†ÙŠ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙƒØ«Ø± Ø£Ùˆ Ù…Ø«Ø§Ù„ØŒ ÙˆØ£Ù†Ø§ Ø­Ø§Ø¶Ø± ğŸ‘€", mood)}

# ØªØ¹Ù„Ù… Ø°Ø§ØªÙŠ
@app.post("/feedback")
def feedback(payload: Dict[str, Any] = Body(...)):
    q = (payload.get("question") or "").strip()
    a = (payload.get("answer") or "").strip()
    tags = payload.get("tags") or []
    if not q or not a:
        return {"ok": False, "error": "question Ùˆ answer Ù…Ø·Ù„ÙˆØ¨Ø©"}
    save_feedback(q, a, tags)
    n = build_index()
    return {"ok": True, "indexed_docs": n}

# Ø¥Ø¹Ø§Ø¯Ø© ÙÙ‡Ø±Ø³Ø©
@app.post("/train")
def train():
    n = build_index()
    return {"ok": True, "indexed_docs": n}

# Ø¥Ø­ØµØ§Ø¡Ø§Øª
@app.get("/stats")
def stats():
    try:
        with open(USAGE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {"requests": 0}

# ØªØ´ØºÙŠÙ„ Ù…Ø­Ù„ÙŠ
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
