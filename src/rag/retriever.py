# ====== RAG Retriever Ø¨Ø³ÙŠØ· Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ ======
# Ø§Ù„Ù‡Ø¯Ù: Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø£Ù‚Ø±Ø¨ Ù…Ù† Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…Ø®Ø²Ù†Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ docs/
# ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ FAISS + sentence-transformers

import os
import faiss
import pickle
from typing import List, Tuple
from sentence_transformers import SentenceTransformer, util
import fitz  # Ù…Ù† Ù…ÙƒØªØ¨Ø© pymupdf Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„ÙØ§Øª PDF

# Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª
DOCS_DIR = "docs"
INDEX_PATH = "docs/index.faiss"
META_PATH = "docs/meta.pkl"

# Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (embedding)
MODEL = SentenceTransformer("all-MiniLM-L6-v2")

# =======================================================
# Ø¯Ø§Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„ÙØ§Øª PDF
# =======================================================
def extract_text_from_pdf(path: str) -> str:
    text = ""
    with fitz.open(path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

# =======================================================
# Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ docs/
# =======================================================
def build_index() -> None:
    docs, metas = [], []
    print("ğŸ” Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª PDF Ù…Ù†:", DOCS_DIR)

    for file in os.listdir(DOCS_DIR):
        if not file.lower().endswith(".pdf"):
            continue
        path = os.path.join(DOCS_DIR, file)
        txt = extract_text_from_pdf(path)
        parts = [p.strip() for p in txt.split("\n") if len(p.strip()) > 100]
        docs.extend(parts)
        metas.extend([(file, i) for i in range(len(parts))])

    if not docs:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª PDF ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯:", DOCS_DIR)
        return

    print(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {len(docs)} â€” Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³...")
    embeddings = MODEL.encode(docs, convert_to_tensor=False)
    dim = embeddings[0].shape[0]

    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    faiss.write_index(index, INDEX_PATH)
    with open(META_PATH, "wb") as f:
        pickle.dump({"docs": docs, "metas": metas}, f)

    print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­!")

# =======================================================
# Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙ‡Ø±Ø³
# =======================================================
def query_index(q: str, top_k: int = 3) -> List[Tuple[str, str]]:
    if not os.path.exists(INDEX_PATH) or not os.path.exists(META_PATH):
        return [("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø¹Ø¯.", "")]

    index = faiss.read_index(INDEX_PATH)
    with open(META_PATH, "rb") as f:
        meta = pickle.load(f)

    docs = meta["docs"]
    metas = meta["metas"]

    q_emb = MODEL.encode([q], convert_to_tensor=False)
    D, I = index.search(q_emb, top_k)

    results = []
    for idx in I[0]:
        if 0 <= idx < len(docs):
            file, part = metas[idx]
            results.append((file, docs[idx][:400] + "..."))

    return results

# =======================================================
# Ù…Ø«Ø§Ù„ Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙŠØ¯ÙˆÙŠ
# =======================================================
if __name__ == "__main__":
    if not os.path.exists(INDEX_PATH):
        build_index()
    else:
        print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«:")
        q = input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ: ")
        for file, text in query_index(q):
            print(f"\nğŸ“˜ {file}\n{text}\n")
