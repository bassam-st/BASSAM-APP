# src/brain/__init__.py â€” Ù†ÙˆØ§Ø© Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ (Smart v2 Ù…Ø¹ Ù…Ø­Ø§Ø¯Ø«Ø© Ø®ÙÙŠÙØ©)

import re
from datetime import datetime
from typing import List

# Ø±ÙŠØ§Ø¶ÙŠØ§Øª
from sympy import symbols, Eq, sympify, solve  # noqa: F401

# Ø¨Ø­Ø« ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ
from duckduckgo_search import DDGS
import httpx
from bs4 import BeautifulSoup
from readability import Document
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

# Ø§Ù‚ØªØ¨Ø§Ø³ Ø¬ÙÙ…Ù„ Ø¬ÙˆØ§Ø¨
from rank_bm25 import BM25Okapi
from rapidfuzz import fuzz

# ÙƒØ§Ø´ ÙˆØ°Ø§ÙƒØ±Ø© Ø¬Ù„Ø³Ø§Øª
from diskcache import Cache
cache = Cache('/tmp/bassam_cache')
sessions = Cache('/tmp/bassam_sessions')

memory_log: List[dict] = []

# -----------------------------
# Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ Ø¹Ø§Ù…Ø© (Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯)
# -----------------------------
def safe_run(query: str) -> str:
    memory_log.append({"time": datetime.now(), "query": query})
    q = (query or "").strip()
    if not q:
        return "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ø§Ù‹."

    if looks_like_math(q):
        return solve_math(q)

    return search_and_summarize(q)


# -----------------------------
# Ù…Ø­Ø§Ø¯Ø«Ø© Ø®ÙÙŠÙØ©
# -----------------------------
def chat_run(session_id: str, message: str) -> str:
    hist = sessions.get(session_id, [])
    msg = rewrite_followup(message, hist[-1]["user"] if hist else "")
    ans = safe_run(msg)
    hist.append({"user": message, "expanded": msg, "bot": ans})
    sessions.set(session_id, hist[-8:], expire=60*60)  # Ø¢Ø®Ø± 8 ØªØ¨Ø§Ø¯Ù„Ø§Øª/Ø³Ø§Ø¹Ø©
    return ans

def rewrite_followup(m, last_q):
    m = (m or "").strip()
    if not last_q:
        return m
    # Ù…ØªØ§Ø¨Ø¹Ø© Ù‚ØµÙŠØ±Ø© â†’ Ù†ÙØ±Ø¬Ø¹Ù‡Ø§ Ù„Ø³Ø¤Ø§Ù„ ÙƒØ§Ù…Ù„
    if len(m) < 6 or re.match(r"^(ÙˆÙ…ØªÙ‰|ÙˆØ£ÙŠÙ†|ÙˆÙƒÙŠÙ|Ù„Ù…Ø§Ø°Ø§|Ù…Ù†|ÙƒÙ…|Ù‡Ø°Ø§|Ù‡Ø°Ù‡|Ù‡Ùˆ|Ù‡ÙŠ)\b", m):
        return f"{last_q} â€” Ù…ØªØ§Ø¨Ø¹Ø©: {m}"
    return m


# ========= Ø±ÙŠØ§Ø¶ÙŠØ§Øª =========
MATH_HINT = (
    "ØªÙ„Ù…ÙŠØ­: Ø§ÙƒØªØ¨ Ø¨ØµÙŠØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ†-Ø³ÙŠÙ…Ø¨ÙˆÙ„ÙŠØ© Ù…Ø«Ù„: x**2, sqrt(x), sin(x), pi. "
    "Ù„Ù„Ø§Ø´ØªÙ‚Ø§Ù‚: diff(x**3, x)  â€” Ù„Ù„ØªÙƒØ§Ù…Ù„: integrate(sin(x), x). "
    "Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: Ø§ÙƒØªØ¨ Ù…Ø«Ù„Ø§Ù‹ x**2-4=0."
)

def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)]", r"\bpi\b", r"sqrt\(", r"sin\(|cos\(|tan\(",
        r"diff\(", r"integrate\(", r"=\s*0", r"x\s*\*\*", r"Ø§Ø´ØªÙ‚|ØªÙƒØ§Ù…Ù„|Ù…Ø¹Ø§Ø¯Ù„Ø©"
    ]
    return any(re.search(p, q) for p in patterns)

def solve_math(q: str) -> str:
    x, y, z = symbols('x y z')
    try:
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {sols}\n\n{MATH_HINT}"

        if q.strip().startswith(("diff(", "integrate(")):
            res = sympify(q)
            return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø±Ù…Ø²ÙŠ: {res}\n\n{MATH_HINT}"

        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬: {res}\n\n{MATH_HINT}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© ({e}).\n{MATH_HINT}"


# ========= Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ + Ø§Ù‚ØªØ¨Ø§Ø³ =========
def search_and_summarize(query: str) -> str:
    key = f"srch::{query}"
    cached = cache.get(key)
    if cached:
        return cached

    try:
        # 1) Ø¨Ø­Ø« DDG
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=6))

        # Ù…Ø­Ø§ÙˆÙ„Ø© ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ùˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¶Ø¹ÙŠÙØ©
        if not results:
            with DDGS() as ddgs:
                results = list(ddgs.text(f"site:ar.wikipedia.org {query}", max_results=5))

        if not results:
            return "ğŸ˜• Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ù†Ø§Ø³Ø¨Ø©."

        # 2) Ø³Ø­Ø¨ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø£ÙˆÙ„ 3 Ø±ÙˆØ§Ø¨Ø· Ù…Ù†Ø§Ø³Ø¨Ø©
        texts, sources = [], []
        for r in results:
            url = r.get("href") or r.get("url")
            if not url:
                continue
            txt = fetch_clean_text(url)
            if txt and len(txt.split()) > 80:
                texts.append(txt)
                sources.append((r.get("title", "Ù…ØµØ¯Ø±"), url))
            if len(texts) >= 3:
                break

        if not texts:
            return "ğŸ˜• Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØµÙˆØµ Ù…ÙÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬."

        # 3) Ø§Ù‚ØªØ¨Ø§Ø³ Ø¬Ù…Ù„ â€œØªØ´Ø¨Ù‡ Ø§Ù„Ø¬ÙˆØ§Ø¨â€
        qa = extract_answer_like(query, texts)

        # 4) ØªÙ„Ø®ÙŠØµ Ø¹Ø§Ù…
        summary = summarize_texts(texts, sentences=4)

        # 5) ØµÙŠØ§ØºØ© Ù†Ù‡Ø§Ø¦ÙŠØ© + Ù…ØµØ§Ø¯Ø±
        src_lines = "\n".join([f"- {t}: {u}" for t, u in sources])
        head = "ğŸ§  Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø©:\n" + qa if qa.strip() else "ğŸ“Œ Ø®Ù„Ø§ØµØ© Ø³Ø±ÙŠØ¹Ø©:"
        body = summary if qa.strip() else summary
        final = f"{head}\n\n{body}\n\nğŸ”— Ù…ØµØ§Ø¯Ø±:\n{src_lines}"

        cache.set(key, final, expire=60*30)  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
        return final

    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}"


def fetch_clean_text(url: str) -> str:
    try:
        with httpx.Client(timeout=15.0, headers={"User-Agent": "Mozilla/5.0"}) as client:
            r = client.get(url, follow_redirects=True)
            r.raise_for_status()
        doc = Document(r.text)
        html = doc.summary()
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav"]):
            tag.decompose()
        text = " ".join(soup.get_text(separator=" ").split())
        return text
    except Exception:
        return ""


def summarize_texts(texts: List[str], sentences: int = 4) -> str:
    joined = "\n\n".join(texts)
    parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
    summarizer = LsaSummarizer()
    sents = summarizer(parser.document, sentences)
    return " ".join(str(s) for s in sents)


def extract_answer_like(question: str, texts: List[str]) -> str:
    sents = []
    for t in texts:
        sents += re.split(r"(?<=[.!ØŸ])\s+", t)
    sents = [s.strip() for s in sents if 20 <= len(s) <= 300]
    if not sents:
        return ""

    bm = BM25Okapi([s.split() for s in sents])
    top = bm.get_top_n(question.split(), sents, n=14)

    scored = sorted(
        ((s, fuzz.token_set_ratio(question, s)) for s in top),
        key=lambda x: x[1], reverse=True
    )[:7]

    best = [s for s, score in scored if score >= 40][:5]
    return ("\nâ€¢ " + "\nâ€¢ ".join(best)) if best else ""
