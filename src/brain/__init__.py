# src/brain/__init__.py â€” Ù†ÙˆØ§Ø© Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ (Smart V3)

import re
import math
from datetime import datetime
from typing import List
from sympy import symbols, Eq, sympify, solve, diff, integrate, sin, cos, tan, exp, log

from diskcache import Cache
from sumy.summarizers.lsa import LsaSummarizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from rapidfuzz import fuzz

from .registry import connector_duckduckgo, connector_wikipedia, fetch_text

cache = Cache("/tmp/bassam_cache")
memory_log: List[dict] = []

# ---------------------- Ù†Ù‚Ø·Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ ----------------------
def safe_run(query: str) -> str:
    memory_log.append({"time": datetime.now(), "query": query})
    q = (query or "").strip()

    if not q:
        return "âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ø§Ù‹."

    # Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙ‡Ù… Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
    if looks_like_math(q):
        return solve_math(q)
    else:
        return smart_search(q)

# ========== Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ==========
def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)]", r"\bpi\b", r"sqrt\(", r"sin\(|cos\(|tan\(",
        r"diff\(", r"integrate\(", r"=\s*0", r"x\s*\*\*"
    ]
    return any(re.search(p, q) for p in patterns)

# ========== Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ ==========
def solve_math(q: str) -> str:
    x, y, z = symbols("x y z")
    try:
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø§Ù„Ø­Ù„: {sols}"
        if q.strip().startswith(("diff(", "integrate(")):
            res = sympify(q)
            return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø±Ù…Ø²ÙŠ: {res}"
        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬: {res}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© ({e})."

# ========== Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø­Ø«ÙŠ ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ ==========
def smart_search(query: str) -> str:
    key = f"smart::{query}"
    cached = cache.get(key)
    if cached:
        return cached

    results = []

    # 1. ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§
    results.extend(connector_wikipedia(query))

    # 2. DuckDuckGo
    results.extend(connector_duckduckgo(query))

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    cleaned = []
    seen = set()
    for r in results:
        text = (r.get("snippet") or "").strip()
        if not text:
            continue
        if any(fuzz.ratio(text, c.get("snippet", "")) > 80 for c in cleaned):
            continue
        cleaned.append(r)
        seen.add(text)

    if not cleaned:
        return "ðŸ” Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø¨ØµÙŠØºØ© Ù…Ø®ØªÙ„ÙØ©."

    # Ø¯Ù…Ø¬ ÙˆØªÙ„Ø®ÙŠØµ
    texts = [r["snippet"] for r in cleaned]
    summary = summarize_texts(texts)
    src_lines = "\n".join([f"- {r['title']} ({r['url']})" for r in cleaned[:5]])

    answer = f"ðŸ¤– Ø¨Ø³Ø§Ù… ÙˆØ¬Ø¯ Ù„Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n{summary}\n\nðŸ”— Ø§Ù„Ù…ØµØ§Ø¯Ø±:\n{src_lines}"
    cache.set(key, answer, expire=3600)
    return answer

# ========== Ø§Ù„ØªÙ„Ø®ÙŠØµ ==========
def summarize_texts(texts: List[str], sentences: int = 4) -> str:
    joined = "\n\n".join(texts)
    parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
    summarizer = LsaSummarizer()
    sents = summarizer(parser.document, sentences)
    return " ".join(str(s) for s in sents)
