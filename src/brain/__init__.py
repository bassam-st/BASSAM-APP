# -*- coding: utf-8 -*-
"""
Ø§Ù„Ø¹Ù‚Ù„ v9 â€” Bassam Brain
- Ø¹Ø±Ø¨ÙŠØŒ Ù…Ø­Ø§Ø¯Ø«ÙŠ
- Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø¹Ø¨Ø± sympy
- Ø¨Ø­Ø« Ø¹Ø§Ù… Ø¹Ø¨Ø± DuckDuckGo + Ø¬Ù„Ø¨ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙ„Ø®ÙŠØµÙ‡Ø§
- ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ (Ø¹Ø±Ø¨ÙŠ Ø£ÙˆÙ„Ø§Ù‹Ø› Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø«Ù… ØªØ±Ø¬Ù…Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©)
- Ø³ÙˆØ´ÙŠØ§Ù„/Ù†Ù‚Ø§Ø´Ø§Øª: ÙŠØ¬Ù„Ø¨ Ù†ØªØ§Ø¦Ø¬ Reddit/YouTube/Stackexchange Ø¹Ø¨Ø± DuckDuckGo Ø«Ù… ÙŠÙ‚Ø±Ø£ ØµÙØ­Ø© Ø§Ù„Ù…ØµØ¯Ø± (Ø¥Ù† Ø£Ù…ÙƒÙ†)
- ØªØ±Ø¬Ù…Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
- ØªØµØ­ÙŠØ­ ÙˆØªÙˆØ³ÙŠØ¹ Ø¨Ø³ÙŠØ· Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø«
"""

from __future__ import annotations

import re
import math
from datetime import datetime
from typing import List, Tuple, Dict, Optional

# ========= Ø±ÙŠØ§Ø¶ÙŠØ§Øª =========
from sympy import symbols, Eq, sympify, solve, diff, integrate, sin, cos, tan, exp, log  # noqa: F401

# ========= Ø¨Ø­Ø« ÙˆÙ‚Ø±Ø§Ø¡Ø© =========
from duckduckgo_search import DDGS
import httpx
from bs4 import BeautifulSoup
from readability import Document

# ========= ØªÙ„Ø®ÙŠØµ =========
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

# ========= ØªØ±Ø¬Ù…Ø© =========
try:
    # ØºÙŠØ± Ø±Ø³Ù…ÙŠØ© Ù„ÙƒÙ†Ù‡Ø§ ØªØ¹Ù…Ù„ Ù…Ø¬Ø§Ù†Ù‹Ø§ ÙÙŠ Ø£ØºÙ„Ø¨ Ø§Ù„ÙˆÙ‚Øª
    from googletrans import Translator  # type: ignore
except Exception:
    Translator = None  # Ø³Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯Ù‡Ø§

# ========= ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ =========
import wikipedia

# ========= ØªØ­Ø³ÙŠÙ† ØµÙŠØ§ØºØ© =========
from rapidfuzz import process, fuzz

# ========= ÙƒØ§Ø´ Ø®ÙÙŠÙ =========
from diskcache import Cache
cache = Cache('/tmp/bassam_cache')

# Ø³Ø¬Ù„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø¬Ù„Ø³Ø©
memory_log: List[dict] = []

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§
wikipedia.set_rate_limiting(True)
wikipedia.set_lang("ar")

# =========================================
# Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„
# =========================================
def safe_run(query: str) -> str:
    """
    ÙŠØ£Ø®Ø° Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙŠØ¹ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø±Ø¨ÙŠØ© Ù…Ø­Ø§Ø¯Ø«ÙŠØ© Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†.
    """
    memory_log.append({"time": datetime.now(), "query": query})
    q = (query or "").strip()
    if not q:
        return "âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ø§Ù‹."

    # Ù‡Ù„ Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŸ
    if looks_like_math(q):
        return _answer_math(q)

    # ØºÙŠØ± Ø±ÙŠØ§Ø¶ÙŠØ§Øª â†’ Ø¨Ø­Ø« ÙˆÙÙ‡Ù…
    # Ø·Ø¨Ù‘Ù‚ ØªØ­Ø³ÙŠÙ†/ØªØµØ­ÙŠØ­ Ø¨Ø³ÙŠØ· Ù„Ù„ØµÙŠØ§ØºØ©
    q_norm = normalize_query(q)

    # Ø¬Ø±Ù‘Ø¨ ÙƒØ§Ø´
    ck = f"brainv9::{q_norm}"
    cached = cache.get(ck)
    if cached:
        return cached

    # 1) ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø£ÙˆÙ„Ø§Ù‹ (Ø³Ø±ÙŠØ¹Ø© ÙˆÙ…ÙÙŠØ¯Ø© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠØ©)
    wiki_text = fetch_wikipedia(q_norm)
    parts: List[str] = []
    sources: List[Tuple[str, str]] = []

    if wiki_text:
        parts.append(wiki_text["text"])
        sources.append(("ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§", wiki_text["url"]))

    # 2) Ø¨Ø­Ø« ÙˆÙŠØ¨ Ø¹Ø§Ù… (ÙŠØ´Ù…Ù„ Ø³ÙˆØ´ÙŠØ§Ù„ Ø¹Ø¨Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬)
    web_summary, web_sources = web_search_and_summarize(q_norm, want_social=True, max_results=6)
    if web_summary:
        parts.append(web_summary)
        sources.extend(web_sources)

    # 3) Ø¯Ù…Ø¬ ÙˆØªØ¬Ù…ÙŠÙ„ + ØªØ±Ø¬Ù…Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù…
    if not parts:
        final = "ğŸ” Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø¬Ø±Ù‘Ø¨ Ø£Ù† ØªØµÙŠØº Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¬Ù…Ù„Ø© Ø£ÙˆØ¶Ø­ Ø£Ùˆ Ø£Ø¶Ù ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©."
        cache.set(ck, final, expire=60*10)
        return final

    merged = "\n\n".join(parts)
    merged_ar = ensure_arabic(merged)

    sources_txt = "\n".join([f"- {t}: {u}" for (t, u) in dedup_sources(sources)][:8])
    answer = (
        f"ğŸ’¬ **Ø®Ù„Ø§ØµØ© Ù…Ø®ØªØµØ±Ø©:**\n{merged_ar}\n\n"
        f"ğŸ”— **Ù…ØµØ§Ø¯Ø± (Ù…Ø®ØªØ§Ø±Ø©):**\n{sources_txt}"
    )

    cache.set(ck, answer, expire=60*30)
    return answer


# =========================================
# Ø±ÙŠØ§Ø¶ÙŠØ§Øª
# =========================================
MATH_HINT = (
    "ØªÙ„Ù…ÙŠØ­: Ø§ÙƒØªØ¨ Ø¨ØµÙŠØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ†-Ø³ÙŠÙ…Ø¨ÙˆÙ„ÙŠØ© Ù…Ø«Ù„: x**2, sqrt(x), sin(x), pi. "
    "Ù„Ù„Ø§Ø´ØªÙ‚Ø§Ù‚: diff(x**3, x) â€” Ù„Ù„ØªÙƒØ§Ù…Ù„: integrate(sin(x), x) â€” "
    "Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: Ù…Ø«Ù„ x**2 - 4 = 0."
)

def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)]", r"\bpi\b", r"sqrt\(", r"sin\(|cos\(|tan\(",
        r"diff\(", r"integrate\(", r"=\s*0", r"x\s*\*\*", r"Ø§Ø´ØªÙ‚|ØªÙØ§Ø¶Ù„|ØªÙƒØ§Ù…Ù„|Ù…Ø¹Ø§Ø¯Ù„Ø©|Ø­Ù„"
    ]
    return any(re.search(p, q) for p in patterns)

def _answer_math(q: str) -> str:
    x, y, z = symbols('x y z')
    try:
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {sols}\n\n{MATH_HINT}"

        if q.strip().startswith(("diff(", "integrate(")):
            res = sympify(q)
            return f"âœ… Ù†Ø§ØªØ¬ Ø±Ù…Ø²ÙŠ: {res}\n\n{MATH_HINT}"

        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬: {res}\n\n{MATH_HINT}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© ({e}).\n{MATH_HINT}"


# =========================================
# ØªØ­Ø³ÙŠÙ†/ØªØµØ­ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„
# =========================================
_COMMON_FIXES = {
    "Ù…Ù† Ù‡Ùˆ": "Ù…Ù† Ù‡Ùˆ",
    "Ù…Ø§ Ù‡Ùˆ": "Ù…Ø§ Ù‡Ùˆ",
    "Ø§ÙˆØ±Ù…Ø§": "Ø£ÙˆØ±Ù…Ø§",
    "Ø¨Ù† Ù„Ø§Ø¯Ù†": "Ø£Ø³Ø§Ù…Ø© Ø¨Ù† Ù„Ø§Ø¯Ù†",
    "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù‚ÙˆÙ‡": "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù‚ÙˆØ©",
}

def normalize_query(q: str) -> str:
    q = " ".join(q.split())  # Ù…Ø³Ø§ÙØ§Øª Ø·Ø¨ÙŠØ¹ÙŠØ©
    # Ø¨Ø¯Ø§Ø¦ÙŠØ§Øª ØªØµØ­ÙŠØ­ Ø´Ø§Ø¦Ø¹Ø©
    for k, v in _COMMON_FIXES.items():
        if k in q:
            q = q.replace(k, v)
    # Ø¥Ù† ÙƒØ§Ù† Ù‚ØµÙŠØ±Ø§Ù‹ Ø¬Ø¯Ù‹Ø§ØŒ ÙˆØ³Ù‘Ø¹Ù‡ Ù‚Ù„ÙŠÙ„Ø§Ù‹
    if len(q) < 4:
        q = f"Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯ Ø¨Ù€ {q}ØŸ"
    return q


# =========================================
# ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§
# =========================================
def fetch_wikipedia(q: str) -> Optional[Dict[str, str]]:
    try:
        # Ø¹Ø±Ø¨ÙŠ Ø£ÙˆÙ„Ø§Ù‹
        wikipedia.set_lang("ar")
        titles = wikipedia.search(q)
        if titles:
            page = wikipedia.page(titles[0], auto_suggest=False, preload=False)
            summary = wikipedia.summary(page.title, sentences=3, auto_suggest=False)
            return {"text": f"ğŸ“š Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§: {summary}", "url": page.url}

        # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø«Ù… ØªØ±Ø¬Ù…Ø©
        wikipedia.set_lang("en")
        titles = wikipedia.search(q)
        if titles:
            page = wikipedia.page(titles[0], auto_suggest=False, preload=False)
            summary = wikipedia.summary(page.title, sentences=3, auto_suggest=False)
            return {"text": f"ğŸ“š Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ (Ù…ØªØ±Ø¬Ù…): {translate_to_ar(summary)}", "url": page.url}
    except Exception:
        pass
    finally:
        wikipedia.set_lang("ar")
    return None


# =========================================
# Ø¨Ø­Ø« ÙˆÙŠØ¨ + ØªÙ„Ø®ÙŠØµ + Ø³ÙˆØ´ÙŠØ§Ù„
# =========================================
SOCIAL_SITES = ["reddit.com", "stackexchange.com", "stackoverflow.com", "medium.com", "quora.com", "youtube.com", "x.com", "twitter.com"]

def web_search_and_summarize(query: str, want_social: bool = True, max_results: int = 6) -> Tuple[str, List[Tuple[str, str]]]:
    texts: List[str] = []
    sources: List[Tuple[str, str]] = []

    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=max_results))
    except Exception:
        results = []

    if not results:
        return "", []

    # ÙØ¶Ù‘Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØºÙ†ÙŠØ© Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    for r in results:
        url = r.get("href") or r.get("url")
        title = r.get("title", "Ù…ØµØ¯Ø±")
        if not url:
            continue

        # Ø¥Ø°Ø§ Ø£Ø±Ø¯Ù†Ø§ Ø³ÙˆØ´ÙŠØ§Ù„: Ø£Ø¹Ø·Ù Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø´
        if want_social and any(s in url for s in SOCIAL_SITES):
            txt = fetch_page_text(url, social=True)
        else:
            txt = fetch_page_text(url)

        if txt and len(txt.split()) >= 60:
            texts.append(txt)
            sources.append((title, url))

    if not texts:
        return "", []

    summary = summarize_texts(texts, sentences=5)
    return summary, sources


def fetch_page_text(url: str, social: bool = False) -> str:
    """
    ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù†Ø¸ÙŠÙ Ø¨Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†.
    - social=True: Ù†Ø­Ø§ÙˆÙ„ Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„ÙˆØµÙ/Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚ØµÙŠØ± Ù„Ù„Ù…Ø´Ø§Ø±ÙƒØ§Øª.
    """
    try:
        headers = {"User-Agent": "Mozilla/5.0 (BassamBot)"}
        with httpx.Client(timeout=20.0, headers=headers, follow_redirects=True) as client:
            r = client.get(url)
            r.raise_for_status()

        # Ù„Ùˆ ÙŠÙˆØªÙŠÙˆØ¨: Ø®Ø° Ø§Ù„ÙˆØµÙ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
        if "youtube.com" in url or "youtu.be" in url:
            soup = BeautifulSoup(r.text, "lxml")
            desc = soup.find("meta", {"name": "description"})
            if desc and desc.get("content"):
                return f"ÙˆØµÙ ÙÙŠØ¯ÙŠÙˆ ÙŠÙˆØªÙŠÙˆØ¨: {desc['content']}"
            # Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            og_desc = soup.find("meta", {"property": "og:description"})
            if og_desc and og_desc.get("content"):
                return f"ÙˆØµÙ ÙÙŠØ¯ÙŠÙˆ ÙŠÙˆØªÙŠÙˆØ¨: {og_desc['content']}"

        # Ù…ÙˆØ§Ù‚Ø¹ Ù†Ù‚Ø§Ø´: Ø®Ø° ÙÙ‚Ø±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¥Ù† Ø£Ù…ÙƒÙ†
        if social and ("reddit.com" in url or "stack" in url or "quora.com" in url or "medium.com" in url):
            soup = BeautifulSoup(r.text, "lxml")
            # ÙˆØµÙ/Ù…Ù‚ØªØ·ÙØ§Øª Ø¹Ø§Ù…Ø©
            og_desc = soup.find("meta", {"property": "og:description"})
            if og_desc and og_desc.get("content"):
                return og_desc["content"]
            desc = soup.find("meta", {"name": "description"})
            if desc and desc.get("content"):
                return desc["content"]

        # Ø¹Ø§Ù…: Ø§Ø³ØªØ®Ø±Ø¬ Ù…ØªÙ† Ø§Ù„ØµÙØ­Ø© Ø¹Ø¨Ø± readability
        doc = Document(r.text)
        html = doc.summary()
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav", "aside"]):
            tag.decompose()
        text = " ".join(soup.get_text(separator=" ").split())
        # ØªØ±Ø¬Ù…Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù…
        text = ensure_arabic(text)
        return text
    except Exception:
        return ""


def summarize_texts(texts: List[str], sentences: int = 5) -> str:
    # Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ
    joined = "\n\n".join(texts)
    try:
        parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
        summarizer = LsaSummarizer()
        sents = summarizer(parser.document, sentences)
        summary = " ".join(str(s) for s in sents)
        if len(summary.strip()) < 20:
            raise ValueError("summary too short")
        return summary
    except Exception:
        # Ø§Ø­ØªÙŠØ§Ø·ÙŠ: Ø®Ø° Ø£ÙˆÙ„ 700 Ø­Ø±Ù
        return (joined[:700] + "â€¦") if len(joined) > 700 else joined


# =========================================
# ØªØ±Ø¬Ù…Ø©/Ù„ØºØ©
# =========================================
def ensure_arabic(text: str) -> str:
    """Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ù†Øµ ØºÙŠØ± Ø¹Ø±Ø¨ÙŠ Ø¨ÙˆØ¶ÙˆØ­ØŒ Ø­Ø§ÙˆÙ„ ØªØ±Ø¬Ù…ØªÙ‡ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©."""
    if is_arabic(text):
        return text
    return translate_to_ar(text)

def is_arabic(s: str) -> bool:
    return bool(re.search(r"[\u0600-\u06FF]", s))

def translate_to_ar(text: str) -> str:
    if not text:
        return text
    try:
        if Translator is None:
            return text  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ØªØ±Ø¬Ù… Ù…Ø«Ø¨Øª
        tr = Translator()
        out = tr.translate(text, dest="ar")
        return out.text
    except Exception:
        return text  # Ø¥Ù† ÙØ´Ù„Øª Ø§Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ø£Ø¹Ø¯Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ


# =========================================
# Ø£Ø¯ÙˆØ§Øª ØµØºÙŠØ±Ø©
# =========================================
def dedup_sources(s: List[Tuple[str, str]]) -> List[Tuple[str, str]]:
    seen = set()
    out = []
    for t, u in s:
        if u in seen:
            continue
        seen.add(u)
        out.append((sanitize_title(t), u))
    return out

def sanitize_title(t: str) -> str:
    t = re.sub(r"\s+", " ", t or "").strip()
    if len(t) > 80:
        t = t[:77] + "â€¦"
    return t
