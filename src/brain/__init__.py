# src/brain/__init__.py â€” Ù†ÙˆØ§Ø© Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ (Smart v2)

import re
from datetime import datetime
from typing import List, Tuple
from urllib.parse import quote

# Ø±ÙŠØ§Ø¶ÙŠØ§Øª
from sympy import symbols, Eq, sympify, solve  # noqa: F401

# Ø¨Ø­Ø« ÙˆØªÙ„Ø®ÙŠØµ
from duckduckgo_search import DDGS
import httpx
from bs4 import BeautifulSoup
from readability import Document
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

# ÙƒØ§Ø´ Ø®ÙÙŠÙ
from diskcache import Cache
cache = Cache("/tmp/bassam_cache")

memory_log: List[dict] = []

# -----------------------------
# Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„
# -----------------------------
def safe_run(query: str) -> str:
    memory_log.append({"time": datetime.now(), "query": query})
    q = (query or "").strip()
    if not q:
        return "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    # Ø¥Ù† Ø¨Ø¯Ø§ Ø£Ù†Ù‡ Ø³Ø¤Ø§Ù„ Ø±ÙŠØ§Ø¶ÙŠØ§Øª
    if looks_like_math(q):
        return solve_math(q)

    # ÙˆØ¥Ù„Ø§: Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ Ù…Ø¹ Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§
    return search_and_summarize(q)


# ========= Ø±ÙŠØ§Ø¶ÙŠØ§Øª =========
MATH_HINT = (
    "ØªÙ„Ù…ÙŠØ­: Ø§ÙƒØªØ¨ Ø¨ØµÙŠØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ†-Ø³ÙŠÙ…Ø¨ÙˆÙ„ÙŠØ© Ù…Ø«Ù„: x**2, sqrt(x), sin(x), pi. "
    "Ù„Ù„Ø§Ø´ØªÙ‚Ø§Ù‚: diff(x**3, x) â€” Ù„Ù„ØªÙƒØ§Ù…Ù„: integrate(sin(x), x). "
    "Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: Ù…Ø«Ù„ x**2-4=0."
)

def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)]", r"\bpi\b", r"sqrt\(", r"sin\(|cos\(|tan\(",
        r"diff\(", r"integrate\(", r"=\s*0", r"x\s*\*\*", r"Ø§Ø´ØªÙ‚|ØªÙƒØ§Ù…Ù„|Ù…Ø¹Ø§Ø¯Ù„Ø©"
    ]
    return any(re.search(p, q) for p in patterns)

def solve_math(q: str) -> str:
    x, y, z = symbols("x y z")
    try:
        # Ø¥Ø°Ø§ ÙÙŠÙ‡Ø§ Ù…Ø³Ø§ÙˆØ§Ø© -> Ø­Ù„ Ù…Ø¹Ø§Ø¯Ù„Ø©
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {sols}\n\n{MATH_HINT}"

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª diff/integrate Ù…Ø¨Ø§Ø´Ø±Ø©
        if q.strip().startswith(("diff(", "integrate(")):
            res = sympify(q)
            return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø±Ù…Ø²ÙŠ: {res}\n\n{MATH_HINT}"

        # ØªÙ‚ÙŠÙŠÙ… Ø¹Ø¯Ø¯ÙŠ
        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬: {res}\n\n{MATH_HINT}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© ({e}).\n{MATH_HINT}"


# ========= Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ =========
def search_and_summarize(query: str) -> str:
    key = f"srch::{query}"
    cached = cache.get(key)
    if cached:
        return cached

    try:
        # 1) Ø¨Ø­Ø« DDG Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        with DDGS() as ddgs:
            results = list(ddgs.text(
                query,
                max_results=8,
                region="xa-ar",
                safesearch="moderate",
                timelimit=None
            ))

        texts, sources = extract_texts_from_results(results)

        # 2) Ø§Ø­ØªÙŠØ§Ø·ÙŠ: ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        if not texts:
            wk = wikipedia_fetch_ar(query)
            if wk:
                texts = [wk["extract"]]
                sources = [(wk["title"], wk["url"])]

        if not texts:
            return "ðŸ˜• Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…ØµØ§Ø¯Ø± Ù…ÙÙŠØ¯Ø© Ø§Ù„Ø¢Ù†. Ø¬Ø±Ù‘Ø¨ ØµÙŠØ§ØºØ© Ø£Ù‚ØµØ± Ø£Ùˆ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ù…Ø®ØªÙ„ÙÙ‹Ø§."

        # 3) Ù„Ø®Ù‘Øµ
        summary = safe_summarize(texts, sentences=4)

        # 4) Ø£Ø¶Ù Ø§Ù„Ù…ØµØ§Ø¯Ø±
        src_lines = "\n".join([f"- {t}: {u}" for t, u in sources])
        final = f"ðŸ“Œ Ø®Ù„Ø§ØµØ© Ø³Ø±ÙŠØ¹Ø©:\n{summary}\n\nðŸ”— Ù…ØµØ§Ø¯Ø±:\n{src_lines}"
        cache.set(key, final, expire=60*30)  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
        return final

    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}"


def extract_texts_from_results(results):
    texts, sources = [], []
    for r in (results or []):
        url = r.get("href") or r.get("url")
        if not url:
            continue
        txt = fetch_clean_text(url)
        if txt and len(txt.split()) >= 80:
            texts.append(txt)
            title = r.get("title") or "Ù…ØµØ¯Ø±"
            sources.append((title, url))
        if len(texts) >= 3:
            break
    return texts, sources


def fetch_clean_text(url: str) -> str:
    try:
        with httpx.Client(timeout=15.0, headers={"User-Agent": "Mozilla/5.0"}) as client:
            r = client.get(url, follow_redirects=True)
            r.raise_for_status()
        doc = Document(r.text)
        html = doc.summary()
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav"]):
            tag.decompose()
        text = " ".join(soup.get_text(separator=" ").split())
        return text
    except Exception:
        return ""


def safe_summarize(texts: List[str], sentences: int = 4) -> str:
    """Ù…Ù„Ø®Ù‘Øµ Ù…Ø¹ Ø¢Ù„ÙŠØ© Ø³Ù‚ÙˆØ· Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©."""
    try:
        joined = "\n\n".join(texts)
        parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
        summarizer = LsaSummarizer()
        sents = summarizer(parser.document, sentences)
        out = " ".join(str(s) for s in sents).strip()
        if out:
            return out
    except Exception:
        pass
    raw = " ".join(texts)
    return (raw[:800] + "â€¦") if len(raw) > 800 else raw


def wikipedia_fetch_ar(query: str):
    """Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ± Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©."""
    try:
        q = query.strip()
        url_search = (
            "https://ar.wikipedia.org/w/api.php"
            "?action=opensearch&limit=1&namespace=0&format=json&search=" + quote(q)
        )
        with httpx.Client(timeout=15.0, headers={"User-Agent": "Mozilla/5.0"}) as client:
            rs = client.get(url_search)
            rs.raise_for_status()
            data = rs.json()
        if not data or len(data) < 4 or not data[1]:
            return None

        title = data[1][0]
        page_url = data[3][0]

        url_extract = (
            "https://ar.wikipedia.org/w/api.php"
            "?action=query&prop=extracts&explaintext=1&exintro=1&format=json&titles=" + quote(title)
        )
        with httpx.Client(timeout=15.0, headers={"User-Agent": "Mozilla/5.0"}) as client:
            rexc = client.get(url_extract)
            rexc.raise_for_status()
            j = rexc.json()
        pages = j.get("query", {}).get("pages", {})
        if not pages:
            return None
        page = next(iter(pages.values()))
        extract = page.get("extract", "").strip()
        if not extract:
            return None
        return {"title": title, "url": page_url, "extract": extract}
    except Exception:
        return None
