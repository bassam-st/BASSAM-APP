# src/brain/__init__.py â€” Smart v4 (Arabic normalize + fuzzy Wikipedia + math fixes)

import re
import math
import urllib.parse
from datetime import datetime
from typing import List, Optional

# Math
from sympy import symbols, Eq, sympify, solve  # noqa: F401

# Search & summarize
from duckduckgo_search import DDGS
import httpx
from bs4 import BeautifulSoup
from readability import Document
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

# Fuzzy match
from rapidfuzz import fuzz

# Cache
from diskcache import Cache
cache = Cache("/tmp/bassam_cache")

USER_AGENT = (
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
)

memory_log: List[dict] = []

# ---------------------------
# Normalization utilities
# ---------------------------
AR_NUMS = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")
MATH_CHARS = {
    "Ã—": "*", "Ã·": "/", "âˆ’": "-", "â€“": "-", "â€”": "-",
    "Ù«": ".", "ØŒ": ",", "â€™": "'", "â€œ": '"', "â€": '"'
}
MATH_WORDS = [
    (r"\bØ§Ù„Ø¬Ø°Ø±\b", "sqrt"),
    (r"\bØ¬Ø°Ø±\b", "sqrt"),
    (r"\bØ£Ø³\b", "**"),
    (r"\bØªÙƒØ§Ù…Ù„\b", "integrate"),
    (r"\bØ§Ø´ØªÙ‚Ø§Ù‚\b|\bØ§Ø´ØªÙ‚\b", "diff"),
    (r"\bØ¬ÙŠØ¨\b", "sin"),
    (r"\bØ¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù…\b", "cos"),
    (r"\bØ¸Ù„\b", "tan"),
]

STOPWORDS = set("""
Ù…Ø§ Ù…Ù† Ù…Ù†Ù’ Ù…Ù†ÙŽ Ù…Ø§Ø°Ø§ Ø§ÙŠÙ† Ø£ÙŠÙ† Ù…ØªÙ‰ Ù„Ù…Ø§Ø°Ø§ ÙƒÙŠÙ ÙƒÙ… Ù‡Ù„ Ø¹Ù„Ù‰ Ø¹Ù† ÙÙŠ Ø§Ù„Ù‰ Ø¥Ù„Ù‰ Ø¨Ø£Ù† Ø£Ù† Ø¥Ù† ÙƒØ§Ù† ØªÙƒÙˆÙ† Ù‡Ùˆ Ù‡ÙŠ Ù‡Ù… Ù‡Ù† Ù‡Ø°Ø§ Ù‡Ø°Ù‡ ØªÙ„Ùƒ Ø°Ù„Ùƒ Ù‡Ù†Ø§Ùƒ Ù‡Ù†Ø§
""".split())

def strip_diacritics(s: str) -> str:
    return re.sub(r"[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]", "", s)

def normalize_ar(s: str) -> str:
    s = s or ""
    s = strip_diacritics(s)
    s = s.replace("Ù€", "")  # ØªØ·ÙˆÙŠÙ„
    s = s.translate(AR_NUMS)
    for k, v in MATH_CHARS.items():
        s = s.replace(k, v)
    # Ø­Ø±ÙˆÙ Ø£Ù„Ù Ù…ÙˆØ­Ø¯Ø©
    s = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", s)
    s = re.sub("Ù‰", "ÙŠ", s)
    s = re.sub("Ø¤", "Ùˆ", s)
    s = re.sub("Ø¦", "ÙŠ", s)
    s = re.sub("Ø©", "Ù‡", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def arabic_math_hints(s: str) -> str:
    t = s
    for pat, rep in MATH_WORDS:
        t = re.sub(pat, rep, t)
    # ÙƒÙ„Ù…Ø§Øª Ø±Ø¨Ø· Ø´Ø§Ø¦Ø¹Ø©
    t = re.sub(r"\bÙŠØ³Ø§ÙˆÙŠ\b", "=", t)
    return t

def keywords(s: str) -> str:
    toks = [w for w in re.split(r"\W+", s) if w]
    toks = [w for w in toks if w not in STOPWORDS]
    return " ".join(toks[:8])  # Ù…Ø®ØªØµØ±

# ---------------------------
# Entry point
# ---------------------------
def safe_run(query: str) -> str:
    raw_q = (query or "").strip()
    if not raw_q:
        return "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    q = normalize_ar(raw_q)
    memory_log.append({"t": datetime.utcnow().isoformat(), "q": q})

    # Math?
    if looks_like_math(q):
        return solve_math(arabic_math_hints(q))

    # Wikipedia (fuzzy)
    wiki = wiki_smart(q)
    if wiki:
        return wiki

    # Search + summarize
    return search_and_summarize(q)

# ---------------------------
# Math
# ---------------------------
MATH_HINT = (
    "ØªÙ„Ù…ÙŠØ­: Ø§ÙƒØªØ¨ Ø¨ØµÙŠØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ† Ù…Ø«Ù„: x**2, sqrt(x), sin(x), pi. "
    "Ù„Ù„Ø§Ø´ØªÙ‚Ø§Ù‚: diff(expr, x) â€” Ù„Ù„ØªÙƒØ§Ù…Ù„: integrate(expr, x) â€” "
    "Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: x**2-4=0."
)

def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)=]", r"\bpi\b", r"sqrt\(",
        r"sin\(|cos\(|tan\(", r"diff\(", r"integrate\(",
        r"Ø§Ø´ØªÙ‚|ØªÙƒØ§Ù…Ù„|Ù…Ø¹Ø§Ø¯Ù„Ù‡|Ø­Ù„|Ù†Ø§ØªØ¬|Ù‚ÙŠÙ…Ø©"
    ]
    return any(re.search(p, q) for p in patterns)

def solve_math(q: str) -> str:
    x, y, z = symbols("x y z")
    try:
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ù‡: {sols}\n\n{MATH_HINT}"

        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬: {res}\n\n{MATH_HINT}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ù‡ ({e}).\n{MATH_HINT}"

# ---------------------------
# Wikipedia (fuzzy search -> summary)
# ---------------------------
def wiki_smart(query: str) -> Optional[str]:
    # ÙƒÙˆÙ‘Ù† Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø±ÙƒØ²
    core = keywords(query) or query
    for lang in ("ar", "en"):
        title = wiki_best_title(core, lang=lang)
        if not title and lang == "ar":  # Ø¬Ø±Ù‘Ø¨ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ Ø¯ÙˆÙ† ØªØ¨Ø³ÙŠØ·
            title = wiki_best_title(query, lang=lang)
        if title:
            summ = wiki_summary_by_title(title, lang=lang)
            if summ:
                return summ
    return None

def wiki_best_title(q: str, lang="ar") -> Optional[str]:
    api = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "list": "search",
        "srsearch": q,
        "srlimit": 10,
        "format": "json"
    }
    try:
        with httpx.Client(timeout=12.0, headers={"User-Agent": USER_AGENT}) as client:
            r = client.get(api, params=params)
            r.raise_for_status()
            items = r.json().get("query", {}).get("search", [])
            if not items:
                return None
            # Ø§Ø®ØªÙŽØ± Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£Ù‚Ø±Ø¨ Ù†ØµÙŠÙ‹Ø§
            best = max(
                items,
                key=lambda it: fuzz.partial_ratio(q.lower(), it.get("title", "").lower())
            )
            score = fuzz.partial_ratio(q.lower(), best.get("title", "").lower())
            return best["title"] if score >= 55 else None
    except Exception:
        return None

def wiki_summary_by_title(title: str, lang="ar") -> Optional[str]:
    url = (
        f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/"
        f"{urllib.parse.quote(title)}"
    )
    try:
        with httpx.Client(timeout=12.0, headers={"User-Agent": USER_AGENT}) as client:
            r = client.get(url, follow_redirects=True)
            if r.status_code == 404:
                return None
            r.raise_for_status()
            data = r.json()
            extract = data.get("extract")
            page_url = data.get("content_urls", {}).get("desktop", {}).get("page")
            title = data.get("title")
            if extract:
                src = f"\n\nðŸ”— Ù…ØµØ¯Ø±: {page_url or f'ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ({lang})'}"
                return f"ðŸ“Œ {title}:\n{extract}{src}"
    except Exception:
        return None
    return None

# ---------------------------
# Web search + summarize
# ---------------------------
def search_and_summarize(query: str) -> str:
    key = f"srch::{query}"
    cached = cache.get(key)
    if cached:
        return cached

    try:
        with DDGS() as ddgs:
            results = list(
                ddgs.text(
                    query,
                    region="xa-ar",
                    safesearch="Off",
                    max_results=8,
                )
            )
        if not results:
            return "ðŸ”Ž Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ø¶Ø­Ø©. Ø¬Ø±Ù‘Ø¨ ØµÙŠØ§ØºØ© Ø£Ù‚ØµØ± Ø£Ùˆ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠÙ‡ Ø£Ø¯Ù‚."

        texts, sources = [], []
        for r in results[:5]:
            url = r.get("href") or r.get("url")
            if not url:
                continue
            txt = fetch_clean_text(url)
            if txt and len(txt.split()) > 120:
                texts.append(txt)
                sources.append((r.get("title") or "Ù…ØµØ¯Ø±", url))
            if len(texts) >= 3:
                break

        if not texts:
            return "ÙˆØ¬Ø¯Øª Ù†ØªØ§Ø¦Ø¬ØŒ Ù„ÙƒÙ† Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ ÙˆØ§Ø¶Ø­ Ù…Ù†Ù‡Ø§. Ø¬Ø±Ù‘Ø¨ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ø£Ø¯Ù‚."

        summary = summarize_texts(texts, sentences=4)
        src_lines = "\n".join([f"- {t}: {u}" for t, u in sources])
        final = f"ðŸ“Œ Ø®Ù„Ø§ØµØ© Ø³Ø±ÙŠØ¹Ù‡:\n{summary}\n\nðŸ”— Ù…ØµØ§Ø¯Ø±:\n{src_lines}"
        cache.set(key, final, expire=60 * 30)
        return final
    except Exception as e:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}"

def fetch_clean_text(url: str) -> str:
    try:
        with httpx.Client(timeout=15.0, headers={"User-Agent": USER_AGENT}) as client:
            r = client.get(url, follow_redirects=True)
            r.raise_for_status()
        doc = Document(r.text)
        html = doc.summary()
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav", "aside"]):
            tag.decompose()
        text = " ".join(soup.get_text(separator=" ").split())
        return text
    except Exception:
        return ""

def summarize_texts(texts: List[str], sentences: int = 4) -> str:
    joined = "\n\n".join(texts)
    parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
    summarizer = LsaSummarizer()
    sents = summarizer(parser.document, sentences)
    return " ".join(str(s) for s in sents)
