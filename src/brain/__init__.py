# src/brain/__init__.py â€” Ù†ÙˆØ§Ø© Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ (Smart v3: Wikipedia + Search)

import re
import math
import urllib.parse
from datetime import datetime
from typing import List

# Ø±ÙŠØ§Ø¶ÙŠØ§Øª
from sympy import symbols, Eq, sympify, solve  # noqa: F401

# Ø¨Ø­Ø« ÙˆØªÙ„Ø®ÙŠØµ
from duckduckgo_search import DDGS
import httpx
from bs4 import BeautifulSoup
from readability import Document
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

# ÙƒØ§Ø´ Ø®ÙÙŠÙ
from diskcache import Cache
cache = Cache("/tmp/bassam_cache")

memory_log: List[dict] = []

USER_AGENT = (
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
)

# =========================
# Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø¹Ù‚Ù„
# =========================
def safe_run(query: str) -> str:
    q = (query or "").strip()
    memory_log.append({"t": datetime.utcnow().isoformat(), "q": q})

    if not q:
        return "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    # 1) Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŸ
    if looks_like_math(q):
        return solve_math(q)

    # 2) ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø£ÙˆÙ„Ù‹Ø§ (Ø¹Ø±Ø¨ÙŠ Ø«Ù… Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
    wiki = wiki_summary(q)
    if wiki:
        return wiki

    # 3) Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ
    return search_and_summarize(q)


# ========= Ø±ÙŠØ§Ø¶ÙŠØ§Øª =========
MATH_HINT = (
    "ØªÙ„Ù…ÙŠØ­: Ø§ÙƒØªØ¨ Ø¨ØµÙŠØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ†-Ø³ÙŠÙ…Ø¨ÙˆÙ„ÙŠØ© Ù…Ø«Ù„: x**2, sqrt(x), sin(x), pi. "
    "Ù„Ù„Ø§Ø´ØªÙ‚Ø§Ù‚: diff(x**3, x) â€” Ù„Ù„ØªÙƒØ§Ù…Ù„: integrate(sin(x), x) â€” "
    "Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: Ù…Ø«Ù„ x**2-4=0."
)

def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)]",
        r"\bpi\b", r"sqrt\(",
        r"sin\(|cos\(|tan\(",
        r"diff\(", r"integrate\(",
        r"=\s*0", r"x\s*\*\*",
        r"Ø§Ø´ØªÙ‚|ØªÙƒØ§Ù…Ù„|Ù…Ø¹Ø§Ø¯Ù„Ø©|Ø­Ù„|Ù†Ø§ØªØ¬|Ù‚ÙŠÙ…Ø©"
    ]
    return any(re.search(p, q) for p in patterns)

def solve_math(q: str) -> str:
    x, y, z = symbols("x y z")
    try:
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {sols}\n\n{MATH_HINT}"

        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬: {res}\n\n{MATH_HINT}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© ({e}).\n{MATH_HINT}"


# ========= ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ =========
def wiki_summary(query: str) -> str | None:
    """
    ÙŠØ­Ø§ÙˆÙ„ Ø¬Ù„Ø¨ Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ± Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø«Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.
    """
    topic = query.strip()
    # Ø¬Ø±Ù‘Ø¨ Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    topic = re.sub(r"[ØŸ?!]", "", topic)
    topic = re.sub(r"^(Ù…Ø§|Ù…Ù†|Ø£ÙŠÙ†|Ù…ØªÙ‰|ÙƒÙ…|Ù„Ù…Ø§Ø°Ø§|ÙƒÙŠÙ)\s+", "", topic).strip()

    if not topic:
        return None

    for lang in ("ar", "en"):
        url = (
            f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/"
            f"{urllib.parse.quote(topic)}"
        )
        try:
            with httpx.Client(
                timeout=15.0, headers={"User-Agent": USER_AGENT}
            ) as client:
                r = client.get(url, follow_redirects=True)
                if r.status_code == 404:
                    continue
                r.raise_for_status()
                data = r.json()
                extract = data.get("extract")
                title = data.get("title")
                page_url = data.get("content_urls", {}).get("desktop", {}).get("page")
                if extract:
                    src = f"\n\nðŸ”— Ù…ØµØ¯Ø±: {page_url or f'ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ({lang})'}"
                    return f"ðŸ“Œ {title}:\n{extract}{src}"
        except Exception:
            continue
    return None


# ========= Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ =========
def search_and_summarize(query: str) -> str:
    key = f"srch::{query}"
    cached = cache.get(key)
    if cached:
        return cached

    try:
        # 1) Ø¨Ø­Ø« DDG
        with DDGS() as ddgs:
            results = list(
                ddgs.text(
                    query,
                    region="xa-ar",     # ØªÙØ¶ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ Ø¥Ù† ÙˆÙØ¬Ø¯
                    safesearch="Off",
                    max_results=6,
                )
            )

        if not results:
            return "ðŸ§ Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø©. Ø¬Ø±Ù‘Ø¨ ØµÙŠØ§ØºØ© Ø£Ù‚ØµØ± Ø£Ùˆ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø£Ø¯Ù‚."

        # 2) Ø­Ù…Ù‘Ù„ 2â€“3 Ù…ØµØ§Ø¯Ø± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        texts, sources = [], []
        for r in results[:4]:
            url = r.get("href") or r.get("url")
            if not url:
                continue
            txt = fetch_clean_text(url)
            if txt and len(txt.split()) > 120:
                texts.append(txt)
                title = r.get("title") or "Ù…ØµØ¯Ø±"
                sources.append((title, url))
            if len(texts) >= 3:
                break

        if not texts:
            return "ðŸ” ÙˆØ¬Ø¯Øª Ù†ØªØ§Ø¦Ø¬ØŒ Ù„ÙƒÙ† Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ ÙˆØ§Ø¶Ø­ Ù…Ù†Ù‡Ø§. Ø¬Ø±Ù‘Ø¨ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ø£Ù‚ØµØ±."

        # 3) Ù„Ø®Øµ
        summary = summarize_texts(texts, sentences=4)

        # 4) Ù…ØµØ§Ø¯Ø±
        src_lines = "\n".join([f"- {t}: {u}" for t, u in sources])
        final = f"ðŸ“Œ Ø®Ù„Ø§ØµØ© Ø³Ø±ÙŠØ¹Ø©:\n{summary}\n\nðŸ”— Ù…ØµØ§Ø¯Ø±:\n{src_lines}"
        cache.set(key, final, expire=60 * 30)
        return final

    except Exception as e:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}"


def fetch_clean_text(url: str) -> str:
    try:
        with httpx.Client(timeout=15.0, headers={"User-Agent": USER_AGENT}) as client:
            r = client.get(url, follow_redirects=True)
            r.raise_for_status()
        doc = Document(r.text)
        html = doc.summary()
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav", "aside"]):
            tag.decompose()
        text = " ".join(soup.get_text(separator=" ").split())
        return text
    except Exception:
        return ""


def summarize_texts(texts: List[str], sentences: int = 4) -> str:
    joined = "\n\n".join(texts)
    parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
    summarizer = LsaSummarizer()
    sents = summarizer(parser.document, sentences)
    return " ".join(str(s) for s in sents)
