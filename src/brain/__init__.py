# src/brain/__init__.py â€” Ù†ÙˆØ§Ø© Ø¨Ø³Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Smart v2

import re
import math
from datetime import datetime
from typing import List
from sympy import symbols, Eq, sympify, solve, diff, integrate, sin, cos, tan, exp, log
from duckduckgo_search import DDGS
import httpx
from bs4 import BeautifulSoup
from readability import Document
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser
from diskcache import Cache

# ÙƒØ§Ø´ Ù„Ù„Ø³Ø±Ø¹Ø©
cache = Cache('/tmp/bassam_cache')
memory_log: List[dict] = []

def safe_run(query: str) -> str:
    """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°ÙƒÙŠØ© â€” Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø£Ùˆ Ø¨Ø­Ø« Ø£Ùˆ ØªÙ„Ø®ÙŠØµ"""
    memory_log.append({"time": datetime.now(), "query": query})
    q = (query or "").strip()
    if not q:
        return "ðŸ“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    if looks_like_math(q):
        return solve_math(q)
    return search_and_summarize(q)

# ========= Ø±ÙŠØ§Ø¶ÙŠØ§Øª =========
def looks_like_math(q: str) -> bool:
    patterns = [
        r"[0-9\+\-\*/\^\(\)]", r"\bpi\b", r"sqrt\(", r"sin\(|cos\(|tan\(",
        r"diff\(", r"integrate\(", r"=\s*0", r"x\s*\*\*", r"Ø§Ø´ØªÙ‚|ØªÙƒØ§Ù…Ù„|Ø­Ù„|Ù…Ø¹Ø§Ø¯Ù„Ø©"
    ]
    return any(re.search(p, q) for p in patterns)

def solve_math(q: str) -> str:
    x, y, z = symbols('x y z')
    try:
        if "=" in q:
            left, right = q.split("=", 1)
            expr = sympify(left) - sympify(right)
            sols = solve(Eq(expr, 0))
            return f"âœ… Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: {sols}"

        if q.strip().startswith(("diff(", "integrate(")):
            res = sympify(q)
            return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø±Ù…Ø²ÙŠ: {res}"

        res = sympify(q).evalf()
        return f"âœ… Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø¹Ø¯Ø¯ÙŠ: {res}"
    except Exception as e:
        return f"âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ø³Ø£Ù„Ø© ({e})"

# ========= Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ =========
def search_and_summarize(query: str) -> str:
    key = f"srch::{query}"
    cached = cache.get(key)
    if cached:
        return cached

    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=5))
        if not results:
            return "ðŸ” Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø¨ØµÙŠØ§ØºØ© Ù…Ø®ØªÙ„ÙØ©."

        texts, sources = [], []
        for r in results[:3]:
            url = r.get("href") or r.get("url")
            if not url:
                continue
            txt = fetch_text(url)
            if txt and len(txt.split()) > 80:
                texts.append(txt)
                sources.append((r.get("title", "Ù…ØµØ¯Ø±"), url))

        if not texts:
            return "ðŸ˜• Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØµÙˆØµ Ù…ÙÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬."

        summary = summarize_texts(texts, sentences=4)
        src_lines = "\n".join([f"- {t}: {u}" for t, u in sources])
        final = f"ðŸ“˜ Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹:\n{summary}\n\nðŸ”— Ø§Ù„Ù…ØµØ§Ø¯Ø±:\n{src_lines}"
        cache.set(key, final, expire=1800)
        return final
    except Exception as e:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}"

def fetch_text(url: str) -> str:
    try:
        with httpx.Client(timeout=15.0, headers={"User-Agent": "Mozilla/5.0"}) as client:
            r = client.get(url, follow_redirects=True)
            r.raise_for_status()
        doc = Document(r.text)
        html = doc.summary()
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav"]):
            tag.decompose()
        text = " ".join(soup.get_text(separator=" ").split())
        return text
    except Exception:
        return ""

def summarize_texts(texts: List[str], sentences: int = 4) -> str:
    joined = "\n\n".join(texts)
    parser = PlaintextParser.from_string(joined, Tokenizer("arabic"))
    summarizer = LsaSummarizer()
    sents = summarizer(parser.document, sentences)
    return " ".join(str(s) for s in sents)
