# src/brain/omni_brain.py â€” Omni Brain v3 (Ø£Ø¯ÙˆØ§Øª + RAG + ÙˆÙŠØ¨ + Ø°Ø§ÙƒØ±Ø©)

import os, re
from typing import List, Dict, Optional
import httpx
from bs4 import BeautifulSoup
from readability import Document
from duckduckgo_search import DDGS
from diskcache import Cache
from wikipedia import summary as wiki_summary
from dateutil import parser as dateparser
from sympy import sympify, diff, integrate

# âœ… Sumy Ø§Ù„ØµØ­ÙŠØ­
from sumy.parsers.plaintext import PlainTextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
from src.memory.memory import remember, recall

# RAG (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
import faiss
from sentence_transformers import SentenceTransformer
from src.rag.indexer import is_ready as rag_cache_ready
from src.rag.retriever import query_index as rag_file_query

UA = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/124 Safari/537.36"}
cache = Cache(".cache")

# Gemini (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
USE_GEMINI = bool(os.getenv("GEMINI_API_KEY"))
if USE_GEMINI:
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    GEMINI = genai.GenerativeModel("gemini-1.5-flash")
else:
    GEMINI = None

# Embeddings Ù„Ù€ RAG
try:
    RAG_EMB = SentenceTransformer(os.getenv(
        "RAG_EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    ))
except Exception:
    RAG_EMB = None

AR = lambda s: re.sub(r"\s+", " ", (s or "").strip())

def summarize_text(text: str, max_sentences: int = 5) -> str:
    try:
        parser = PlainTextParser.from_string(text, Tokenizer("arabic"))
        s = TextRankSummarizer()(parser.document, max_sentences)
        return " ".join(str(x) for x in s)
    except Exception:
        return (text or "")[:700]

def ddg_text(q: str, n: int = 5) -> List[Dict]:
    with DDGS() as ddgs:
        return list(ddgs.text(q, region="xa-ar", safesearch="moderate", max_results=n) or [])

def fetch_clean(url: str, timeout: int = 12) -> str:
    try:
        r = httpx.get(url, headers=UA, timeout=timeout, follow_redirects=True)
        r.raise_for_status()
        html = Document(r.text).summary()
        return BeautifulSoup(html, "lxml").get_text("\n", strip=True)[:8000]
    except Exception:
        return ""

MATH_PAT = re.compile(r"[=+\-*/^()]|sin|cos|tan|log|sqrt|âˆ«|dx|dy|d/dx|Ù…Ø´ØªÙ‚Ø©|ØªÙƒØ§Ù…Ù„", re.I)
CURRENCY = {"USD":1.0, "EUR":0.92, "SAR":3.75, "AED":3.67, "YER":250.0}

def answer_math(q: str) -> Optional[str]:
    if not MATH_PAT.search(q): return None
    try:
        expr = sympify(q.replace("^","**"))
        return f"Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ: {expr.evalf()}"
    except Exception:
        if q.strip().startswith("Ù…Ø´ØªÙ‚Ø© "):
            t=q.split("Ù…Ø´ØªÙ‚Ø© ",1)[1]
            try: return f"Ù…Ø´ØªÙ‚Ø© {t} = {diff(sympify(t))}"
            except: return "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„Ù…Ø´ØªÙ‚Ø©."
        if q.strip().startswith("ØªÙƒØ§Ù…Ù„ "):
            t=q.split("ØªÙƒØ§Ù…Ù„ ",1)[1]
            try: return f"ØªÙƒØ§Ù…Ù„ {t} = {integrate(sympify(t))}"
            except: return "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„ØªÙƒØ§Ù…Ù„."
        return None

def answer_units_dates(q: str) -> Optional[str]:
    m = re.search(r"(\d+[\.,]?\d*)\s*(USD|EUR|SAR|AED|YER)\s*(?:->|Ø§Ù„Ù‰|Ø¥Ù„Ù‰|to)\s*(USD|EUR|SAR|AED|YER)", q, re.I)
    if m:
        amount=float(m.group(1).replace(",",".")); src=m.group(2).upper(); dst=m.group(3).upper()
        usd=amount / CURRENCY[src]; out=usd*CURRENCY[dst]
        return f"ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§: {amount} {src} â‰ˆ {round(out,2)} {dst}"
    m2=re.search(r"(\d+)\s*(ÙŠÙˆÙ…|Ø£ÙŠØ§Ù…|day|days)\s*(?:Ø¨Ø¹Ø¯|later|from)\s*([0-9\-/: ]+)", q, re.I)
    if m2:
        from datetime import timedelta
        n=int(m2.group(1)); base=dateparser.parse(m2.group(3))
        if base: return (base+timedelta(days=n)).strftime("%Y-%m-%d %H:%M")
    return None

def answer_wikipedia(q: str) -> Optional[str]:
    m=re.search(r"^(Ù…Ù† Ù‡Ùˆ|Ù…Ù† Ù‡ÙŠ|Ù…Ø§ Ù‡ÙŠ|Ù…Ø§Ù‡Ùˆ|Ù…Ø§Ù‡ÙŠ)\s+(.+)$", q.strip(), re.I)
    topic=m.group(2) if m else (q if len(q.split())<=6 else None)
    if not topic: return None
    try:
        return AR(wiki_summary(topic, sentences=3, auto_suggest=False, redirect=True))
    except Exception:
        return None

GREET = ["Ù…Ø±Ø­Ø¨Ø§","Ù…Ø±Ø­Ø¨Ø§Ù‹","Ø§Ù‡Ù„Ø§Ù‹","Ø£Ù‡Ù„Ø§Ù‹","Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…","Ù‡Ù„Ø§","ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±","Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±","Ù‡Ø§ÙŠ","Ø´Ù„ÙˆÙ†Ùƒ","ÙƒÙŠÙÙƒ"]
BYE = ["Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©","Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡","ØªØµØ¨Ø­ Ø¹Ù„Ù‰ Ø®ÙŠØ±","Ø§Ø´ÙˆÙÙƒ Ù„Ø§Ø­Ù‚Ø§Ù‹","Ø¨Ø§ÙŠ"]
def answer_empathy(q: str) -> Optional[str]:
    for w in GREET:
        if w in q: return "Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø£Ù†Ø§ Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ â€” Ù‡Ù†Ø§ Ù„Ø®Ø¯Ù…ØªÙƒ âœ¨"
    for w in BYE:
        if w in q: return "ÙÙŠ Ø£Ù…Ø§Ù† Ø§Ù„Ù„Ù‡! Ø¥Ø°Ø§ Ø§Ø­ØªØ¬ØªÙ†ÙŠ Ø£Ù†Ø§ Ø­Ø§Ø¶Ø± ğŸŒŸ"
    if re.search(r"(Ø­Ø²ÙŠÙ†|Ù…ØªØ¶Ø§ÙŠÙ‚|Ù‚Ù„Ù‚Ø§Ù†|Ø²Ø¹Ù„Ø§Ù†)", q): return "Ø£Ù†Ø§ Ù…Ø¹Ùƒ ğŸ’™ Ø§Ø­ÙƒÙŠ Ù„ÙŠ Ø¨Ù‡Ø¯ÙˆØ¡ ÙˆØ³Ù†Ø¬Ø¯ Ø­Ù„Ù‹Ø§."
    if re.search(r"(Ø´ÙƒØ±Ø§|Ø«Ù†ÙƒÙŠÙˆ|thank|Ø¬Ø²Ø§Ùƒ)", q, re.I): return "Ø§Ù„Ø¹ÙÙˆ! ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ ğŸ™"
    return None

BEAUTY_PAT = re.compile(r"(Ø¨Ø´Ø±Ø©|ØªÙØªÙŠØ­|Ø¨ÙŠØ§Ø¶|ØºØ³ÙˆÙ„|Ø±ØªÙŠÙ†ÙˆÙ„|ÙÙŠØªØ§Ù…ÙŠÙ†|Ø´Ø¹Ø±|ØªØ³Ø§Ù‚Ø·|Ù‚Ø´Ø±Ù‡|Ø­Ø¨ Ø´Ø¨Ø§Ø¨|Ø­Ø¨ÙˆØ¨|Ø±Ø¤ÙˆØ³ Ø³ÙˆØ¯Ø§Ø¡|ØªØ±Ø·ÙŠØ¨|ÙˆØ§Ù‚ÙŠ|Ø±Ø´Ø§Ù‚Ù‡|ØªØ®Ø³ÙŠØ³|Ø±Ø¬ÙŠÙ…)", re.I)
def beauty_coach(q: str) -> Optional[str]:
    if not BEAUTY_PAT.search(q): return None
    tips = [
        "ğŸ§¼ ØºØ³ÙˆÙ„ Ù„Ø·ÙŠÙ ØµØ¨Ø§Ø­Ù‹Ø§ ÙˆÙ…Ø³Ø§Ø¡Ù‹.",
        "ğŸ§´ ØªØ±Ø·ÙŠØ¨ ÙŠÙˆÙ…ÙŠ Ø­ØªÙ‰ Ù„Ù„Ø¨Ø´Ø±Ø© Ø§Ù„Ø¯Ù‡Ù†ÙŠØ© (Ø¬Ù„ Ø®ÙÙŠÙ).",
        "ğŸ›¡ï¸ ÙˆØ§Ù‚ÙŠ Ø´Ù…Ø³ 30+ SPF ÙŠÙˆÙ…ÙŠÙ‹Ø§.",
        "ğŸ’¤ Ù†ÙˆÙ… ÙƒØ§ÙÙ ÙˆÙ…Ø§Ø¡ Ø¨Ø§Ù†ØªØ¸Ø§Ù….",
    ]
    if re.search(r"(ØªÙØªÙŠØ­|Ø§Ø³Ù…Ø±Ø§Ø±|ØºÙ…ÙˆÙ‚)", q): tips += ["ÙÙŠØªØ§Ù…ÙŠÙ† C ØµØ¨Ø§Ø­Ù‹Ø§ + SPF","Ù†ÙŠØ§Ø³ÙŠÙ†Ø§Ù…ÙŠØ¯ Ù…Ø³Ø§Ø¡Ù‹","ØªØ¬Ù†Ø¨ Ø§Ù„Ø®Ù„Ø·Ø§Øª Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„Ø©"]
    if re.search(r"(Ø­Ø¨ Ø´Ø¨Ø§Ø¨|Ø±Ø¤ÙˆØ³|blackhead|whitehead)", q): tips += ["Ø¨Ù†Ø²ÙˆÙŠÙ„ Ø¨ÙŠØ±ÙˆÙƒØ³ÙŠØ¯ 2.5â€“5%","Ø³Ø§Ù„ÙŠØ³ÙŠÙ„ÙŠÙƒ 0.5â€“2%","Ø±ÙŠØªÙŠÙ†ÙˆÙ„ ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§"]
    if re.search(r"(Ø´Ø¹Ø±|ØªØ³Ø§Ù‚Ø·|Ù‚Ø´Ø±Ù‡)", q): tips += ["ØªØ¯Ù„ÙŠÙƒ Ø§Ù„ÙØ±ÙˆØ© 5 Ø¯Ù‚Ø§Ø¦Ù‚","Ø²ÙŠÙˆØª Ø®ÙÙŠÙØ© Ù„Ù„Ø£Ø·Ø±Ø§Ù","ÙØ­Øµ Ø§Ù„Ø­Ø¯ÙŠØ¯/ÙÙŠØªØ§Ù…ÙŠÙ† D"]
    if re.search(r"(Ø±Ø´Ø§Ù‚Ù‡|ØªØ®Ø³ÙŠØ³|Ø±Ø¬ÙŠÙ…|Ø¯Ø§ÙŠØª|ÙˆØ²Ù†)", q): tips += ["Ø¹Ø¬Ø² 300â€“500 Ø³Ø¹Ø±Ø©","Ù…Ø´ÙŠ 30 Ø¯Ù‚ÙŠÙ‚Ø© 5Ã—Ø£Ø³Ø¨ÙˆØ¹","ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ù…ÙŠØ§Øª Ø§Ù„Ù‚Ø§Ø³ÙŠØ©"]
    return "Ø£Ù†Ø§ Ù…Ø¹Ùƒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© âœ¨\n" + "\n".join("â€¢ "+t for t in tips[:10])

def answer_rag(q: str, k: int = 4) -> Optional[str]:
    # Ø¹Ø¨Ø± indexer (Cache) Ø¥Ù† ÙˆÙØ¬Ø¯
    if RAG_EMB and rag_cache_ready():
        index  = cache.get("rag:index")
        chunks = cache.get("rag:chunks")
        metas  = cache.get("rag:metas")
        if index and chunks and metas:
            vec = RAG_EMB.encode([q], convert_to_numpy=True, normalize_embeddings=True)
            D, I = index.search(vec, k)
            picks = [i for i in I[0] if 0 <= i < len(chunks)]
            if picks:
                ctx  = "\n\n".join(chunks[i] for i in picks)
                srcs = sorted(set(metas[i]["source"] for i in picks))
                summ = summarize_text(ctx, 6)
                return f"{AR(summ)}\n\nØ§Ù„Ù…ØµØ§Ø¯Ø± (RAG):\n" + "\n".join(f"- {s}" for s in srcs)
    # Ø¹Ø¨Ø± Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø±Øµ
    try:
        hits = rag_file_query(q, top_k=k)
        ctx = "\n\n".join(s for _, s in hits)
        if not ctx.strip(): return None
        srcs = [f for f, _ in hits]
        summ = summarize_text(ctx, 6)
        return f"{AR(summ)}\n\nØ§Ù„Ù…ØµØ§Ø¯Ø± (RAG):\n" + "\n".join(f"- {s}" for s in sorted(set(srcs)))
    except Exception:
        return None

def answer_gemini(q: str) -> Optional[str]:
    if not GEMINI: return None
    try:
        r = GEMINI.generate_content("Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆØ¯Ù‚Ø© ÙˆÙˆØ¯:\n"+q)
        return (r.text or "").strip()
    except Exception as e:
        return f"(ØªÙ†Ø¨ÙŠÙ‡ Gemini): {e}"

def answer_from_web(q: str) -> str:
    key=f"w:{q}"; c=cache.get(key)
    if c: return c
    hits=ddg_text(q, n=5)
    ctxs, cites=[],[]
    for h in hits:
        url=h.get("href") or h.get("url")
        if not url: continue
        t=fetch_clean(url)
        if t: ctxs.append(t); cites.append(url)
    if not ctxs: return "Ù„Ù… Ø£Ø¬Ø¯ Ù…ØµØ§Ø¯Ø± ÙƒØ§ÙÙŠØ© Ø§Ù„Ø¢Ù†."
    blob="\n\n".join(ctxs)[:16000]
    summ=summarize_text(blob, 6)
    ans=AR(summ)+("\n\nØ§Ù„Ù…ØµØ§Ø¯Ø±:\n"+"\n".join(f"- {u}" for u in cites[:5]) if cites else "")
    cache.set(key, ans, expire=3600)
    return ans

def omni_answer(q: str) -> str:
    q=AR(q)
    if not q: return "Ø§ÙƒØªØ¨/ÙŠ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."
    for tool in (answer_empathy, answer_math, answer_units_dates, beauty_coach, answer_wikipedia):
        a=tool(q)
        if a: return a
    a=answer_rag(q);     if a: return a
    a=answer_gemini(q);  if a: return a
    return answer_from_web(q)

# ===== Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© =====
def _extract_name(text: str) -> Optional[str]:
    m=re.search(r"(?:Ø§Ø³Ù…ÙŠ|Ø£Ù†Ø§ Ø§Ø³Ù…ÙŠ|my name is)\s+([^\.,\|\n\r]+)", text, re.I)
    if not m: return None
    name=re.sub(r"[^\w\u0600-\u06FF\s\-']", "", m.group(1)).strip()
    return name[:40] if name else None

def qa_pipeline(query: str, user_id: str = "guest") -> str:
    q=AR(query)
    if not q: return "Ø§ÙƒØªØ¨/ÙŠ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."
    name_candidate=_extract_name(q)
    if name_candidate:
        remember(user_id, "name", name_candidate)
        return f"ØªØ´Ø±ÙØª Ø¨Ù…Ø¹Ø±ÙØªÙƒ ÙŠØ§ {name_candidate} ğŸŒŸ"
    name=recall(user_id, "name", None)
    if name and re.search(r"(Ø´ÙƒØ±Ø§|thanks|Ø«Ù†ÙƒÙŠÙˆ)", q, re.I): return f"Ø§Ù„Ø¹ÙÙˆ ÙŠØ§ {name} ğŸ™"
    ans=omni_answer(q)
    remember(user_id, "last_query", q)
    if name and isinstance(ans,str) and len(ans)<400:
        ans += f"\n\nâ€” Ù…Ø¹Ùƒ Ø¨Ø³Ù‘Ø§Ù…ØŒ Ø­Ø§Ø¶Ø± ÙŠØ§ {name} ğŸŒŸ"
    return ans
