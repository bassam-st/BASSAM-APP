# src/brain/omni_brain.py
# Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©: Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ© + RAG + Gemini + ÙˆÙŠØ¨ + Ø°Ø§ÙƒØ±Ø© Ù…Ø³ØªØ®Ø¯Ù…

import os, re, math, json, time
from datetime import datetime
from dateutil import parser as dateparser
from typing import List, Dict, Optional

import numpy as np
import httpx
from bs4 import BeautifulSoup
from readability import Document
from duckduckgo_search import DDGS
from diskcache import Cache
from wikipedia import summary as wiki_summary

from sympy import sympify, diff, integrate

# âœ… Sumy Ø§Ù„ØµØ­ÙŠØ­
from sumy.parsers.text import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# âœ… Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
from src.memory.memory import remember, recall

# âœ… RAG
import faiss
from sentence_transformers import SentenceTransformer
from src.rag.indexer import is_ready as rag_cache_ready
from src.rag.retriever import query_index as rag_file_query

# ===== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© =====
UA = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/124 Safari/537.36"}
cache = Cache(".cache")

# Gemini (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
USE_GEMINI = bool(os.getenv("GEMINI_API_KEY"))
if USE_GEMINI:
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    GEMINI = genai.GenerativeModel("gemini-1.5-flash")
else:
    GEMINI = None

# ===== RAG Embeddings =====
try:
    RAG_MODEL_NAME = os.getenv("RAG_EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    RAG_EMB = SentenceTransformer(RAG_MODEL_NAME)
except Exception:
    RAG_EMB = None

# ===== Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ù†ØµÙŠØ© =====
AR = lambda s: re.sub(r"\s+", " ", (s or "").strip())

# ===== ØªÙ„Ø®ÙŠØµ Ù…Ø­Ù„ÙŠ =====
def summarize_text(text: str, max_sentences: int = 5) -> str:
    try:
        parser = PlainTextParser.from_string(text, Tokenizer("arabic"))
        summ = TextRankSummarizer()
        sents = summ(parser.document, max_sentences)
        return " ".join(str(s) for s in sents)
    except Exception:
        return text[:700]

# ===== Ø¨Ø­Ø« Ø§Ù„ÙˆÙŠØ¨ =====
def ddg_text(q: str, n: int = 5) -> List[Dict]:
    with DDGS() as ddgs:
        return list(ddgs.text(q, region="xa-ar", safesearch="moderate", max_results=n) or [])

def fetch_clean(url: str, timeout: int = 12) -> str:
    try:
        r = httpx.get(url, headers=UA, timeout=timeout, follow_redirects=True)
        r.raise_for_status()
        doc = Document(r.text)
        html_clean = doc.summary()
        text = BeautifulSoup(html_clean, "lxml").get_text("\n", strip=True)
        return text[:8000]
    except Exception:
        return ""

# ===== Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ© (Ø±ÙŠØ§Ø¶ÙŠØ§Øª/ÙˆØ­Ø¯Ø§Øª/ØªÙˆØ§Ø±ÙŠØ®) =====
MATH_PAT = re.compile(r"[=+\-*/^()]|sin|cos|tan|log|sqrt|âˆ«|dx|dy|d/dx|Ù…Ø´ØªÙ‚Ø©|ØªÙƒØ§Ù…Ù„", re.I)
CURRENCY = {"USD":1.0, "EUR":0.92, "SAR":3.75, "AED":3.67, "YER":250.0}

def answer_math(q: str) -> Optional[str]:
    if not MATH_PAT.search(q):
        return None
    try:
        s = q.replace("^", "**")
        expr = sympify(s)
        return f"Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ: {expr.evalf()}"
    except Exception:
        if q.strip().startswith("Ù…Ø´ØªÙ‚Ø© "):
            t = q.split("Ù…Ø´ØªÙ‚Ø© ",1)[1]
            try: return f"Ù…Ø´ØªÙ‚Ø© {t} = {diff(sympify(t))}"
            except: return "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„Ù…Ø´ØªÙ‚Ø©."
        if q.strip().startswith("ØªÙƒØ§Ù…Ù„ "):
            t = q.split("ØªÙƒØ§Ù…Ù„ ",1)[1]
            try: return f"ØªÙƒØ§Ù…Ù„ {t} = {integrate(sympify(t))}"
            except: return "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„ØªÙƒØ§Ù…Ù„."
        return None

def answer_units_dates(q: str) -> Optional[str]:
    m = re.search(r"(\d+[\.,]?\d*)\s*(USD|EUR|SAR|AED|YER)\s*(?:->|Ø§Ù„Ù‰|Ø¥Ù„Ù‰|to)\s*(USD|EUR|SAR|AED|YER)", q, re.I)
    if m:
        amount = float(m.group(1).replace(",", "."))
        src, dst = m.group(2).upper(), m.group(3).upper()
        usd = amount / CURRENCY[src]
        out = usd * CURRENCY[dst]
        return f"ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§: {amount} {src} â‰ˆ {round(out,2)} {dst}"
    m2 = re.search(r"(\d+)\s*(ÙŠÙˆÙ…|Ø£ÙŠØ§Ù…|day|days)\s*(?:Ø¨Ø¹Ø¯|later|from)\s*([0-9\-/: ]+)", q, re.I)
    if m2:
        n = int(m2.group(1)); base = dateparser.parse(m2.group(3))
        if base:
            return (base + __import__('datetime').timedelta(days=n)).strftime("%Y-%m-%d %H:%M")
    return None

# ===== ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ù‚ØµÙŠØ±Ø© =====
def answer_wikipedia(q: str) -> Optional[str]:
    m = re.search(r"^(Ù…Ù† Ù‡Ùˆ|Ù…Ù† Ù‡ÙŠ|Ù…Ø§ Ù‡ÙŠ|Ù…Ø§Ù‡Ùˆ|Ù…Ø§Ù‡ÙŠ)\s+(.+)$", q.strip(), re.I)
    topic = m.group(2) if m else None
    topic = topic or (q if len(q.split())<=6 else None)
    if not topic:
        return None
    try:
        s = wiki_summary(topic, sentences=3, auto_suggest=False, redirect=True)
        return AR(s)
    except Exception:
        return None

# ===== Ù…Ø´Ø§Ø¹Ø± ÙˆØªØ­ÙŠØ§Øª =====
GREET_WORDS = ["Ù…Ø±Ø­Ø¨Ø§","Ù…Ø±Ø­Ø¨Ø§Ù‹","Ø§Ù‡Ù„Ø§Ù‹","Ø£Ù‡Ù„Ø§Ù‹","Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…","Ù‡Ù„Ø§","ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±","Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±","Ù‡Ø§ÙŠ","Ø´Ù„ÙˆÙ†Ùƒ","ÙƒÙŠÙÙƒ"]
FAREWELL_WORDS = ["Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©","Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡","ØªØµØ¨Ø­ Ø¹Ù„Ù‰ Ø®ÙŠØ±","Ø§Ø´ÙˆÙÙƒ Ù„Ø§Ø­Ù‚Ø§Ù‹","Ø¨Ø§ÙŠ"]
PERSONA_TAGLINES = [
    "Ø£Ù†Ø§ Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ â€” Ù‡Ù†Ø§ Ø¹Ø´Ø§Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ø®Ø·ÙˆØ§Øª Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø© âœ¨",
    "Ø¨Ø³Ù‘Ø§Ù… Ù…Ø¹Ùƒ! Ù†Ø­Ù„Ù‡Ø§ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© ÙˆØ¨Ù‡Ø¯ÙˆØ¡ ğŸ’ª",
]
def answer_empathy(q: str) -> Optional[str]:
    for w in GREET_WORDS:
        if w in q:
            return ("ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ â€” Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! ğŸ˜Š\n"+PERSONA_TAGLINES[0]) if "Ø§Ù„Ø³Ù„Ø§Ù…" in w else ("Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø³Ø¹ÙŠØ¯ Ø¨ÙˆØ¬ÙˆØ¯Ùƒ ğŸ¤\n"+PERSONA_TAGLINES[1])
    for w in FAREWELL_WORDS:
        if w in q:
            return "ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù„Ù‡! Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Øª Ø£ÙŠ Ø´ÙŠØ¡ Ø£Ù†Ø§ Ø­Ø§Ø¶Ø± Ø¯Ø§Ø¦Ù…Ù‹Ø§ ğŸŒŸ"
    if re.search(r"(Ø£Ù†Ø§ Ø­Ø²ÙŠÙ†|Ø­Ø²ÙŠÙ†Ù‡|Ù…ØªØ¶Ø§ÙŠÙ‚|Ù…ØªØ¶Ø§ÙŠÙ‚Ø©|Ù‚Ù„Ù‚Ø§Ù†|Ù‚Ù„Ù‚Ø§Ù†Ù‡|Ø²Ø¹Ù„Ø§Ù†)", q):
        return "Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù…Ø¹Ùƒ ğŸ’™ â€” Ø®Ø° Ù†ÙØ³Ù‹Ø§ Ø¹Ù…ÙŠÙ‚Ù‹Ø§ØŒ ÙˆÙ‚Ù„ Ù„ÙŠ Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ²Ø¹Ø¬Ùƒ Ø®Ø·ÙˆØ© Ø®Ø·ÙˆØ©."
    if re.search(r"(Ø´ÙƒØ±Ø§|Ø«Ù†ÙƒÙŠÙˆ|thank|Ù…Ù…ØªØ§Ø²|Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±)", q, re.I):
        return "Ø´ÙƒØ±Ù‹Ø§ Ù„Ø°ÙˆÙ‚Ùƒ! ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ ğŸ™"
    return None

# ===== Beauty Coach (Ø§Ù„Ø¹Ù†Ø§ÙŠØ© ÙˆØ§Ù„Ø¬Ù…Ø§Ù„) =====
BEAUTY_PAT = re.compile(r"(Ø¨Ø´Ø±Ø©|ØªÙØªÙŠØ­|Ø¨ÙŠØ§Ø¶|ØºØ³ÙˆÙ„|Ø±ØªÙŠÙ†ÙˆÙ„|ÙÙŠØªØ§Ù…ÙŠÙ†|Ø´Ø¹Ø±|ØªØ³Ø§Ù‚Ø·|Ù‚Ø´Ø±Ù‡|Ø­Ø¨ Ø´Ø¨Ø§Ø¨|Ø­Ø¨ÙˆØ¨|Ø±Ø¤ÙˆØ³ Ø³ÙˆØ¯Ø§Ø¡|ØªØ±Ø·ÙŠØ¨|ÙˆØ§Ù‚ÙŠ|Ø±Ø´Ø§Ù‚Ù‡|ØªØ®Ø³ÙŠØ³|Ø±Ø¬ÙŠÙ…)", re.I)
def beauty_coach(q: str) -> Optional[str]:
    if not BEAUTY_PAT.search(q): return None
    ql = q.lower()
    tips = [
        "ğŸ§¼ ØºØ³ÙˆÙ„ Ù„Ø·ÙŠÙ ØµØ¨Ø§Ø­Ù‹Ø§ ÙˆÙ…Ø³Ø§Ø¡Ù‹.",
        "ğŸ§´ ØªØ±Ø·ÙŠØ¨ ÙŠÙˆÙ…ÙŠ (Ø­ØªÙ‰ Ù„Ù„Ø¨Ø´Ø±Ø© Ø§Ù„Ø¯Ù‡Ù†ÙŠØ© Ø¨Ø¬Ù„Ù‘ Ø®ÙÙŠÙ).",
        "ğŸ›¡ï¸ ÙˆØ§Ù‚ÙŠ Ø´Ù…Ø³ SPF 30+ ÙŠÙˆÙ…ÙŠÙ‹Ø§.",
        "ğŸ›Œ Ù†ÙˆÙ… ÙƒØ§ÙÙ + Ù…Ø§Ø¡ Ø¨Ø§Ù†ØªØ¸Ø§Ù….",
    ]
    if re.search(r"(ØªÙØªÙŠØ­|Ø¨ÙŠØ§Ø¶|Ø§Ø³Ù…Ø±Ø§Ø±|ØºÙ…ÙˆÙ‚)", ql):
        tips += ["ÙÙŠØªØ§Ù…ÙŠÙ† C ØµØ¨Ø§Ø­Ù‹Ø§ 3â€“10% + SPF","Ù†ÙŠØ§Ø³ÙŠÙ†Ø§Ù…ÙŠØ¯ 4â€“10% Ù…Ø³Ø§Ø¡Ù‹","ØªØ¬Ù†Ø¨ Ø§Ù„Ø®Ù„Ø·Ø§Øª Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„Ø©."]
    if re.search(r"(Ø­Ø¨ Ø´Ø¨Ø§Ø¨|Ø§Ù„Ø­Ø¨ÙˆØ¨|blackhead|whitehead|Ø±Ø¤ÙˆØ³)", ql):
        tips += ["Ø¨Ù†Ø²ÙˆÙŠÙ„ Ø¨ÙŠØ±ÙˆÙƒØ³ÙŠØ¯ 2.5â€“5% Ù„Ù„Ø­Ø¨ÙˆØ¨ Ø§Ù„Ù…Ù„ØªÙ‡Ø¨Ø©","Ø³Ø§Ù„ÙŠØ³ÙŠÙ„ÙŠÙƒ Ø£Ø³ÙŠØ¯ 0.5â€“2%","Ø±ÙŠØªÙŠÙ†ÙˆÙ„ ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§ Ù„ÙŠÙ„Ù‹Ø§ 1â€“2Ã—/Ø£Ø³Ø¨ÙˆØ¹"]
    if re.search(r"(Ø´Ø¹Ø±|ØªØ³Ø§Ù‚Ø·|Ù‚Ø´Ø±Ù‡)", ql):
        tips += ["ØªØ¯Ù„ÙŠÙƒ Ø§Ù„ÙØ±ÙˆØ© 5 Ø¯Ù‚Ø§Ø¦Ù‚ ÙŠÙˆÙ…ÙŠÙ‹Ø§","Ø²ÙŠÙˆØª Ø®ÙÙŠÙØ© Ù„Ù„Ø£Ø·Ø±Ø§Ù","ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙŠØ¯/ÙÙŠØªØ§Ù…ÙŠÙ† D"]
    if re.search(r"(Ø±Ø´Ø§Ù‚Ù‡|ØªØ®Ø³ÙŠØ³|ÙˆØ²Ù†|Ø±Ø¬ÙŠÙ…|Ø¯Ø§ÙŠØª)", ql):
        tips += ["Ø¹Ø¬Ø² Ø­Ø±Ø§Ø±ÙŠ Ù…Ø¹ØªØ¯Ù„ 300â€“500 Ø³Ø¹Ø±Ø©","Ù…Ø´ÙŠ 30 Ø¯Ù‚ÙŠÙ‚Ø© 5 Ø£ÙŠØ§Ù…/Ø£Ø³Ø¨ÙˆØ¹","ØªØ¬Ù†Ù‘Ø¨ Ø§Ù„Ø­Ù…ÙŠØ§Øª Ø§Ù„Ù‚Ø§Ø³ÙŠØ©"]
    return "Ø£Ù†Ø§ Ù…Ø¹Ùƒ â€” Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© âœ¨\n" + "\n".join("â€¢ "+t for t in tips[:10])

# ===== RAG =====
def answer_rag(q: str, k: int = 4) -> Optional[str]:
    # 1) Ø¹Ø¨Ø± indexer (cache)
    if RAG_EMB and rag_cache_ready():
        index  = cache.get("rag:index")
        chunks = cache.get("rag:chunks")
        metas  = cache.get("rag:metas")
        if index and chunks and metas:
            qv = RAG_EMB.encode([q], convert_to_numpy=True, normalize_embeddings=True)
            D, I = index.search(qv, k)
            picks = [i for i in I[0] if 0 <= i < len(chunks)]
            if picks:
                ctx  = "\n\n".join(chunks[i] for i in picks)
                srcs = sorted(set(metas[i]["source"] for i in picks))
                summ = summarize_text(ctx, max_sentences=6)
                return f"{AR(summ)}\n\nØ§Ù„Ù…ØµØ§Ø¯Ø± (RAG Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ):\n" + "\n".join(f"- {s}" for s in srcs)

    # 2) Ø¹Ø¨Ø± retriever (Ù…Ù„ÙØ§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ)
    try:
        hits = rag_file_query(q, top_k=k)
        if len(hits) == 1 and isinstance(hits[0], tuple) and "Ù„Ù… ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³" in hits[0][0]:
            return None
        ctx  = "\n\n".join(snippet for _, snippet in hits)
        srcs = [fname for fname, _ in hits]
        if not ctx.strip():
            return None
        summ = summarize_text(ctx, max_sentences=6)
        return f"{AR(summ)}\n\nØ§Ù„Ù…ØµØ§Ø¯Ø± (RAG Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ):\n" + "\n".join(f"- {s}" for s in sorted(set(srcs)))
    except Exception:
        return None

# ===== Gemini Ø§Ø®ØªÙŠØ§Ø±ÙŠ =====
def answer_gemini(q: str) -> Optional[str]:
    if not GEMINI: return None
    try:
        resp = GEMINI.generate_content("Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø© Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆØ¯Ù‚Ø© ÙˆØ¨Ù†Ø¨Ø±Ø© ÙˆØ¯ÙˆØ¯Ø©:\n"+q)
        return (resp.text or "").strip()
    except Exception as e:
        return f"(ØªÙ†Ø¨ÙŠÙ‡ Gemini): {e}"

# ===== ÙˆÙŠØ¨ + ØªÙ„Ø®ÙŠØµ Ù…Ø­Ù„ÙŠ Ù…Ø¹ Ù…ØµØ§Ø¯Ø± =====
def answer_from_web(q: str) -> str:
    key = f"w:{q}"
    c = cache.get(key)
    if c: return c
    hits = ddg_text(q, n=5)
    contexts, cites = [], []
    for h in hits:
        url = h.get("href") or h.get("url")
        if not url: continue
        txt = fetch_clean(url)
        if txt:
            contexts.append(txt)
            cites.append(url)
    if not contexts:
        return "Ù„Ù… Ø£Ø¬Ø¯ Ù…ØµØ§Ø¯Ø± ÙƒØ§ÙÙŠØ© Ø§Ù„Ø¢Ù†. Ø¬Ø±Ù‘Ø¨/ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ©."
    blob = "\n\n".join(contexts)[:16000]
    summ = summarize_text(blob, max_sentences=6)
    ans = AR(summ) + ("\n\nØ§Ù„Ù…ØµØ§Ø¯Ø±:\n" + "\n".join(f"- {u}" for u in cites[:5]) if cites else "")
    cache.set(key, ans, expire=3600)
    return ans

# ===== Ø§Ù„Ù…ÙˆØ¬Ù‘Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ø¨Ø¯ÙˆÙ† Ø°Ø§ÙƒØ±Ø©) =====
def omni_answer(q: str) -> str:
    q = AR(q)
    if not q: return "Ø§ÙƒØªØ¨/ÙŠ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    # 0) ØªØ­ÙŠØ§Øª/Ù…Ø´Ø§Ø¹Ø±
    a = answer_empathy(q)
    if a: return a

    # 1) Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ© + Beauty + ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§
    for tool in (answer_math, answer_units_dates, beauty_coach, answer_wikipedia):
        a = tool(q)
        if a: return a

    # 2) RAG Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ (Ø¥Ù† ÙˆÙØ¬Ø¯ ÙÙ‡Ø±Ø³/Ù…Ù„ÙØ§Øª)
    a = answer_rag(q)
    if a: return a

    # 3) Gemini (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    a = answer_gemini(q)
    if a: return a

    # 4) ÙˆÙŠØ¨ + ØªÙ„Ø®ÙŠØµ Ù…Ø­Ù„ÙŠ
    return answer_from_web(q)

# ===== Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Memory) =====
def _extract_name(text: str) -> Optional[str]:
    # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¨Ø³ÙŠØ·Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù… Ù…Ù† Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ
    m = re.search(r"(?:Ø§Ø³Ù…ÙŠ|Ø§Ù†Ø§ Ø§Ø³Ù…ÙŠ|Ø£Ù†Ø§ Ø§Ø³Ù…ÙŠ|my name is)\s+([^\.,\|\n\r]+)", text, re.I)
    if m:
        name = m.group(1).strip()
        # ØªÙ†Ø¸ÙŠÙ Ø³Ø±ÙŠØ¹
        name = re.sub(r"[^\w\u0600-\u06FF\s\-']", "", name)
        return name[:40]
    return None

def qa_pipeline(query: str, user_id: str = "guest") -> str:
    """
    Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø³Ù‘Ø§Ù… Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
    - ÙŠØªØ¹Ø±Ù‘Ù Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³Ù… ÙˆÙŠØ­ÙØ¸Ù‡
    - ÙŠØ®ØµØµ Ø¨Ø¹Ø¶ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³Ù…
    - ÙŠØ­ÙØ¸ Ø¢Ø®Ø± Ø³Ø¤Ø§Ù„
    """
    q = AR(query or "")
    if not q:
        return "Ø§ÙƒØªØ¨/ÙŠ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ø§Ù‹."

    # Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ø§Ø³Ù…
    possible_name = _extract_name(q)
    if possible_name:
        remember(user_id, "name", possible_name)
        return f"ØªØ´Ø±ÙØª Ø¨Ù…Ø¹Ø±ÙØªÙƒ ÙŠØ§ {possible_name} ğŸŒŸ"

    # ØªØ®ØµÙŠØµ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø¨Ø§Ù„Ø§Ø³Ù… Ø¥Ù† ÙˆÙØ¬Ø¯
    name = recall(user_id, "name", None)
    if name:
        if re.search(r"(Ø´ÙƒØ±Ø§|Ø«Ù†ÙƒÙŠÙˆ|thanks)", q, re.I):
            remember(user_id, "last_query", q)
            return f"Ø§Ù„Ø¹ÙÙˆ ÙŠØ§ {name}! ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ ğŸ™"
        if re.search(r"(ÙƒÙŠÙÙƒ|Ø´Ù„ÙˆÙ†Ùƒ|Ø§Ø®Ø¨Ø§Ø±Ùƒ)", q):
            remember(user_id, "last_query", q)
            return f"ØªÙ…Ø§Ù… Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ØŒ ÙˆØ£Ù†Øª ÙŠØ§ {name}ØŸ ğŸ˜Š"

    # Ø§Ù„Ø±Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    answer = omni_answer(q)

    # Ø­ÙØ¸ Ø¢Ø®Ø± Ø³Ø¤Ø§Ù„
    remember(user_id, "last_query", q)

    # Ø¥Ø¶Ø§ÙØ© Ù„Ù…Ø³Ø© Ø¨Ø³ÙŠØ·Ø© Ø¨Ø§Ù„Ø§Ø³Ù… Ø¹Ù†Ø¯ ØªÙˆÙØ±Ù‡
    if name and isinstance(answer, str) and len(answer) < 400:
        answer = f"{answer}\n\nâ€” Ù…Ø¹Ùƒ Ø¨Ø³Ù‘Ø§Ù…ØŒ Ø¯Ø§ÙŠÙ…Ù‹Ø§ Ø­Ø§Ø¶Ø± ÙŠØ§ {name} ğŸŒŸ"

    return answer
