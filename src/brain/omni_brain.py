# src/brain/omni_brain.py
# Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø®ÙÙŠÙØ© (v3.1 Lite): Ø¨Ø¯ÙˆÙ† FAISS / Sentence Transformers â€” ØªØ¹Ù…Ù„ ÙÙŠ Render Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ

import os, re, math, json, time
from datetime import datetime
from dateutil import parser as dateparser
from typing import List, Dict, Optional

import httpx
from bs4 import BeautifulSoup
from readability import Document
from duckduckgo_search import DDGS
from diskcache import Cache
from wikipedia import summary as wiki_summary
from sympy import sympify, diff, integrate

# âœ… Sumy Ø§Ù„ØµØ­ÙŠØ­
from sumy.parsers.text import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# âœ… Ù…Ø³ØªØ±Ø¬Ø¹ Ø®ÙÙŠÙ Ù…Ù† Ù…Ø¬Ù„Ø¯ docs (BM25 ÙÙ‚Ø·) â€” ÙŠÙØ³ØªÙˆØ±Ø¯ Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„
from src.rag.retriever import query_index as rag_file_query

# ===== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© =====
UA = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/124 Safari/537.36"}
cache = Cache(".cache")

# âœ… RAG Switch (ØªØ´ØºÙŠÙ„/Ø¥ÙŠÙ‚Ø§Ù Ø¹Ø¨Ø± Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦ÙŠ)
RAG_ENABLED = os.getenv("BASSAM_RAG", "off").lower() in {"1", "true", "on", "yes"}

# âœ… Gemini (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
USE_GEMINI = bool(os.getenv("GEMINI_API_KEY"))
if USE_GEMINI:
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    GEMINI = genai.GenerativeModel("gemini-1.5-flash")
else:
    GEMINI = None

# ===== Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ù†ØµÙŠØ© =====
AR = lambda s: re.sub(r"\s+", " ", (s or "").strip())

# ===== ØªÙ„Ø®ÙŠØµ Ù…Ø­Ù„ÙŠ =====
def summarize_text(text: str, max_sentences: int = 5) -> str:
    try:
        parser = PlainTextParser.from_string(text, Tokenizer("arabic"))
        summ = TextRankSummarizer()
        sents = summ(parser.document, max_sentences)
        return " ".join(str(s) for s in sents)
    except Exception:
        return text[:700]

# ===== Ø¨Ø­Ø« Ø§Ù„ÙˆÙŠØ¨ =====
def ddg_text(q: str, n: int = 5) -> List[Dict]:
    with DDGS() as ddgs:
        return list(ddgs.text(q, region="xa-ar", safesearch="moderate", max_results=n) or [])

def fetch_clean(url: str, timeout: int = 12) -> str:
    try:
        r = httpx.get(url, headers=UA, timeout=timeout, follow_redirects=True)
        r.raise_for_status()
        doc = Document(r.text)
        html_clean = doc.summary()
        text = BeautifulSoup(html_clean, "lxml").get_text("\n", strip=True)
        return text[:8000]
    except Exception:
        return ""

# ===== Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ© =====
MATH_PAT = re.compile(r"[=+\-*/^()]|sin|cos|tan|log|sqrt|âˆ«|dx|dy|Ù…Ø´ØªÙ‚Ø©|ØªÙƒØ§Ù…Ù„", re.I)
CURRENCY = {"USD": 1.0, "EUR": 0.92, "SAR": 3.75, "AED": 3.67, "YER": 250.0}

def answer_math(q: str) -> Optional[str]:
    if not MATH_PAT.search(q):
        return None
    try:
        expr = sympify(q.replace("^", "**"))
        return f"ğŸ”¹ Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ: {expr.evalf()}"
    except Exception:
        if "Ù…Ø´ØªÙ‚Ø©" in q:
            try:
                term = q.split("Ù…Ø´ØªÙ‚Ø©", 1)[1].strip()
                return f"Ù…Ø´ØªÙ‚Ø© {term} = {diff(sympify(term))}"
            except:
                return "âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ù„Ù„Ù…Ø´ØªÙ‚Ø©."
        if "ØªÙƒØ§Ù…Ù„" in q:
            try:
                term = q.split("ØªÙƒØ§Ù…Ù„", 1)[1].strip()
                return f"ØªÙƒØ§Ù…Ù„ {term} = {integrate(sympify(term))}"
            except:
                return "âš ï¸ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„ØªÙƒØ§Ù…Ù„."
        return None

def answer_units_dates(q: str) -> Optional[str]:
    m = re.search(r"(\d+[\.,]?\d*)\s*(USD|EUR|SAR|AED|YER)\s*(?:->|Ø§Ù„Ù‰|Ø¥Ù„Ù‰|to)\s*(USD|EUR|SAR|AED|YER)", q, re.I)
    if m:
        amount = float(m.group(1).replace(",", "."))
        src, dst = m.group(2).upper(), m.group(3).upper()
        out = (amount / CURRENCY[src]) * CURRENCY[dst]
        return f"ğŸ’± ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§: {amount} {src} â‰ˆ {round(out,2)} {dst}"
    return None

# ===== Wikipedia =====
def answer_wikipedia(q: str) -> Optional[str]:
    try:
        return wiki_summary(q, sentences=3, auto_suggest=False, redirect=True)
    except Exception:
        return None

# ===== Beauty Coach =====
BEAUTY_PAT = re.compile(r"(Ø¨Ø´Ø±Ø©|ØªÙØªÙŠØ­|Ø¨ÙŠØ§Ø¶|ØºØ³ÙˆÙ„|Ø´Ø¹Ø±|Ø­Ø¨ Ø´Ø¨Ø§Ø¨|ØªØ±Ø·ÙŠØ¨|Ù‚Ø´Ø±Ø©|Ø±Ø´Ø§Ù‚Ø©|Ø±Ø¬ÙŠÙ…)", re.I)
def beauty_coach(q: str) -> Optional[str]:
    if not BEAUTY_PAT.search(q):
        return None
    tips = [
        "ğŸ§¼ Ø§ØºØ³Ù„ÙŠ ÙˆØ¬Ù‡Ùƒ Ø¨ØºØ³ÙˆÙ„ Ù„Ø·ÙŠÙ Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠÙ‹Ø§.",
        "ğŸ§´ Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ù…Ø±Ø·Ø¨ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù†ÙˆØ¹ Ø¨Ø´Ø±ØªÙƒ.",
        "ğŸ›¡ï¸ Ù„Ø§ ØªÙ†Ø³ÙŠ ÙˆØ§Ù‚ÙŠ Ø§Ù„Ø´Ù…Ø³ ØµØ¨Ø§Ø­Ù‹Ø§.",
        "ğŸ’§ Ø§Ø´Ø±Ø¨ÙŠ Ù…Ø§Ø¡ ÙƒØ§ÙÙ ÙˆØ­Ø§ÙØ¸ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙˆÙ… Ø§Ù„Ù…Ù†ØªØ¸Ù….",
    ]
    return "âœ¨ Ù†ØµÙŠØ­Ø© Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø¬Ù…Ø§Ù„ÙŠØ©:\n" + "\n".join(f"â€¢ {t}" for t in tips)

# ===== Ø§Ù„ØªØ­ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø± =====
def answer_empathy(q: str) -> Optional[str]:
    if any(w in q for w in ["Ù…Ø±Ø­Ø¨Ø§", "Ù‡Ù„Ø§", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±"]):
        return "ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØŒ Ø¬Ø§Ù‡Ø² Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„ÙŠÙˆÙ… âœ¨"
    if any(w in q for w in ["Ø´ÙƒØ±Ø§", "Ø«Ù†ÙƒÙŠÙˆ", "thank", "Ù…Ù…ØªØ§Ø²"]):
        return "ğŸ™ ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§!"
    return None

# ===== RAG Ø§Ù„Ø®ÙÙŠÙ =====
def answer_rag(q: str, k: int = 4):
    if not RAG_ENABLED:
        return None
    try:
        hits = rag_file_query(q, top_k=k)
        if not hits or (len(hits) == 1 and "Ù„Ù… ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³" in hits[0][0]):
            return None
        ctx = "\n\n".join(snippet for _, snippet in hits if snippet)
        srcs = [fname for fname, _ in hits if fname]
        summ = summarize_text(ctx, max_sentences=6)
        return f"{AR(summ)}\n\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ):\n" + "\n".join(f"- {s}" for s in srcs)
    except Exception:
        return None

# ===== Gemini =====
def answer_gemini(q: str) -> Optional[str]:
    if not GEMINI:
        return None
    try:
        res = GEMINI.generate_content("Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆØ¯Ù‚Ø©:\n" + q)
        return (res.text or "").strip()
    except Exception:
        return None

# ===== Ø¨Ø­Ø« Ø§Ù„ÙˆÙŠØ¨ =====
def answer_from_web(q: str) -> str:
    hits = ddg_text(q)
    texts, urls = [], []
    for h in hits:
        u = h.get("href") or h.get("url")
        if not u:
            continue
        t = fetch_clean(u)
        if t:
            texts.append(t)
            urls.append(u)
    if not texts:
        return "âš ï¸ Ù„Ù… Ø£Ø¬Ø¯ Ù…ØµØ§Ø¯Ø± ÙƒØ§ÙÙŠØ©ØŒ Ø­Ø§ÙˆÙ„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ©."
    summary = summarize_text("\n\n".join(texts), 6)
    return f"{AR(summary)}\n\nğŸŒ Ø§Ù„Ù…ØµØ§Ø¯Ø±:\n" + "\n".join(f"- {u}" for u in urls[:5])

# ===== Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ =====
def omni_answer(q: str) -> str:
    q = AR(q)
    if not q:
        return "âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    # Ù…Ø´Ø§Ø¹Ø±
    a = answer_empathy(q)
    if a: return a

    # Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ©
    for tool in (answer_math, answer_units_dates, beauty_coach, answer_wikipedia):
        a = tool(q)
        if a: return a

    # RAG
    a = answer_rag(q)
    if a: return a

    # Gemini
    a = answer_gemini(q)
    if a: return a

    # ÙˆÙŠØ¨
    return answer_from_web(q)
