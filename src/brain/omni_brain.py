# src/brain/omni_brain.py
# Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©: Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ© + RAG + Gemini + ÙˆÙŠØ¨ + Ø°Ø§ÙƒØ±Ø© Ù…Ø³ØªØ®Ø¯Ù… (Ø¥ØµØ¯Ø§Ø± Ø«Ø§Ø¨Øª)

import os, re, math, json, time
from datetime import datetime
from dateutil import parser as dateparser
from typing import List, Dict, Optional

import numpy as np
import httpx
from bs4 import BeautifulSoup
from readability import Document
from duckduckgo_search import DDGS
from diskcache import Cache
from wikipedia import summary as wiki_summary

from sympy import sympify, diff, integrate

# âœ… Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ù†Ø§
from sumy.parsers.plaintext import PlainTextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# âœ… Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
from src.memory.memory import remember, recall

# âœ… RAG
import faiss
from sentence_transformers import SentenceTransformer
from src.rag.indexer import is_ready as rag_cache_ready
from src.rag.retriever import query_index as rag_file_query

# ===== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© =====
UA = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/124 Safari/537.36"}
cache = Cache(".cache")

# ===== Gemini (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) =====
USE_GEMINI = bool(os.getenv("GEMINI_API_KEY"))
if USE_GEMINI:
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    GEMINI = genai.GenerativeModel("gemini-1.5-flash")
else:
    GEMINI = None

# ===== RAG Embeddings =====
try:
    RAG_MODEL_NAME = os.getenv("RAG_EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    RAG_EMB = SentenceTransformer(RAG_MODEL_NAME)
except Exception:
    RAG_EMB = None

# ===== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====
AR = lambda s: re.sub(r"\s+", " ", (s or "").strip())

# ===== ØªÙ„Ø®ÙŠØµ Ù…Ø­Ù„ÙŠ =====
def summarize_text(text: str, max_sentences: int = 5) -> str:
    try:
        parser = PlainTextParser.from_string(text, Tokenizer("arabic"))
        summarizer = TextRankSummarizer()
        sentences = summarizer(parser.document, max_sentences)
        return " ".join(str(s) for s in sentences)
    except Exception:
        return text[:700]

# ===== Ø¨Ø­Ø« Ø§Ù„ÙˆÙŠØ¨ =====
def ddg_text(q: str, n: int = 5) -> List[Dict]:
    with DDGS() as ddgs:
        return list(ddgs.text(q, region="xa-ar", safesearch="moderate", max_results=n) or [])

def fetch_clean(url: str, timeout: int = 12) -> str:
    try:
        r = httpx.get(url, headers=UA, timeout=timeout, follow_redirects=True)
        r.raise_for_status()
        doc = Document(r.text)
        html_clean = doc.summary()
        text = BeautifulSoup(html_clean, "lxml").get_text("\n", strip=True)
        return text[:8000]
    except Exception:
        return ""

# ===== Ø£Ø¯ÙˆØ§Øª Ù…Ø­Ù„ÙŠØ© (Ø±ÙŠØ§Ø¶ÙŠØ§Øª + Ø¹Ù…Ù„Ø§Øª + ØªÙˆØ§Ø±ÙŠØ®) =====
MATH_PAT = re.compile(r"[=+\-*/^()]|sin|cos|tan|log|sqrt|âˆ«|dx|dy|d/dx|Ù…Ø´ØªÙ‚Ø©|ØªÙƒØ§Ù…Ù„", re.I)
CURRENCY = {"USD":1.0, "EUR":0.92, "SAR":3.75, "AED":3.67, "YER":250.0}

def answer_math(q: str) -> Optional[str]:
    if not MATH_PAT.search(q):
        return None
    try:
        s = q.replace("^", "**")
        expr = sympify(s)
        return f"Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ: {expr.evalf()}"
    except Exception:
        if q.strip().startswith("Ù…Ø´ØªÙ‚Ø© "):
            t = q.split("Ù…Ø´ØªÙ‚Ø© ",1)[1]
            try: return f"Ù…Ø´ØªÙ‚Ø© {t} = {diff(sympify(t))}"
            except: return "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„Ù…Ø´ØªÙ‚Ø©."
        if q.strip().startswith("ØªÙƒØ§Ù…Ù„ "):
            t = q.split("ØªÙƒØ§Ù…Ù„ ",1)[1]
            try: return f"ØªÙƒØ§Ù…Ù„ {t} = {integrate(sympify(t))}"
            except: return "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„Ù„ØªÙƒØ§Ù…Ù„."
        return None

def answer_units_dates(q: str) -> Optional[str]:
    m = re.search(r"(\d+[\.,]?\d*)\s*(USD|EUR|SAR|AED|YER)\s*(?:->|Ø§Ù„Ù‰|Ø¥Ù„Ù‰|to)\s*(USD|EUR|SAR|AED|YER)", q, re.I)
    if m:
        amount = float(m.group(1).replace(",", "."))
        src, dst = m.group(2).upper(), m.group(3).upper()
        usd = amount / CURRENCY[src]
        out = usd * CURRENCY[dst]
        return f"ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§: {amount} {src} â‰ˆ {round(out,2)} {dst}"
    return None

# ===== ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ù‚ØµÙŠØ±Ø© =====
def answer_wikipedia(q: str) -> Optional[str]:
    m = re.search(r"^(Ù…Ù† Ù‡Ùˆ|Ù…Ù† Ù‡ÙŠ|Ù…Ø§ Ù‡Ùˆ|Ù…Ø§Ù‡ÙŠ|Ù…Ø§Ù‡ÙŠÙ‡|Ù…Ø§Ù‡ÙŠ)\s+(.+)$", q.strip(), re.I)
    topic = m.group(2) if m else None
    topic = topic or (q if len(q.split())<=6 else None)
    if not topic:
        return None
    try:
        s = wiki_summary(topic, sentences=3, auto_suggest=False, redirect=True)
        return AR(s)
    except Exception:
        return None

# ===== ØªØ­ÙŠØ§Øª + Ù…Ø´Ø§Ø¹Ø± =====
def answer_empathy(q: str) -> Optional[str]:
    greetings = ["Ù…Ø±Ø­Ø¨Ø§","Ø§Ù„Ø³Ù„Ø§Ù…","Ø§Ù‡Ù„Ø§","Ø£Ù‡Ù„Ø§Ù‹","Ù‡Ù„Ø§","ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±","Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±"]
    farewells = ["ÙˆØ¯Ø§Ø¹Ø§","Ø§Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡","Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©","ØªØµØ¨Ø­ Ø¹Ù„Ù‰ Ø®ÙŠØ±"]
    if any(w in q for w in greetings):
        return "Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ! ðŸ˜Š Ø£Ù†Ø§ Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ â€” Ø¬Ø§Ù‡Ø² Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª."
    if any(w in q for w in farewells):
        return "ÙÙŠ Ø£Ù…Ø§Ù† Ø§Ù„Ù„Ù‡ ðŸŒ·"
    if "Ø´ÙƒØ±Ø§" in q or "Ø«Ù†ÙƒÙŠÙˆ" in q or "thanks" in q:
        return "Ø§Ù„Ø¹ÙÙˆ ðŸ™ ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§."
    return None

# ===== Ø§Ù„Ø¹Ù†Ø§ÙŠØ© ÙˆØ§Ù„Ø¬Ù…Ø§Ù„ =====
def beauty_coach(q: str) -> Optional[str]:
    if not re.search(r"(Ø¨Ø´Ø±Ø©|ØªÙØªÙŠØ­|Ø­Ø¨ÙˆØ¨|ØªØ±Ø·ÙŠØ¨|Ø´Ø¹Ø±|Ù‚Ø´Ø±Ù‡|Ø±ØªÙŠÙ†ÙˆÙ„|ØºØ³ÙˆÙ„|ÙˆØ§Ù‚ÙŠ)", q, re.I):
        return None
    tips = [
        "ðŸ§¼ Ø§Ø³ØªØ®Ø¯Ù… ØºØ³ÙˆÙ„ Ù„Ø·ÙŠÙ Ù…Ø±ØªÙŠÙ† Ø¨Ø§Ù„ÙŠÙˆÙ….",
        "ðŸ§´ Ù„Ø§ ØªÙ†Ø³ÙŽ Ø§Ù„ØªØ±Ø·ÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ù„ØºØ³ÙˆÙ„.",
        "ðŸ›¡ï¸ Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù‚ÙŠ Ø´Ù…Ø³ SPF 30+ ÙŠÙˆÙ…ÙŠÙ‹Ø§.",
        "ðŸ’§ Ø§Ø´Ø±Ø¨ Ù…Ø§Ø¡ ÙƒØ§ÙÙ ÙˆÙ†ÙŽÙ… Ø¬ÙŠØ¯Ù‹Ø§.",
    ]
    return "Ù†ØµÙŠØ­ØªÙŠ Ù„Ùƒ âœ¨\n" + "\n".join(f"â€¢ {t}" for t in tips)

# ===== RAG =====
def answer_rag(q: str, k: int = 4) -> Optional[str]:
    try:
        if RAG_EMB and rag_cache_ready():
            index = cache.get("rag:index")
            chunks = cache.get("rag:chunks")
            metas = cache.get("rag:metas")
            if index and chunks:
                qv = RAG_EMB.encode([q], convert_to_numpy=True, normalize_embeddings=True)
                D, I = index.search(qv, k)
                picks = [i for i in I[0] if 0 <= i < len(chunks)]
                if picks:
                    ctx = "\n\n".join(chunks[i] for i in picks)
                    summ = summarize_text(ctx, 6)
                    return f"{summ}\n\nðŸ“š Ù…Ù† Ù…Ù„ÙØ§ØªÙƒ Ø§Ù„Ù…Ø­Ù„ÙŠØ©."
    except Exception:
        return None
    return None

# ===== Gemini =====
def answer_gemini(q: str) -> Optional[str]:
    if not GEMINI:
        return None
    try:
        resp = GEMINI.generate_content("Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø®ØªØµØ±Ø© ÙˆØ§Ù„ÙˆØ§Ø¶Ø­Ø©:\n" + q)
        return (resp.text or "").strip()
    except Exception:
        return None

# ===== ÙˆÙŠØ¨ + ØªÙ„Ø®ÙŠØµ Ù…Ø­Ù„ÙŠ =====
def answer_from_web(q: str) -> str:
    hits = ddg_text(q, n=5)
    contexts, cites = [], []
    for h in hits:
        url = h.get("href") or h.get("url")
        if not url: continue
        txt = fetch_clean(url)
        if txt:
            contexts.append(txt)
            cites.append(url)
    if not contexts:
        return "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø§Ù„Ø¢Ù†ØŒ Ø­Ø§ÙˆÙ„ Ø¨ØµÙŠØºØ© Ù…Ø®ØªÙ„ÙØ©."
    summ = summarize_text("\n\n".join(contexts), 6)
    return f"{summ}\n\nðŸŒ Ø§Ù„Ù…ØµØ§Ø¯Ø±:\n" + "\n".join(f"- {u}" for u in cites[:5])

# ===== Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ =====
def omni_answer(q: str) -> str:
    q = AR(q)
    if not q:
        return "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙˆÙ„Ù‹Ø§."

    for fn in (answer_empathy, answer_math, answer_units_dates, beauty_coach, answer_wikipedia, answer_rag, answer_gemini):
        ans = fn(q)
        if ans:
            return ans
    return answer_from_web(q)
