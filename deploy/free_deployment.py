"""
ูุธุงู ุงููุดุฑ ุงููุฌุงูู ุงููุญุณู - ุจุณุงู ุงูุฐูู
ุชุญุณูู ูููุดุฑ ุนูู Renderุ Vercelุ Replit ูุน ุฃูุตู ุงุณุชูุงุฏุฉ ูู ุงูุฎุฏูุงุช ุงููุฌุงููุฉ
"""

import os
import json
import yaml
from typing import Dict, Any, List

class FreeDeploymentOptimizer:
    """ูุญุณู ุงููุดุฑ ุงููุฌุงูู"""
    
    def __init__(self):
        self.platforms = {
            'render': {
                'build_minutes': 500,  # ุดูุฑูุงู
                'bandwidth_gb': 100,   # ุดูุฑูุงู
                'sleep_minutes': 15,   # ุจุนุฏ ุนุฏู ุงููุดุงุท
                'memory_mb': 512,      # ูุฌุงูู
                'databases': 1         # PostgreSQL ูุฌุงูู
            },
            'vercel': {
                'executions': 100000,  # ุดูุฑูุงู
                'bandwidth_gb': 100,   # ุดูุฑูุงู
                'build_minutes': 400,  # ุดูุฑูุงู
                'serverless': True
            },
            'replit': {
                'always_on': False,    # ุบูุฑ ูุฌุงูู
                'databases': True,     # PostgreSQL ูุฌุงูู
                'storage_gb': 1,       # ูุฌุงูู
                'bandwidth_unlimited': True  # ูุฌุงูู
            }
        }
    
    def generate_render_config(self) -> Dict[str, Any]:
        """ุฅูุดุงุก ููู render.yaml ูุญุณู"""
        return {
            'services': [
                {
                    'type': 'web',
                    'name': 'bassam-smart-ai',
                    'runtime': 'python',
                    'plan': 'free',
                    'region': 'oregon',  # ุฃุณุฑุน ููุทูุฉ ูุฌุงููุฉ
                    'buildCommand': 'pip install --upgrade pip && pip install -r requirements.txt',
                    'startCommand': 'gunicorn -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:$PORT --workers 1 --timeout 120 main:app',
                    'healthCheckPath': '/health',
                    'envVars': [
                        {'key': 'PYTHON_VERSION', 'value': '3.11.7'},
                        {'key': 'PYTHONUNBUFFERED', 'value': '1'},
                        {'key': 'WEB_CONCURRENCY', 'value': '1'},  # ุชุญุณูู ููุฐุงูุฑุฉ ุงููุญุฏูุฏุฉ
                        {'key': 'MAX_WORKERS', 'value': '1'},
                        {'key': 'CACHE_SIZE_MB', 'value': '50'},  # ุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ
                        {'key': 'ENABLE_LOCAL_MODELS', 'value': 'false'}  # ุชุนุทูู ุงูููุงุฐุฌ ุงููุญููุฉ ููุฎุงุฏู
                    ],
                    'disk': {
                        'name': 'bassam-cache',
                        'sizeGB': 1,
                        'mountPath': '/opt/render/project/src/cache'
                    }
                }
            ]
        }
    
    def generate_vercel_config(self) -> Dict[str, Any]:
        """ุฅูุดุงุก ููู vercel.json ูุญุณู"""
        return {
            'version': 2,
            'name': 'bassam-smart-ai',
            'builds': [
                {
                    'src': 'main.py',
                    'use': '@vercel/python',
                    'config': {
                        'maxLambdaSize': '50mb',
                        'runtime': 'python3.11'
                    }
                }
            ],
            'routes': [
                {'src': '/(.*)', 'dest': 'main.py'}
            ],
            'env': {
                'PYTHONPATH': './core',
                'CACHE_SIZE_MB': '20',
                'ENABLE_LOCAL_MODELS': 'false'
            },
            'functions': {
                'main.py': {
                    'memory': 1024,  # MB
                    'maxDuration': 30  # ุซุงููุฉ ููุฎุทุฉ ุงููุฌุงููุฉ
                }
            }
        }
    
    def generate_replit_config(self) -> Dict[str, Any]:
        """ุฅูุดุงุก ููู .replit ูุญุณู"""
        return {
            'language': 'python3',
            'run': 'uvicorn main:app --host 0.0.0.0 --port 5000 --reload',
            'entrypoint': 'main.py',
            'modules': ['python-3.11'],
            'env': {
                'CACHE_SIZE_MB': '100',
                'ENABLE_LOCAL_MODELS': 'true',  # Replit ูุฏุนู ุงูููุงุฐุฌ ุงููุญููุฉ
                'PYTHONPATH': './core'
            },
            'gitignore': [
                'cache/',
                'data/',
                '__pycache__/',
                '*.pyc',
                '.env',
                'logs/'
            ]
        }
    
    def generate_docker_config(self) -> str:
        """ุฅูุดุงุก Dockerfile ูุญุณู ููุฎุฏูุงุช ุงููุฌุงููุฉ"""
        dockerfile = """# Dockerfile ูุญุณู ูุจุณุงู ุงูุฐูู
FROM python:3.11-slim

# ุชุญุณูู ุงูุญุฌู
RUN apt-get update && apt-get install -y --no-install-recommends \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# ุฅุนุฏุงุฏ ุงููุดุฑูุน
WORKDIR /app
COPY requirements.txt .

# ุชุซุจูุช ุงููุชุทูุจุงุช ูุน ุชุญุณูู ุงูุญุฌู
RUN pip install --no-cache-dir --upgrade pip && \\
    pip install --no-cache-dir -r requirements.txt

# ูุณุฎ ุงูููุฏ
COPY . .

# ูุชุบูุฑุงุช ุงูุจูุฆุฉ ููุชุญุณูู
ENV PYTHONUNBUFFERED=1
ENV CACHE_SIZE_MB=50
ENV ENABLE_LOCAL_MODELS=false
ENV MAX_WORKERS=1

# ุฅูุดุงุก ูุฌูุฏุงุช ุงููุงุด
RUN mkdir -p cache data

# ุชุดุบูู ุงูุชุทุจูู
EXPOSE 5000
CMD ["gunicorn", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:5000", "--workers", "1", "--timeout", "120", "main:app"]
"""
        return dockerfile
    
    def generate_optimized_requirements(self) -> List[str]:
        """ูุงุฆูุฉ ูุชุทูุจุงุช ูุญุณูุฉ ูููุดุฑ ุงููุฌุงูู"""
        return [
            # ุงูุฃุณุงุณูุงุช
            'fastapi==0.115.0',
            'uvicorn[standard]==0.30.6',
            'gunicorn>=20.1.0',
            'httpx>=0.28.1',
            'python-multipart==0.0.9',
            
            # ุงูุฐูุงุก ุงูุงุตุทูุงุนู (ุงูุฃุณุงุณู ููุท)
            'google-generativeai>=0.3.0',
            'anthropic>=0.68.0',
            
            # ุงูุฑูุงุถูุงุช (ูุฎูู)
            'sympy==1.13.2',
            'numpy==1.26.4',
            'matplotlib>=3.8.0',
            
            # ุงูุจุญุซ ูุงูุชูุฎูุต (ูุฎูู)
            'duckduckgo-search>=5.0.0',
            'beautifulsoup4>=4.12.0',
            'sumy==0.11.0',
            
            # ุงูุชุฎุฒูู ุงููุคูุช
            'diskcache>=5.6.3',
            
            # ุงููุณุงุนุฏุงุช
            'rapidfuzz==3.9.6',
            'deep-translator==1.11.4'
        ]
    
    def generate_environment_configs(self) -> Dict[str, Dict[str, str]]:
        """ุฅุนุฏุงุฏุงุช ูุชุบูุฑุงุช ุงูุจูุฆุฉ ููู ููุตุฉ"""
        return {
            'render': {
                'PYTHON_VERSION': '3.11.7',
                'PYTHONUNBUFFERED': '1',
                'WEB_CONCURRENCY': '1',
                'CACHE_SIZE_MB': '50',
                'ENABLE_LOCAL_MODELS': 'false',
                'DATABASE_MAX_CONNECTIONS': '3'
            },
            'vercel': {
                'PYTHONPATH': './core',
                'CACHE_SIZE_MB': '20',
                'ENABLE_LOCAL_MODELS': 'false',
                'VERCEL_TIMEOUT': '30'
            },
            'replit': {
                'CACHE_SIZE_MB': '100',
                'ENABLE_LOCAL_MODELS': 'true',
                'PYTHONPATH': './core',
                'DATABASE_MAX_CONNECTIONS': '5'
            },
            'heroku': {
                'PYTHON_VERSION': '3.11.7',
                'CACHE_SIZE_MB': '30',
                'ENABLE_LOCAL_MODELS': 'false',
                'WEB_CONCURRENCY': '1'
            }
        }
    
    def generate_monitoring_config(self) -> Dict[str, Any]:
        """ุฅุนุฏุงุฏ ุงููุฑุงูุจุฉ ุงููุฌุงููุฉ"""
        return {
            'health_checks': {
                'endpoint': '/health',
                'interval_seconds': 300,  # 5 ุฏูุงุฆู
                'timeout_seconds': 30,
                'failure_threshold': 3
            },
            'logging': {
                'level': 'INFO',
                'format': 'json',
                'max_size_mb': 10,
                'retention_days': 7
            },
            'metrics': {
                'enabled': True,
                'endpoint': '/metrics',
                'track_requests': True,
                'track_errors': True,
                'track_performance': True
            }
        }
    
    def create_deployment_files(self) -> Dict[str, str]:
        """ุฅูุดุงุก ุฌููุน ูููุงุช ุงููุดุฑ"""
        files = {}
        
        # Render
        files['render.yaml'] = yaml.dump(self.generate_render_config(), 
                                       default_flow_style=False, allow_unicode=True)
        
        # Vercel  
        files['vercel.json'] = json.dumps(self.generate_vercel_config(), 
                                        indent=2, ensure_ascii=False)
        
        # Replit
        files['.replit'] = json.dumps(self.generate_replit_config(), 
                                    indent=2, ensure_ascii=False)
        
        # Docker
        files['Dockerfile'] = self.generate_docker_config()
        
        # Requirements ูุญุณู
        files['requirements-deploy.txt'] = '\n'.join(self.generate_optimized_requirements())
        
        # ูุชุบูุฑุงุช ุงูุจูุฆุฉ
        env_configs = self.generate_environment_configs()
        for platform, config in env_configs.items():
            files[f'.env.{platform}'] = '\n'.join([f'{k}={v}' for k, v in config.items()])
        
        # ูุฑุงูุจุฉ
        files['monitoring.json'] = json.dumps(self.generate_monitoring_config(), 
                                            indent=2, ensure_ascii=False)
        
        return files
    
    def get_deployment_guide(self) -> str:
        """ุฏููู ุงููุดุฑ ุงูุดุงูู"""
        return """
# ๐ ุฏููู ุงููุดุฑ ุงููุฌุงูู ุงูุดุงูู - ุจุณุงู ุงูุฐูู

## ๐ ุงูุฎุทูุงุช ุงูุณุฑูุนุฉ:

### 1๏ธโฃ Render.com (ูููุตู ุจู ููุฎูููุฉ):
```bash
git add .
git commit -m "๐ ุจุณุงู ุงูุฐูู - ุฌุงูุฒ ูููุดุฑ"
git push origin main
```
- ุงุฐูุจ ุฅูู render.com
- ุงุฑุจุท ูุณุชูุฏุน GitHub
- ุงุฎุชุฑ "Web Service"
- ุงุณุชุฎุฏู ุงูุฅุนุฏุงุฏุงุช ูู render.yaml

### 2๏ธโฃ Vercel (ูููุตู ุจู ูููุงุฌูุฉ):
```bash
npm i -g vercel
vercel --prod
```

### 3๏ธโฃ Replit (ููุชุทููุฑ ูุงูุงุฎุชุจุงุฑ):
- ุงุณุชูุฑุฏ ุงููุดุฑูุน ูู GitHub
- ุงูุชุทุจูู ุณูุนูู ุชููุงุฆูุงู

## ๐ ูุชุบูุฑุงุช ุงูุจูุฆุฉ ุงููุทููุจุฉ:
```
GEMINI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here (ุงุฎุชูุงุฑู)
PERPLEXITY_API_KEY=your_key_here (ุงุฎุชูุงุฑู)
```

## ๐ฏ ุงูุชุญุณููุงุช ุงููุทุจูุฉ:
- โ ุงุณุชุฎุฏุงู ุงูุญุฏ ุงูุฃุฏูู ูู ุงูุฐุงูุฑุฉ
- โ ุชุญุณูู ุฃููุงุช ุงูุจูุงุก
- โ ุชุฎุฒูู ูุคูุช ุฐูู
- โ ูุฑุงูุจุฉ ูุฌุงููุฉ
- โ ููุงุฐุฌ ูุญููุฉ (ุนูุฏ ุงูุฅููุงู)

## ๐ ุญุฏูุฏ ุงูุฎุฏูุงุช ุงููุฌุงููุฉ:
- **Render**: 500 ุฏูููุฉ ุจูุงุก/ุดูุฑุ 100GB ููู
- **Vercel**: 100k ุชูููุฐ/ุดูุฑุ 100GB ููู  
- **Replit**: 1GB ุชุฎุฒููุ ููู ุบูุฑ ูุญุฏูุฏ

## ๐ ุญู ุงููุดุงูู:
1. **ุงูุชูุงุก ุงูุฐุงูุฑุฉ**: ููู CACHE_SIZE_MB
2. **ุจุทุก ูู ุงูุจูุงุก**: ุงุณุชุฎุฏู requirements-deploy.txt
3. **ุงููุทุงุน ุงูุงุชุตุงู**: ูุนูู ุงูููุงุฐุฌ ุงููุญููุฉ
"""

# ุฅูุดุงุก ูุซูู ุนุงู
free_deployment = FreeDeploymentOptimizer()